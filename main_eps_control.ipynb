{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")  # close all previous plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "cache_path: str = \"./cache/exp/\"\n",
    "data_frequency = \"weekly\"\n",
    "start = \"2000-01-01\"\n",
    "end = \"2021-09-30\"  # Data frequency and start/end dates\n",
    "split_ratio_list = [0.6, 0.4]  # Train, validation and test split percentage\n",
    "number_of_observe_per_window: int = 104\n",
    "number_of_asset: int = 20  # Number of assets n_y = 20\n",
    "AV_key: str = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTest:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        number_of_observation_per_window: int,\n",
    "        split_ratio_list: list[float],\n",
    "    ) -> None:\n",
    "        self.data: pd.DataFrame = data\n",
    "        self.number_of_observation_per_window: int = number_of_observation_per_window\n",
    "        self.split_ratio: list[float] = split_ratio_list\n",
    "\n",
    "        num_total_observations: int = self.data.shape[\n",
    "            0\n",
    "        ]  # Calculate the total number of observations in the DataFrame\n",
    "        num_observations_cumulative_split: list[float] = (\n",
    "            num_total_observations * np.cumsum(split_ratio_list)\n",
    "        )  # np.cumsum([0.7, 0.2, 0.1]) = [0.7, 0.9, 1.0]\n",
    "        self.cumulative_number_window_observation: list[int] = [\n",
    "            round(num_observation_cumulative_split)\n",
    "            for num_observation_cumulative_split in num_observations_cumulative_split\n",
    "        ]\n",
    "\n",
    "    def split_update(self, split_ratio_list: list[float]) -> None:\n",
    "        self.split_ratio: list[float] = split_ratio_list\n",
    "        num_observations_total: int = self.data.shape[0]\n",
    "        num_observations_cumulative_split: list[float] = (\n",
    "            num_observations_total * np.cumsum(split_ratio_list)\n",
    "        )  # np.cumsum([0.7, 0.2, 0.1]) = [0.7, 0.9, 1.0]\n",
    "        self.cumulative_number_window_observation = [\n",
    "            round(i) for i in num_observations_cumulative_split\n",
    "        ]\n",
    "\n",
    "    def train(self) -> pd.DataFrame:\n",
    "        return self.data[\n",
    "            : self.cumulative_number_window_observation[0]\n",
    "        ]  # Return the training subset of observations\n",
    "\n",
    "    def test(self):\n",
    "        if (\n",
    "            self.cumulative_number_window_observation[0]\n",
    "            - self.number_of_observation_per_window\n",
    "            < 0\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"The number of observations per window exceeds the number of observations of train data in the dataset.\"\n",
    "            )\n",
    "        return self.data[\n",
    "            self.cumulative_number_window_observation[0]\n",
    "            - self.number_of_observation_per_window : self.cumulative_number_window_observation[\n",
    "                1\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "    def shape(self):\n",
    "        return self.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_original_data: pd.DataFrame = pd.read_pickle(\n",
    "    \"./cache/factor_\" + data_frequency + \".pkl\"\n",
    ")\n",
    "Y_original_data: pd.DataFrame = pd.read_pickle(\n",
    "    \"./cache/asset_\" + data_frequency + \".pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mkt-RF</th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>Mom</th>\n",
       "      <th>ST_Rev</th>\n",
       "      <th>LT_Rev</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>-0.024889</td>\n",
       "      <td>-0.003948</td>\n",
       "      <td>0.005921</td>\n",
       "      <td>-0.008655</td>\n",
       "      <td>0.021933</td>\n",
       "      <td>-0.033839</td>\n",
       "      <td>0.034927</td>\n",
       "      <td>0.002774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-14</th>\n",
       "      <td>0.020696</td>\n",
       "      <td>0.013858</td>\n",
       "      <td>-0.015028</td>\n",
       "      <td>-0.028196</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.015969</td>\n",
       "      <td>-0.001553</td>\n",
       "      <td>0.008910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-21</th>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.060555</td>\n",
       "      <td>-0.025968</td>\n",
       "      <td>-0.048690</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>0.053417</td>\n",
       "      <td>-0.043407</td>\n",
       "      <td>0.020229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-28</th>\n",
       "      <td>-0.057084</td>\n",
       "      <td>0.009003</td>\n",
       "      <td>0.016956</td>\n",
       "      <td>0.013910</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>-0.030094</td>\n",
       "      <td>0.031843</td>\n",
       "      <td>-0.012653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-04</th>\n",
       "      <td>0.044559</td>\n",
       "      <td>-0.001065</td>\n",
       "      <td>-0.026655</td>\n",
       "      <td>-0.019944</td>\n",
       "      <td>-0.014198</td>\n",
       "      <td>0.037680</td>\n",
       "      <td>-0.001666</td>\n",
       "      <td>0.015425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Mkt-RF       SMB       HML       RMW       CMA    Mom     \\\n",
       "Date                                                                     \n",
       "2000-01-07 -0.024889 -0.003948  0.005921 -0.008655  0.021933 -0.033839   \n",
       "2000-01-14  0.020696  0.013858 -0.015028 -0.028196  0.000918  0.015969   \n",
       "2000-01-21  0.000378  0.060555 -0.025968 -0.048690  0.001365  0.053417   \n",
       "2000-01-28 -0.057084  0.009003  0.016956  0.013910  0.016216 -0.030094   \n",
       "2000-02-04  0.044559 -0.001065 -0.026655 -0.019944 -0.014198  0.037680   \n",
       "\n",
       "              ST_Rev    LT_Rev  \n",
       "Date                            \n",
       "2000-01-07  0.034927  0.002774  \n",
       "2000-01-14 -0.001553  0.008910  \n",
       "2000-01-21 -0.043407  0.020229  \n",
       "2000-01-28  0.031843 -0.012653  \n",
       "2000-02-04 -0.001666  0.015425  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_original_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>C</th>\n",
       "      <th>JPM</th>\n",
       "      <th>BAC</th>\n",
       "      <th>XOM</th>\n",
       "      <th>HAL</th>\n",
       "      <th>MCD</th>\n",
       "      <th>WMT</th>\n",
       "      <th>COST</th>\n",
       "      <th>CAT</th>\n",
       "      <th>LMT</th>\n",
       "      <th>JNJ</th>\n",
       "      <th>PFE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>VZ</th>\n",
       "      <th>T</th>\n",
       "      <th>ED</th>\n",
       "      <th>NEM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>-0.032195</td>\n",
       "      <td>-0.045482</td>\n",
       "      <td>-0.086300</td>\n",
       "      <td>-0.030347</td>\n",
       "      <td>-0.058169</td>\n",
       "      <td>-0.029886</td>\n",
       "      <td>0.054369</td>\n",
       "      <td>0.010932</td>\n",
       "      <td>-0.010667</td>\n",
       "      <td>-0.009113</td>\n",
       "      <td>0.019836</td>\n",
       "      <td>0.132809</td>\n",
       "      <td>-0.020110</td>\n",
       "      <td>0.034853</td>\n",
       "      <td>0.063502</td>\n",
       "      <td>0.064274</td>\n",
       "      <td>-0.038464</td>\n",
       "      <td>-0.089726</td>\n",
       "      <td>0.045217</td>\n",
       "      <td>-0.124898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-14</th>\n",
       "      <td>0.009447</td>\n",
       "      <td>0.007268</td>\n",
       "      <td>-0.076337</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.015533</td>\n",
       "      <td>0.037174</td>\n",
       "      <td>-0.014010</td>\n",
       "      <td>-0.052347</td>\n",
       "      <td>0.068455</td>\n",
       "      <td>-0.058394</td>\n",
       "      <td>0.054374</td>\n",
       "      <td>-0.025699</td>\n",
       "      <td>-0.043843</td>\n",
       "      <td>-0.029119</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.078060</td>\n",
       "      <td>-0.042510</td>\n",
       "      <td>-0.048266</td>\n",
       "      <td>-0.065724</td>\n",
       "      <td>-0.029384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-21</th>\n",
       "      <td>0.108224</td>\n",
       "      <td>-0.075724</td>\n",
       "      <td>-0.034086</td>\n",
       "      <td>-0.026897</td>\n",
       "      <td>-0.012723</td>\n",
       "      <td>-0.095248</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.099066</td>\n",
       "      <td>-0.036376</td>\n",
       "      <td>-0.031938</td>\n",
       "      <td>-0.011415</td>\n",
       "      <td>-0.081857</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>-0.040666</td>\n",
       "      <td>-0.052432</td>\n",
       "      <td>-0.024136</td>\n",
       "      <td>0.066596</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>-0.003859</td>\n",
       "      <td>0.018260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-28</th>\n",
       "      <td>-0.087054</td>\n",
       "      <td>-0.053012</td>\n",
       "      <td>-0.005962</td>\n",
       "      <td>-0.005493</td>\n",
       "      <td>0.051412</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>-0.072000</td>\n",
       "      <td>-0.155026</td>\n",
       "      <td>-0.104968</td>\n",
       "      <td>-0.117072</td>\n",
       "      <td>-0.051546</td>\n",
       "      <td>-0.081891</td>\n",
       "      <td>-0.100952</td>\n",
       "      <td>-0.059858</td>\n",
       "      <td>0.003708</td>\n",
       "      <td>0.122137</td>\n",
       "      <td>-0.040460</td>\n",
       "      <td>-0.087209</td>\n",
       "      <td>-0.029797</td>\n",
       "      <td>-0.053327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-04</th>\n",
       "      <td>0.062783</td>\n",
       "      <td>0.084580</td>\n",
       "      <td>0.273464</td>\n",
       "      <td>-0.021814</td>\n",
       "      <td>0.065980</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>0.025355</td>\n",
       "      <td>0.027925</td>\n",
       "      <td>-0.044354</td>\n",
       "      <td>0.021404</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>-0.027356</td>\n",
       "      <td>0.013242</td>\n",
       "      <td>-0.028047</td>\n",
       "      <td>0.015914</td>\n",
       "      <td>0.037551</td>\n",
       "      <td>0.016137</td>\n",
       "      <td>0.070064</td>\n",
       "      <td>-0.009521</td>\n",
       "      <td>0.196411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                AAPL      MSFT      AMZN         C       JPM       BAC  \\\n",
       "date                                                                     \n",
       "2000-01-07 -0.032195 -0.045482 -0.086300 -0.030347 -0.058169 -0.029886   \n",
       "2000-01-14  0.009447  0.007268 -0.076337  0.074074  0.015533  0.037174   \n",
       "2000-01-21  0.108224 -0.075724 -0.034086 -0.026897 -0.012723 -0.095248   \n",
       "2000-01-28 -0.087054 -0.053012 -0.005962 -0.005493  0.051412  0.001313   \n",
       "2000-02-04  0.062783  0.084580  0.273464 -0.021814  0.065980  0.002842   \n",
       "\n",
       "                 XOM       HAL       MCD       WMT      COST       CAT  \\\n",
       "date                                                                     \n",
       "2000-01-07  0.054369  0.010932 -0.010667 -0.009113  0.019836  0.132809   \n",
       "2000-01-14 -0.014010 -0.052347  0.068455 -0.058394  0.054374 -0.025699   \n",
       "2000-01-21  0.014925  0.099066 -0.036376 -0.031938 -0.011415 -0.081857   \n",
       "2000-01-28 -0.072000 -0.155026 -0.104968 -0.117072 -0.051546 -0.081891   \n",
       "2000-02-04  0.025355  0.027925 -0.044354  0.021404  0.152174 -0.027356   \n",
       "\n",
       "                 LMT       JNJ       PFE       DIS        VZ         T  \\\n",
       "date                                                                     \n",
       "2000-01-07 -0.020110  0.034853  0.063502  0.064274 -0.038464 -0.089726   \n",
       "2000-01-14 -0.043843 -0.029119  0.072464  0.078060 -0.042510 -0.048266   \n",
       "2000-01-21  0.024390 -0.040666 -0.052432 -0.024136  0.066596  0.023810   \n",
       "2000-01-28 -0.100952 -0.059858  0.003708  0.122137 -0.040460 -0.087209   \n",
       "2000-02-04  0.013242 -0.028047  0.015914  0.037551  0.016137  0.070064   \n",
       "\n",
       "                  ED       NEM  \n",
       "date                            \n",
       "2000-01-07  0.045217 -0.124898  \n",
       "2000-01-14 -0.065724 -0.029384  \n",
       "2000-01-21 -0.003859  0.018260  \n",
       "2000-01-28 -0.029797 -0.053327  \n",
       "2000-02-04 -0.009521  0.196411  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_original_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def AV(\n",
    "    start: str,\n",
    "    end: str,\n",
    "    split_ratio: list,\n",
    "    data_frequency: str = \"weekly\",\n",
    "    num_observations_per_window: int = 104,\n",
    "    num_assets=None,\n",
    "    use_cache: bool = False,\n",
    "    save_results: bool = False,\n",
    "    AV_key: str = None,\n",
    "):\n",
    "    if use_cache:\n",
    "        X: pd.DataFrame = pd.read_pickle(\"./cache/factor_\" + data_frequency + \".pkl\")\n",
    "        Y: pd.DataFrame = pd.read_pickle(\"./cache/asset_\" + data_frequency + \".pkl\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"We cannot download data from AlphaVantage without an API key.\"\n",
    "        )\n",
    "\n",
    "    # Partition dataset into training and testing sets. Lag the data by one observation, since we are predicting future returns, so we don't need the last observation that doesn't have a future return.\n",
    "    # we don't need the first Y observation that doesn't have a corresponding X observation.\n",
    "    return TrainTest(X[:-1], num_observations_per_window, split_ratio), TrainTest(\n",
    "        Y[1:], num_observations_per_window, split_ratio\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1134, 8)\n",
      "(680, 8)\n",
      "(680, 20)\n"
     ]
    }
   ],
   "source": [
    "X_data, Y_data = AV(\n",
    "    start,\n",
    "    end,\n",
    "    split_ratio_list,\n",
    "    data_frequency=data_frequency,\n",
    "    num_observations_per_window=number_of_observe_per_window,\n",
    "    num_assets=number_of_asset,\n",
    "    use_cache=True,\n",
    "    save_results=False,\n",
    "    AV_key=AV_key,\n",
    ")\n",
    "print(X_data.shape())\n",
    "print(X_data.train().shape)\n",
    "print(Y_data.train().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  8\n",
      "Number of assets:  20\n"
     ]
    }
   ],
   "source": [
    "# Number of features and assets\n",
    "n_X: int = X_data.train().shape[1]\n",
    "n_Y: int = Y_data.train().shape[1]\n",
    "print(\"Number of features: \", n_X)\n",
    "print(\"Number of assets: \", n_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low p-values (< 0.05) suggest that the factor significantly affects the stock returns.\n",
    "High p-values (> 0.05) suggest that the factor's effect on the stock returns is not statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Mkt-RF   SMB   HML   RMW   CMA  Mom     ST_Rev  LT_Rev\n",
      "AAPL    0.34  0.37  0.53  0.92  0.70    0.41    0.41    0.53\n",
      "MSFT    0.64  0.85  0.63  0.80  0.20    0.21    0.97    0.22\n",
      "AMZN    0.31  0.02  0.64  0.28  0.63    0.18    0.45    0.34\n",
      "C       0.25  0.69  0.02  0.04  0.21    0.07    0.02    0.24\n",
      "JPM     0.33  0.64  0.00  0.50  0.48    0.18    0.18    0.01\n",
      "BAC     0.16  0.91  0.01  0.16  0.56    0.15    0.06    0.19\n",
      "XOM     0.03  0.77  0.34  0.15  0.10    0.11    0.51    0.04\n",
      "HAL     0.92  0.48  0.47  0.14  0.14    0.42    0.92    0.05\n",
      "MCD     0.48  0.05  0.57  0.02  0.54    0.27    0.81    0.03\n",
      "WMT     0.00  0.01  0.25  0.00  0.40    0.62    0.04    0.03\n",
      "COST    0.00  0.22  0.85  0.01  0.38    0.66    0.39    0.05\n",
      "CAT     0.92  0.27  0.59  0.23  0.07    0.40    0.67    0.51\n",
      "LMT     0.27  0.74  0.04  0.00  0.61    0.32    0.38    0.60\n",
      "JNJ     0.00  0.91  0.39  0.09  0.84    0.82    0.19    0.06\n",
      "PFE     0.06  0.38  0.95  0.91  0.56    0.75    0.50    0.12\n",
      "DIS     0.35  0.67  0.82  0.01  0.39    0.61    0.19    0.04\n",
      "VZ      0.72  0.30  0.01  0.04  0.15    0.09    0.15    0.00\n",
      "T       0.62  0.80  0.00  0.95  0.01    0.14    0.86    0.00\n",
      "ED      0.30  0.94  0.96  0.00  0.23    0.89    0.73    0.16\n",
      "NEM     0.12  0.07  0.05  0.01  0.20    0.05    0.76    0.50\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def statanalysis(X: pd.DataFrame, Y: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Initialize an empty DataFrame to store p-values\n",
    "    # Rows correspond to assets (Y.columns) and columns correspond to features (X.columns)\n",
    "    stats = pd.DataFrame(\n",
    "        columns=X.columns, index=Y.columns\n",
    "    )  # Create an empty DataFrame to store the p-values\n",
    "    for ticker in Y.columns:\n",
    "        for feature in X.columns:\n",
    "            stats.loc[ticker, feature] = (\n",
    "                sm.OLS(Y[ticker].values, sm.add_constant(X[feature]).values)\n",
    "                .fit()\n",
    "                .pvalues[1]  # Get the p-value of the feature\n",
    "            )\n",
    "\n",
    "    return stats.astype(float).round(2)\n",
    "\n",
    "\n",
    "statistical_analysis: pd.DataFrame = statanalysis(X_data.train(), Y_data.train())\n",
    "print(statistical_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhang\\anaconda3\\envs\\py12\\Lib\\site-packages\\diffcp\\cone_program.py:154: SyntaxWarning: invalid escape sequence '\\i'\n",
      "  \"\"\"Solves a cone program, returns its derivative as an abstract linear map.\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SlidingWindow(Dataset):\n",
    "    \"\"\"Sliding window dataset constructor for time series data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        XData: pd.DataFrame,\n",
    "        YData: pd.DataFrame,\n",
    "        num_observations: int,\n",
    "        performance_window: int,\n",
    "    ) -> None:\n",
    "        # Convert the feature DataFrame to a PyTorch tensor with double precision\n",
    "        self.X: torch.Tensor = torch.tensor(XData.values, dtype=torch.float64)\n",
    "        # Convert the asset return DataFrame to a PyTorch tensor with double precision\n",
    "        self.Y: torch.Tensor = torch.tensor(YData.values, dtype=torch.float64)\n",
    "        # Store the number of observations (scenarios) in the sliding window\n",
    "        self.num_observations: int = num_observations\n",
    "        # Store the number of scenarios in the performance window\n",
    "        self.perf_period: int = performance_window\n",
    "\n",
    "    def __getitem__(\n",
    "        self, index: int\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # Extract the feature window starting at 'index' and spanning 'n_obs + 1' time steps\n",
    "        x: torch.Tensor = self.X[index : index + self.num_observations + 1]\n",
    "        # Extract the realizations window starting at 'index' and spanning 'n_obs' time steps\n",
    "        y: torch.Tensor = self.Y[index : index + self.num_observations]\n",
    "        # Extract the performance window starting after the observations window and spanning 'perf_period + 1' time steps\n",
    "        y_future_performance: torch.Tensor = self.Y[\n",
    "            index\n",
    "            + self.num_observations : index\n",
    "            + self.num_observations\n",
    "            + self.perf_period\n",
    "            + 1\n",
    "        ]\n",
    "        # Return the extracted windows as a tuple\n",
    "        return x, y, y_future_performance\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # Calculate the effective length by subtracting the window sizes from the total data length\n",
    "        total_length: int = len(self.X) - self.num_observations - self.perf_period\n",
    "        # Return the calculated length\n",
    "        return total_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackTest:\n",
    "    \"\"\"Backtest object to store out-of-sample results.\"\"\"\n",
    "\n",
    "    def __init__(self, len_test: int, n_y: int, dates: pd.DatetimeIndex) -> None:\n",
    "        # Initialize the weights array with zeros; dimensions are (len_test, n_y)\n",
    "        self.weights: np.ndarray = np.zeros((len_test, n_y))\n",
    "        # Initialize the returns array with zeros; length is len_test\n",
    "        self.rets: np.ndarray = np.zeros(len_test)\n",
    "        # Store the dates corresponding to the out-of-sample period\n",
    "        self.dates: pd.DatetimeIndex = pd.DatetimeIndex(dates[-len_test:])\n",
    "\n",
    "    def stats(self) -> None:\n",
    "        # Calculate the cumulative product of returns plus one to get the total return index\n",
    "        tri: np.ndarray = np.cumprod(self.rets + 1)\n",
    "        # Calculate the geometric mean return over the out-of-sample period\n",
    "        self.mean: float = (tri[-1]) ** (1 / len(tri)) - 1\n",
    "        # Calculate the volatility (standard deviation) of the returns\n",
    "        self.vol: float = np.std(self.rets)\n",
    "        # Calculate the pseudo-Sharpe ratio, handling division by zero\n",
    "        self.sharpe: float = self.mean / self.vol if self.vol != 0 else np.nan\n",
    "        # Create a DataFrame with dates, realized returns, and total return index\n",
    "        self.returns = pd.DataFrame({\"Date\": self.dates, \"rets\": self.rets, \"tri\": tri})\n",
    "        # Set the 'Date' column as the index of the DataFrame\n",
    "        self.returns = self.returns.set_index(\"Date\")\n",
    "\n",
    "    def plot_cumulative_returns(\n",
    "        self,\n",
    "        figsize: tuple = (12, 6),\n",
    "        resample_freq: str | None = None,\n",
    "        title: str | None = None,\n",
    "    ) -> None:\n",
    "        if self.returns is None:\n",
    "            raise ValueError(\n",
    "                \"Returns DataFrame not initialized. Run 'compute_stats' after populating 'rets' and 'weights'.\"\n",
    "            )\n",
    "\n",
    "        data_to_plot = self.returns[\"tri\"]\n",
    "        label = \"Cumulative Return\"\n",
    "\n",
    "        if resample_freq:\n",
    "            # Resample the cumulative return\n",
    "            # For cumulative returns, resampling can be tricky. We'll resample the returns first,\n",
    "            # then compute cumulative returns on the resampled data.\n",
    "            resampled_rets = (\n",
    "                self.returns[\"rets\"]\n",
    "                .resample(resample_freq)\n",
    "                .apply(lambda x: (x + 1).prod() - 1)\n",
    "            )\n",
    "            data_to_plot = (resampled_rets + 1).cumprod()\n",
    "            label = f\"Cumulative Return ({resample_freq})\"\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.plot(data_to_plot, label=label, color=\"blue\")\n",
    "        plt.title(title if title else \"Cumulative Return Over Time\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Cumulative Return\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()  # Adjust layout to prevent clipping of labels\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "\n",
    "\n",
    "# Define the Sharpe loss function\n",
    "def sharpe_loss(z_star: torch.Tensor, y_perf: torch.Tensor) -> torch.Tensor:\n",
    "    loss = -torch.mean(y_perf @ z_star) / torch.std(y_perf @ z_star)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Define the portfolio variance risk function\n",
    "def p_var(z: cp.Variable, c: cp.Variable, x: cp.Expression) -> cp.Expression:\n",
    "    return cp.square(x @ z - c)\n",
    "\n",
    "\n",
    "# Define the Hellinger distance-based DRO optimization layer\n",
    "def hellinger(num_assets: int, num_observations: int, prisk) -> CvxpyLayer:\n",
    "    # Define decision variables\n",
    "    z = cp.Variable((num_assets, 1), nonneg=True)  # Portfolio weights\n",
    "    c_aux = cp.Variable()  # Centering parameter\n",
    "    lambda_aux = cp.Variable(nonneg=True)\n",
    "    xi_aux = cp.Variable()\n",
    "    beta_aux = cp.Variable(num_observations, nonneg=True)\n",
    "    tau_aux = cp.Variable(num_observations, nonneg=True)\n",
    "    mu_aux = cp.Variable()\n",
    "\n",
    "    # Define parameters\n",
    "    ep = cp.Parameter((num_observations, num_assets))  # Residuals matrix\n",
    "    y_hat = cp.Parameter(num_assets)  # Predicted returns\n",
    "    gamma = cp.Parameter(nonneg=True)  # Risk-return trade-off parameter\n",
    "    delta = cp.Parameter(nonneg=True)  # Ambiguity size parameter\n",
    "\n",
    "    # Define constraints\n",
    "    constraints = [\n",
    "        cp.sum(z) == 1,  # Total budget constraint\n",
    "        mu_aux == y_hat @ z,  # Expected return constraint\n",
    "    ]\n",
    "    for i in range(num_observations):\n",
    "        # Constraints based on the risk function\n",
    "        constraints += [xi_aux + lambda_aux >= prisk(z, c_aux, ep[i, :]) + tau_aux[i]]\n",
    "        constraints += [\n",
    "            beta_aux[i] >= cp.quad_over_lin(lambda_aux, tau_aux[i])\n",
    "        ]  # Constraint on the ambiguity set\n",
    "\n",
    "    # Define the objective function\n",
    "    objective = cp.Minimize(\n",
    "        xi_aux\n",
    "        + (delta - 1) * lambda_aux\n",
    "        + (1 / num_observations) * cp.sum(beta_aux)\n",
    "        - gamma * mu_aux\n",
    "    )\n",
    "\n",
    "    # Define the problem\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    # Create a CVXPY layer\n",
    "    cvxpylayer = CvxpyLayer(\n",
    "        problem, parameters=[ep, y_hat, gamma, delta], variables=[z]\n",
    "    )\n",
    "\n",
    "    return cvxpylayer\n",
    "\n",
    "\n",
    "# Define mappings for performance loss functions, risk functions, and optimization layers\n",
    "perf_loss_functions = {\n",
    "    \"sharpe_loss\": sharpe_loss,\n",
    "    # Add other performance loss functions here if needed\n",
    "}\n",
    "\n",
    "risk_functions = {\n",
    "    \"p_var\": p_var,\n",
    "    # Add other risk functions here if needed\n",
    "}\n",
    "\n",
    "opt_layer_functions = {\n",
    "    \"hellinger\": hellinger,\n",
    "    # Add other optimization layer functions here if needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def compute_annualized_sharpe_ratio(\n",
    "    returns: pd.Series | np.ndarray,\n",
    "    risk_free_rate: float = 0.02,\n",
    "    periods_per_year: int = 52,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute the annualized Sharpe Ratio.\n",
    "\n",
    "    Args:\n",
    "        returns (pd.Series | np.ndarray): Periodic returns of the portfolio.\n",
    "        risk_free_rate (float, optional): Annual risk-free rate (e.g., 0.02 for 2%). Defaults to 0.02.\n",
    "        periods_per_year (int, optional): Number of return periods in a year (e.g., 52 for weekly). Defaults to 52.\n",
    "\n",
    "    Returns:\n",
    "        float: Annualized Sharpe Ratio.\n",
    "    \"\"\"\n",
    "    if isinstance(returns, pd.Series):\n",
    "        returns = returns.dropna().values\n",
    "    else:\n",
    "        returns = np.array(returns)\n",
    "        returns = returns[~np.isnan(returns)]\n",
    "\n",
    "    if len(returns) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # Calculate excess returns by subtracting the per-period risk-free rate\n",
    "    excess_returns = returns - (risk_free_rate / periods_per_year)\n",
    "\n",
    "    # Calculate mean and standard deviation of excess returns\n",
    "    mean_excess_return = np.mean(excess_returns) * periods_per_year  # Annualize mean\n",
    "    std_excess_return = np.std(excess_returns, ddof=1) * np.sqrt(\n",
    "        periods_per_year\n",
    "    )  # Annualize std\n",
    "\n",
    "    # Compute Sharpe Ratio with numerical stability\n",
    "    sharpe_ratio = mean_excess_return / (\n",
    "        std_excess_return + 1e-18\n",
    "    )  # Add epsilon to prevent division by zero\n",
    "\n",
    "    return sharpe_ratio\n",
    "\n",
    "\n",
    "def analyze_returns(\n",
    "    returns: pd.DataFrame, risk_free_rate: float = 0.02, frequency: int = 52\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze the returns DataFrame to verify date index, count total weeks,\n",
    "    identify years covered, count weeks per year, and compute Sharpe Ratio per year.\n",
    "\n",
    "    Args:\n",
    "        returns (pd.DataFrame): DataFrame containing portfolio returns with a DatetimeIndex.\n",
    "        risk_free_rate (float, optional): Annual risk-free rate (default is 2%). Defaults to 0.02.\n",
    "        frequency (int, optional): Number of periods per year (default is 52 for weekly data). Defaults to 52.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary containing analysis results.\n",
    "    \"\"\"\n",
    "    analysis_results: Dict[str, Any] = {}\n",
    "\n",
    "    # 1. Verify that the DataFrame is indexed by dates\n",
    "    if not isinstance(returns.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"The DataFrame index must be a pandas DatetimeIndex.\")\n",
    "    analysis_results[\"DateIndexValid\"] = True\n",
    "\n",
    "    # 2. Count the total number of weeks\n",
    "    total_weeks: int = len(returns)\n",
    "    analysis_results[\"TotalWeeks\"] = total_weeks\n",
    "\n",
    "    # 3. Identify all the years covered in the dataset\n",
    "    years: list[int] = returns.index.year.unique().tolist()\n",
    "    years.sort()  # Sort the years in ascending order\n",
    "    analysis_results[\"YearsCovered\"] = years\n",
    "\n",
    "    # 4. Count the number of weeks for each individual year\n",
    "    weeks_per_year: Dict[int, int] = {}\n",
    "    grouped = returns.groupby(returns.index.year)\n",
    "\n",
    "    for year, group in grouped:\n",
    "        weeks_count = len(group)\n",
    "        weeks_per_year[year] = weeks_count\n",
    "\n",
    "    analysis_results[\"WeeksPerYear\"] = weeks_per_year\n",
    "\n",
    "    # 5. Compute Sharpe Ratio for each year\n",
    "    sharpe_ratios_per_year: Dict[int, float] = {}\n",
    "    for year, group in grouped:\n",
    "        sharpe = compute_annualized_sharpe_ratio(\n",
    "            returns=group[\"rets\"],\n",
    "            risk_free_rate=risk_free_rate,\n",
    "            periods_per_year=analysis_results[\"WeeksPerYear\"][year],\n",
    "        )\n",
    "        sharpe_ratios_per_year[year] = sharpe\n",
    "\n",
    "    analysis_results[\"SharpeRatiosPerYear\"] = sharpe_ratios_per_year\n",
    "\n",
    "    return analysis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E2E_net_Eps_Control(nn.Module):\n",
    "    \"\"\"End-to-end Distributionally Robust Optimization (DRO) learning neural net module.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_input_features: int,\n",
    "        num_assets: int,\n",
    "        num_observations: int,\n",
    "        optimization_layer: str = \"hellinger\",\n",
    "        prisk: str = \"p_var\",\n",
    "        performance_objective: str = \"sharpe_loss\",\n",
    "        pred_model: str = \"3layer\",\n",
    "        prediction_loss_factor: float | None = 0.5,\n",
    "        performance_period: int = 13,\n",
    "        train_pred: bool = True,\n",
    "        train_gamma: bool = True,\n",
    "        train_delta: bool = True,\n",
    "        set_seed: int | None = None,\n",
    "        cache_path: str = \"./cache/\",\n",
    "        self_overall_std_dev_factor: float = 1.0,\n",
    "        model_name: str = \"E2E_net_Eps_Control\",\n",
    "    ) -> None:\n",
    "        super(E2E_net_Eps_Control, self).__init__()\n",
    "\n",
    "        # Set random seed for reproducibility\n",
    "        if set_seed is not None:\n",
    "            torch.manual_seed(set_seed)\n",
    "            self.seed: int = set_seed\n",
    "\n",
    "        self.num_features: int = num_input_features  # Number of input features\n",
    "        self.num_assets: int = num_assets  # Number of assets\n",
    "        self.num_observations: int = (\n",
    "            num_observations  # Number of observations/scenarios\n",
    "        )\n",
    "\n",
    "        # Prediction loss function\n",
    "        if prediction_loss_factor is not None:\n",
    "            self.pred_loss_factor: float = prediction_loss_factor\n",
    "            self.pred_loss = nn.MSELoss()  # Mean squared error loss\n",
    "        else:\n",
    "            self.pred_loss = None\n",
    "\n",
    "        # Performance loss function\n",
    "        if performance_objective in perf_loss_functions:\n",
    "            self.perf_loss = perf_loss_functions[performance_objective]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown perf_loss function: {performance_objective}\")\n",
    "\n",
    "        self.perf_period: int = performance_period\n",
    "\n",
    "        # Initialize gamma parameter\n",
    "        self.gamma: nn.Parameter = nn.Parameter(\n",
    "            torch.FloatTensor(1).uniform_(0.02, 0.1)\n",
    "        )\n",
    "        self.gamma.requires_grad = train_gamma\n",
    "        self.gamma_init: float = self.gamma.item()\n",
    "\n",
    "        ub: float = (1 - 1 / (num_observations**0.5)) / 2\n",
    "        lb: float = (1 - 1 / (num_observations**0.5)) / 10\n",
    "        self.delta: nn.Parameter = nn.Parameter(torch.FloatTensor(1).uniform_(lb, ub))\n",
    "        self.delta.requires_grad = train_delta\n",
    "        self.delta_init: float = self.delta.item()\n",
    "        self.model_type = \"dro\"\n",
    "\n",
    "        self.pred_model: str = pred_model\n",
    "\n",
    "        if pred_model == \"2layer\":\n",
    "            hidden_size = int(0.5 * (num_input_features + num_assets))\n",
    "            self.pred_layer = nn.Sequential(\n",
    "                nn.Linear(num_input_features, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, num_assets),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_assets, num_assets),\n",
    "            )\n",
    "        elif pred_model == \"3layer\":\n",
    "            hidden_size1 = int(0.5 * (num_input_features + num_assets))\n",
    "            hidden_size2 = int(0.6 * (num_input_features + num_assets))\n",
    "            self.pred_layer = nn.Sequential(\n",
    "                nn.Linear(num_input_features, hidden_size1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size1, hidden_size2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size2, num_assets),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_assets, num_assets),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pred_model type: {pred_model}\")\n",
    "\n",
    "        # Define the optimization layer\n",
    "        if optimization_layer in opt_layer_functions:\n",
    "            if prisk in risk_functions:\n",
    "                self.opt_layer = opt_layer_functions[optimization_layer](\n",
    "                    num_assets, num_observations, risk_functions[prisk]\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prisk function: {prisk}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown opt_layer function: {optimization_layer}\")\n",
    "\n",
    "        self.cache_path: str = cache_path\n",
    "\n",
    "        self.overall_std_dev_factor: float = self_overall_std_dev_factor\n",
    "        self.model_name: str = model_name\n",
    "\n",
    "    def forward(\n",
    "        self, X: torch.Tensor, Y: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Generate predictions for each time step in X\n",
    "        Y_hat: torch.Tensor = self.pred_layer(X)  # Shape: (n_obs + 1, n_y)\n",
    "        # Calculate residuals\n",
    "        ep: torch.Tensor = Y - Y_hat[:-1]  # Shape: (n_obs, n_y)\n",
    "        # Calculate overall standard deviation (scalar)\n",
    "        self.overall_eps_std_dev: torch.Tensor = (\n",
    "            torch.std(ep, unbiased=True).to(\"cpu\").detach().numpy()\n",
    "        )\n",
    "\n",
    "        # Extract the last prediction\n",
    "        y_hat: torch.Tensor = Y_hat[-1]  # Shape: (n_y,)\n",
    "\n",
    "        # Solver arguments\n",
    "        solver_args: Dict[str, Any] = {\n",
    "            \"solve_method\": \"ECOS\",\n",
    "            \"max_iters\": 120,\n",
    "            \"abstol\": 1e-7,\n",
    "        }\n",
    "\n",
    "        # Optimize z_star\n",
    "        z_star: torch.Tensor\n",
    "        (z_star,) = self.opt_layer(\n",
    "            ep, y_hat, self.gamma, self.delta, solver_args=solver_args\n",
    "        )\n",
    "\n",
    "        return z_star, y_hat\n",
    "\n",
    "    def net_train(\n",
    "        self,\n",
    "        train_set: DataLoader,\n",
    "        val_set: DataLoader | None = None,\n",
    "        epochs: int | None = None,\n",
    "        lr: float | None = None,\n",
    "    ) -> float | None:\n",
    "        # Assign number of epochs and learning rate\n",
    "        if epochs is None:\n",
    "            epochs = self.epochs\n",
    "        if lr is None:\n",
    "            lr = self.lr\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        n_train: int = len(train_set)\n",
    "        self.train()\n",
    "        for epoch in range(epochs):\n",
    "            train_loss: float = 0.0\n",
    "            optimizer.zero_grad()\n",
    "            for _, (x, y, y_perf) in enumerate(train_set):\n",
    "                # Move tensors to the same device as the model\n",
    "                x = x.to(next(self.parameters()).device)\n",
    "                y = y.to(next(self.parameters()).device)\n",
    "                y_perf = y_perf.to(next(self.parameters()).device)\n",
    "\n",
    "                # Forward pass\n",
    "                z_star, y_hat = self(x.squeeze(), y.squeeze())\n",
    "\n",
    "                # Compute loss\n",
    "                if self.pred_loss is None:\n",
    "                    loss = (1 / n_train) * self.perf_loss(z_star, y_perf.squeeze())\n",
    "                else:\n",
    "                    loss = (1 / n_train) * (\n",
    "                        self.perf_loss(z_star, y_perf.squeeze())\n",
    "                        + (self.pred_loss_factor / self.num_assets)\n",
    "                        * self.pred_loss(y_hat, y_perf.squeeze()[0])\n",
    "                        + (\n",
    "                            self.overall_std_dev_factor\n",
    "                            # / self.num_assets\n",
    "                            # / self.num_observations\n",
    "                        )\n",
    "                        * self.overall_eps_std_dev\n",
    "                    )\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Accumulate loss\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "            # Ensure gamma and delta remain positive\n",
    "            for name, param in self.named_parameters():\n",
    "                if name == \"gamma\":\n",
    "                    param.data.clamp_(min=0.0001)\n",
    "\n",
    "        # Validation\n",
    "        if val_set is not None:\n",
    "            n_val: int = len(val_set)\n",
    "            val_loss: float = 0.0\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                for t, (x, y, y_perf) in enumerate(val_set):\n",
    "                    # Forward pass\n",
    "                    z_val, y_val = self(x.squeeze(), y.squeeze())\n",
    "\n",
    "                    # Compute loss\n",
    "                    if self.pred_loss is None:\n",
    "                        loss = (1 / n_val) * self.perf_loss(z_val, y_perf.squeeze())\n",
    "                    else:\n",
    "                        loss = (1 / n_val) * (\n",
    "                            self.perf_loss(z_val, y_perf.squeeze())\n",
    "                            + (self.pred_loss_factor / self.num_assets)\n",
    "                            * self.pred_loss(y_val, y_perf.squeeze()[0])\n",
    "                            + (\n",
    "                                self.overall_std_dev_factor\n",
    "                                # / self.num_assets\n",
    "                                # / self.num_observations\n",
    "                            )\n",
    "                            * self.overall_eps_std_dev\n",
    "                        )\n",
    "\n",
    "                    # Accumulate validation loss\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            return val_loss\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    # net_roll_test: Test the e2e neural net\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def net_roll_test(\n",
    "        self,\n",
    "        X: TrainTest,\n",
    "        Y: TrainTest,\n",
    "        n_roll: int,\n",
    "        lr: float,\n",
    "        epochs: int,\n",
    "        load_state: list[bool] = [False, False, False, False],\n",
    "        save_state: list[bool] = [False, False, False, False],\n",
    "    ) -> None:\n",
    "        # Initialize backtest object\n",
    "        portfolio = BackTest(\n",
    "            len(Y.test()) - Y.number_of_observation_per_window,\n",
    "            self.num_assets,\n",
    "            Y.test().index[Y.number_of_observation_per_window :],\n",
    "        )\n",
    "\n",
    "        # Initialize lists to store trained parameters\n",
    "        self.gamma_trained = []\n",
    "        self.delta_trained = []\n",
    "\n",
    "        # Store initial split\n",
    "        init_split = Y.split_ratio\n",
    "\n",
    "        # Calculate window size\n",
    "        win_size = init_split[1] / n_roll\n",
    "\n",
    "        split = [0, 0]\n",
    "        t = 0\n",
    "        for i in range(n_roll):\n",
    "\n",
    "            print(f\"Out-of-sample window: {i+1} / {n_roll}\")\n",
    "\n",
    "            split[0] = init_split[0] + win_size * i\n",
    "            if i < n_roll - 1:\n",
    "                split[1] = win_size\n",
    "            else:\n",
    "                split[1] = 1 - split[0]\n",
    "\n",
    "            X.split_update(split)\n",
    "            Y.split_update(split)\n",
    "            train_set = DataLoader(\n",
    "                SlidingWindow(\n",
    "                    X.train(), Y.train(), self.num_observations, self.perf_period\n",
    "                )\n",
    "            )\n",
    "            test_set = DataLoader(\n",
    "                SlidingWindow(X.test(), Y.test(), self.num_observations, 0)\n",
    "            )\n",
    "            if load_state[i]:\n",
    "                # Reset model parameters to initial state\n",
    "                self.load_state_dict(\n",
    "                    torch.load(\n",
    "                        self.cache_path + self.model_name + \"_model_\" + str(i) + \".pt\",\n",
    "                        weights_only=True,\n",
    "                    )\n",
    "                )\n",
    "            # Train the model\n",
    "            self.train()\n",
    "            self.net_train(train_set, lr=lr, epochs=epochs)\n",
    "            # Save the trained model\n",
    "            if save_state[i]:\n",
    "                torch.save(\n",
    "                    self.state_dict(),\n",
    "                    self.cache_path + self.model_name + \"_model_\" + str(i) + \".pt\",\n",
    "                )\n",
    "            self.gamma_trained.append(self.gamma.item())\n",
    "            self.delta_trained.append(self.delta.item())\n",
    "            # Test the model\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                for __, (x, y, y_perf) in enumerate(test_set):\n",
    "                    # Move tensors to the same device as the model\n",
    "                    x = x.to(next(self.parameters()).device)\n",
    "                    y = y.to(next(self.parameters()).device)\n",
    "                    y_perf = y_perf.to(next(self.parameters()).device)\n",
    "\n",
    "                    z_star, _ = self(x.squeeze(), y.squeeze())\n",
    "                    portfolio.weights[t] = z_star.squeeze().cpu().numpy()\n",
    "                    portfolio.rets[t] = (\n",
    "                        y_perf.squeeze().cpu().numpy() @ portfolio.weights[t]\n",
    "                    ).item()\n",
    "                    t += 1\n",
    "\n",
    "        # Reset dataset splits\n",
    "        X.split_update(init_split)\n",
    "        Y.split_update(init_split)\n",
    "\n",
    "        # Calculate portfolio statistics\n",
    "        portfolio.stats()\n",
    "        self.portfolio = portfolio\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    # load_cv_results: Load cross-validation results\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def load_cv_results(self, cv_results):\n",
    "        self.cv_results = cv_results\n",
    "\n",
    "        # Select and store the optimal hyperparameters\n",
    "        idx = cv_results.val_loss.idxmin()\n",
    "        self.lr = cv_results.lr[idx]\n",
    "        self.epochs = cv_results.epochs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "Out-of-sample window: 1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhang\\anaconda3\\envs\\py12\\Lib\\site-packages\\diffcp\\cone_program.py:371: UserWarning: Solved/Inaccurate.\n",
      "  warnings.warn(\"Solved/Inaccurate.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60, Loss: -0.2960\n",
      "Epoch 2/60, Loss: -0.2672\n",
      "Epoch 3/60, Loss: -0.2862\n",
      "Epoch 4/60, Loss: -0.2827\n",
      "Epoch 5/60, Loss: -0.2834\n",
      "Epoch 6/60, Loss: -0.2908\n",
      "Epoch 7/60, Loss: -0.2910\n",
      "Epoch 8/60, Loss: -0.2884\n",
      "Epoch 9/60, Loss: -0.2893\n",
      "Epoch 10/60, Loss: -0.2930\n",
      "Epoch 11/60, Loss: -0.2952\n",
      "Epoch 12/60, Loss: -0.2962\n",
      "Epoch 13/60, Loss: -0.2959\n",
      "Epoch 14/60, Loss: -0.2970\n",
      "Epoch 15/60, Loss: -0.2989\n",
      "Epoch 16/60, Loss: -0.2989\n",
      "Epoch 17/60, Loss: -0.3004\n",
      "Epoch 18/60, Loss: -0.3022\n",
      "Epoch 19/60, Loss: -0.3023\n",
      "Epoch 20/60, Loss: -0.3031\n",
      "Epoch 21/60, Loss: -0.3037\n",
      "Epoch 22/60, Loss: -0.3045\n",
      "Epoch 23/60, Loss: -0.3053\n",
      "Epoch 24/60, Loss: -0.3055\n",
      "Epoch 25/60, Loss: -0.3063\n",
      "Epoch 26/60, Loss: -0.3072\n",
      "Epoch 27/60, Loss: -0.3077\n",
      "Epoch 28/60, Loss: -0.3080\n",
      "Epoch 29/60, Loss: -0.3085\n",
      "Epoch 30/60, Loss: -0.3093\n",
      "Epoch 31/60, Loss: -0.3101\n",
      "Epoch 32/60, Loss: -0.3107\n",
      "Epoch 33/60, Loss: -0.3111\n",
      "Epoch 34/60, Loss: -0.3115\n",
      "Epoch 35/60, Loss: -0.3121\n",
      "Epoch 36/60, Loss: -0.3127\n",
      "Epoch 37/60, Loss: -0.3131\n",
      "Epoch 38/60, Loss: -0.3137\n",
      "Epoch 39/60, Loss: -0.3143\n",
      "Epoch 40/60, Loss: -0.3150\n",
      "Epoch 41/60, Loss: -0.3155\n",
      "Epoch 42/60, Loss: -0.3159\n",
      "Epoch 43/60, Loss: -0.3164\n",
      "Epoch 44/60, Loss: -0.3167\n",
      "Epoch 45/60, Loss: -0.3172\n",
      "Epoch 46/60, Loss: -0.3178\n",
      "Epoch 47/60, Loss: -0.3182\n",
      "Epoch 48/60, Loss: -0.3185\n",
      "Epoch 49/60, Loss: -0.3193\n",
      "Epoch 50/60, Loss: -0.3196\n",
      "Epoch 51/60, Loss: -0.3201\n",
      "Epoch 52/60, Loss: -0.3204\n",
      "Epoch 53/60, Loss: -0.3210\n",
      "Epoch 54/60, Loss: -0.3213\n",
      "Epoch 55/60, Loss: -0.3219\n",
      "Epoch 56/60, Loss: -0.3223\n",
      "Epoch 57/60, Loss: -0.3226\n",
      "Epoch 58/60, Loss: -0.3231\n",
      "Epoch 59/60, Loss: -0.3235\n",
      "Epoch 60/60, Loss: -0.3240\n",
      "Out-of-sample window: 2 / 4\n",
      "Epoch 1/60, Loss: -0.3520\n",
      "Epoch 2/60, Loss: -0.3080\n",
      "Epoch 3/60, Loss: -0.3413\n",
      "Epoch 4/60, Loss: -0.3376\n",
      "Epoch 5/60, Loss: -0.3361\n",
      "Epoch 6/60, Loss: -0.3384\n",
      "Epoch 7/60, Loss: -0.3439\n",
      "Epoch 8/60, Loss: -0.3457\n",
      "Epoch 9/60, Loss: -0.3459\n",
      "Epoch 10/60, Loss: -0.3475\n",
      "Epoch 11/60, Loss: -0.3502\n",
      "Epoch 12/60, Loss: -0.3523\n",
      "Epoch 13/60, Loss: -0.3525\n",
      "Epoch 14/60, Loss: -0.3529\n",
      "Epoch 15/60, Loss: -0.3547\n",
      "Epoch 16/60, Loss: -0.3553\n",
      "Epoch 17/60, Loss: -0.3549\n",
      "Epoch 18/60, Loss: -0.3557\n",
      "Epoch 19/60, Loss: -0.3563\n",
      "Epoch 20/60, Loss: -0.3570\n",
      "Epoch 21/60, Loss: -0.3581\n",
      "Epoch 22/60, Loss: -0.3592\n",
      "Epoch 23/60, Loss: -0.3595\n",
      "Epoch 24/60, Loss: -0.3595\n",
      "Epoch 25/60, Loss: -0.3598\n",
      "Epoch 26/60, Loss: -0.3603\n",
      "Epoch 27/60, Loss: -0.3610\n",
      "Epoch 28/60, Loss: -0.3614\n",
      "Epoch 29/60, Loss: -0.3619\n",
      "Epoch 30/60, Loss: -0.3625\n",
      "Epoch 31/60, Loss: -0.3625\n",
      "Epoch 32/60, Loss: -0.3630\n",
      "Epoch 33/60, Loss: -0.3635\n",
      "Epoch 34/60, Loss: -0.3643\n",
      "Epoch 35/60, Loss: -0.3642\n",
      "Epoch 36/60, Loss: -0.3644\n",
      "Epoch 37/60, Loss: -0.3646\n",
      "Epoch 38/60, Loss: -0.3644\n",
      "Epoch 39/60, Loss: -0.3650\n",
      "Epoch 40/60, Loss: -0.3651\n",
      "Epoch 41/60, Loss: -0.3656\n",
      "Epoch 42/60, Loss: -0.3659\n",
      "Epoch 43/60, Loss: -0.3662\n",
      "Epoch 44/60, Loss: -0.3664\n",
      "Epoch 45/60, Loss: -0.3664\n",
      "Epoch 46/60, Loss: -0.3668\n",
      "Epoch 47/60, Loss: -0.3670\n",
      "Epoch 48/60, Loss: -0.3673\n",
      "Epoch 49/60, Loss: -0.3675\n",
      "Epoch 50/60, Loss: -0.3678\n",
      "Epoch 51/60, Loss: -0.3681\n",
      "Epoch 52/60, Loss: -0.3682\n",
      "Epoch 53/60, Loss: -0.3684\n",
      "Epoch 54/60, Loss: -0.3687\n",
      "Epoch 55/60, Loss: -0.3689\n",
      "Epoch 56/60, Loss: -0.3691\n",
      "Epoch 57/60, Loss: -0.3693\n",
      "Epoch 58/60, Loss: -0.3693\n",
      "Epoch 59/60, Loss: -0.3695\n",
      "Epoch 60/60, Loss: -0.3698\n",
      "Out-of-sample window: 3 / 4\n",
      "Epoch 1/60, Loss: -0.3750\n",
      "Epoch 2/60, Loss: -0.3124\n",
      "Epoch 3/60, Loss: -0.3369\n",
      "Epoch 4/60, Loss: -0.3395\n",
      "Epoch 5/60, Loss: -0.3409\n",
      "Epoch 6/60, Loss: -0.3426\n",
      "Epoch 7/60, Loss: -0.3535\n",
      "Epoch 8/60, Loss: -0.3569\n",
      "Epoch 9/60, Loss: -0.3557\n",
      "Epoch 10/60, Loss: -0.3568\n",
      "Epoch 11/60, Loss: -0.3567\n",
      "Epoch 12/60, Loss: -0.3567\n",
      "Epoch 13/60, Loss: -0.3581\n",
      "Epoch 14/60, Loss: -0.3594\n",
      "Epoch 15/60, Loss: -0.3607\n",
      "Epoch 16/60, Loss: -0.3617\n",
      "Epoch 17/60, Loss: -0.3633\n",
      "Epoch 18/60, Loss: -0.3650\n",
      "Epoch 19/60, Loss: -0.3661\n",
      "Epoch 20/60, Loss: -0.3667\n",
      "Epoch 21/60, Loss: -0.3670\n",
      "Epoch 22/60, Loss: -0.3674\n",
      "Epoch 23/60, Loss: -0.3681\n",
      "Epoch 24/60, Loss: -0.3686\n",
      "Epoch 25/60, Loss: -0.3697\n",
      "Epoch 26/60, Loss: -0.3707\n",
      "Epoch 27/60, Loss: -0.3711\n",
      "Epoch 28/60, Loss: -0.3716\n",
      "Epoch 29/60, Loss: -0.3716\n",
      "Epoch 30/60, Loss: -0.3718\n",
      "Epoch 31/60, Loss: -0.3721\n",
      "Epoch 32/60, Loss: -0.3726\n",
      "Epoch 33/60, Loss: -0.3728\n",
      "Epoch 34/60, Loss: -0.3727\n",
      "Epoch 35/60, Loss: -0.3727\n",
      "Epoch 36/60, Loss: -0.3736\n",
      "Epoch 37/60, Loss: -0.3740\n",
      "Epoch 38/60, Loss: -0.3741\n",
      "Epoch 39/60, Loss: -0.3743\n",
      "Epoch 40/60, Loss: -0.3746\n",
      "Epoch 41/60, Loss: -0.3747\n",
      "Epoch 42/60, Loss: -0.3748\n",
      "Epoch 43/60, Loss: -0.3752\n",
      "Epoch 44/60, Loss: -0.3753\n",
      "Epoch 45/60, Loss: -0.3756\n",
      "Epoch 46/60, Loss: -0.3758\n",
      "Epoch 47/60, Loss: -0.3760\n",
      "Epoch 48/60, Loss: -0.3761\n",
      "Epoch 49/60, Loss: -0.3763\n",
      "Epoch 50/60, Loss: -0.3766\n",
      "Epoch 51/60, Loss: -0.3768\n",
      "Epoch 52/60, Loss: -0.3769\n",
      "Epoch 53/60, Loss: -0.3770\n",
      "Epoch 54/60, Loss: -0.3772\n",
      "Epoch 55/60, Loss: -0.3773\n",
      "Epoch 56/60, Loss: -0.3775\n",
      "Epoch 57/60, Loss: -0.3776\n",
      "Epoch 58/60, Loss: -0.3778\n",
      "Epoch 59/60, Loss: -0.3778\n",
      "Epoch 60/60, Loss: -0.3779\n",
      "Out-of-sample window: 4 / 4\n",
      "Epoch 1/60, Loss: -0.3704\n",
      "Epoch 2/60, Loss: -0.3021\n",
      "Epoch 3/60, Loss: -0.2877\n",
      "Epoch 4/60, Loss: -0.3199\n",
      "Epoch 5/60, Loss: -0.3498\n",
      "Epoch 6/60, Loss: -0.3511\n",
      "Epoch 7/60, Loss: -0.3526\n",
      "Epoch 8/60, Loss: -0.3539\n",
      "Epoch 9/60, Loss: -0.3536\n",
      "Epoch 10/60, Loss: -0.3485\n",
      "Epoch 11/60, Loss: -0.3494\n",
      "Epoch 12/60, Loss: -0.3516\n",
      "Epoch 13/60, Loss: -0.3529\n",
      "Epoch 14/60, Loss: -0.3554\n",
      "Epoch 15/60, Loss: -0.3574\n",
      "Epoch 16/60, Loss: -0.3591\n",
      "Epoch 17/60, Loss: -0.3580\n",
      "Epoch 18/60, Loss: -0.3577\n",
      "Epoch 19/60, Loss: -0.3594\n",
      "Epoch 20/60, Loss: -0.3614\n",
      "Epoch 21/60, Loss: -0.3613\n",
      "Epoch 22/60, Loss: -0.3616\n",
      "Epoch 23/60, Loss: -0.3621\n",
      "Epoch 24/60, Loss: -0.3639\n",
      "Epoch 25/60, Loss: -0.3644\n",
      "Epoch 26/60, Loss: -0.3647\n",
      "Epoch 27/60, Loss: -0.3641\n",
      "Epoch 28/60, Loss: -0.3648\n",
      "Epoch 29/60, Loss: -0.3654\n",
      "Epoch 30/60, Loss: -0.3656\n",
      "Epoch 31/60, Loss: -0.3656\n",
      "Epoch 32/60, Loss: -0.3654\n",
      "Epoch 33/60, Loss: -0.3662\n",
      "Epoch 34/60, Loss: -0.3666\n",
      "Epoch 35/60, Loss: -0.3663\n",
      "Epoch 36/60, Loss: -0.3671\n",
      "Epoch 37/60, Loss: -0.3674\n",
      "Epoch 38/60, Loss: -0.3677\n",
      "Epoch 39/60, Loss: -0.3678\n",
      "Epoch 40/60, Loss: -0.3678\n",
      "Epoch 41/60, Loss: -0.3680\n",
      "Epoch 42/60, Loss: -0.3681\n",
      "Epoch 43/60, Loss: -0.3683\n",
      "Epoch 44/60, Loss: -0.3686\n",
      "Epoch 45/60, Loss: -0.3686\n",
      "Epoch 46/60, Loss: -0.3687\n",
      "Epoch 47/60, Loss: -0.3689\n",
      "Epoch 48/60, Loss: -0.3688\n",
      "Epoch 49/60, Loss: -0.3691\n",
      "Epoch 50/60, Loss: -0.3689\n",
      "Epoch 51/60, Loss: -0.3693\n",
      "Epoch 52/60, Loss: -0.3691\n",
      "Epoch 53/60, Loss: -0.3695\n",
      "Epoch 54/60, Loss: -0.3696\n",
      "Epoch 55/60, Loss: -0.3696\n",
      "Epoch 56/60, Loss: -0.3699\n",
      "Epoch 57/60, Loss: -0.3695\n",
      "Epoch 58/60, Loss: -0.3698\n",
      "Epoch 59/60, Loss: -0.3697\n",
      "Epoch 60/60, Loss: -0.3697\n"
     ]
    }
   ],
   "source": [
    "n_roll: int = 4\n",
    "dr_net_eps = E2E_net_Eps_Control(\n",
    "    num_input_features=n_X,\n",
    "    num_assets=n_Y,\n",
    "    num_observations=number_of_observe_per_window,\n",
    "    prisk=\"p_var\",\n",
    "    train_pred=True,\n",
    "    train_gamma=True,\n",
    "    train_delta=True,\n",
    "    set_seed=19260817,\n",
    "    optimization_layer=\"hellinger\",\n",
    "    performance_objective=\"sharpe_loss\",\n",
    "    cache_path=\"./cache/\",\n",
    "    performance_period=13,\n",
    "    prediction_loss_factor=0.5,\n",
    "    self_overall_std_dev_factor=0.05,\n",
    ").double()\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "dr_net_eps.to(device)\n",
    "dr_net_eps.net_roll_test(\n",
    "    X_data,\n",
    "    Y_data,\n",
    "    n_roll=n_roll,\n",
    "    lr=0.005,\n",
    "    epochs=60,\n",
    "    load_state=[True] * (n_roll),\n",
    "    save_state=[True] * (n_roll),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "portfolio = dr_net_eps.portfolio\n",
    "portfolio.plot_cumulative_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe Ratio:  0.14394240889782436\n",
      "Mean Return:  0.0031555756712555727\n",
      "Volatility:  0.021922487579706388\n",
      "Annualized Sharpe Ratio:  1.0379834719898193\n"
     ]
    }
   ],
   "source": [
    "# print the sharpe ratio\n",
    "print(\"Sharpe Ratio: \", portfolio.sharpe)\n",
    "print(\"Mean Return: \", portfolio.mean)\n",
    "print(\"Volatility: \", portfolio.vol)\n",
    "print(\"Annualized Sharpe Ratio: \", portfolio.sharpe * np.sqrt(52))\n",
    "analyze = analyze_returns(portfolio.returns, risk_free_rate=0, frequency=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Results:  {2013: 2.5115578603405098, 2014: 0.9140769416439934, 2015: 0.9860075388432159, 2016: 1.9005994697259132, 2017: 3.2160919252133935, 2018: 0.0068300901541702, 2019: 2.409825411212179, 2020: 0.6071309677768069, 2021: 0.34349846108498533}\n",
      "Mean Sharpe Ratio:  1.4328465184439074\n"
     ]
    }
   ],
   "source": [
    "print(\"Analysis Results: \", analyze[\"SharpeRatiosPerYear\"])\n",
    "print(\"Mean Sharpe Ratio: \", np.mean(list(analyze[\"SharpeRatiosPerYear\"].values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_45556\\4089635158.py:90: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  factor_5 = pdr.get_data_famafrench(\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_45556\\4089635158.py:99: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  mom_df = pdr.get_data_famafrench(\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_45556\\4089635158.py:106: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  st_df = pdr.get_data_famafrench(\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_45556\\4089635158.py:113: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  lt_df = pdr.get_data_famafrench(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6868, 8) (6868, 20)\n",
      "(6180, 8) (6180, 20) (791, 8) (791, 20)\n",
      "            Mkt-RF     SMB     HML     RMW     CMA  Mom     ST_Rev  LT_Rev\n",
      "Date                                                                      \n",
      "1997-05-16 -0.0113  0.0108  0.0037 -0.0044  0.0047 -0.0036  0.0113  0.0069\n",
      "1997-05-19  0.0027 -0.0004 -0.0028 -0.0001  0.0006  0.0024  0.0016  0.0000\n",
      "1997-05-20  0.0091 -0.0055 -0.0021  0.0030 -0.0079 -0.0001 -0.0050 -0.0078\n",
      "1997-05-21 -0.0013  0.0073 -0.0073 -0.0013 -0.0006 -0.0056  0.0022  0.0010\n",
      "1997-05-22 -0.0027  0.0064  0.0041 -0.0008  0.0008 -0.0011  0.0029  0.0005             Mkt-RF     SMB     HML     RMW     CMA  Mom     ST_Rev  LT_Rev\n",
      "Date                                                                      \n",
      "2024-08-23  0.0129  0.0190  0.0085 -0.0048  0.0068  0.0013  0.0078 -0.0012\n",
      "2024-08-26 -0.0034  0.0033  0.0016  0.0013 -0.0006 -0.0045  0.0020  0.0059\n",
      "2024-08-27  0.0006 -0.0090  0.0002  0.0027  0.0023  0.0053 -0.0080 -0.0016\n",
      "2024-08-28 -0.0067 -0.0022  0.0114  0.0055 -0.0016  0.0030  0.0029  0.0068\n",
      "2024-08-29  0.0008  0.0067  0.0028 -0.0015 -0.0122 -0.0079  0.0067  0.0021\n",
      "Ticker          AMZN      MSFT      COST       JPM       BAC        ED  \\\n",
      "Date                                                                     \n",
      "1997-05-19 -0.012040 -0.002707  0.038910 -0.010681 -0.016563  0.012932   \n",
      "1997-05-20 -0.042685  0.034745  0.029963  0.041835  0.012632  0.000000   \n",
      "1997-05-21 -0.127392  0.010493 -0.040000 -0.034974 -0.027027 -0.008511   \n",
      "1997-05-22 -0.021891  0.002076 -0.020834  0.016107  0.000000 -0.017168   \n",
      "1997-05-23  0.074622  0.018653 -0.013539  0.005284  0.008547  0.013100   \n",
      "\n",
      "Ticker           NEM       HAL      AAPL       MCD       XOM         C  \\\n",
      "Date                                                                     \n",
      "1997-05-19  0.016835 -0.009967 -0.014492  0.002404  0.017204  0.009049   \n",
      "1997-05-20  0.003311  0.006712  0.014705 -0.016786  0.000000  0.033633   \n",
      "1997-05-21 -0.003300  0.008333 -0.021738 -0.007317  0.006343 -0.030369   \n",
      "1997-05-22  0.000000  0.001653 -0.014813  0.000000 -0.016806 -0.006711   \n",
      "1997-05-23  0.013245  0.000000  0.015036  0.004914  0.023503  0.018018   \n",
      "\n",
      "Ticker           LMT       DIS       PFE         T       CAT        VZ  \\\n",
      "Date                                                                     \n",
      "1997-05-19  0.016643  0.029366 -0.003722  0.002179 -0.005076  0.000000   \n",
      "1997-05-20  0.013643  0.003003  0.004981  0.006522 -0.010204  0.000000   \n",
      "1997-05-21  0.008075 -0.016467 -0.006196 -0.017279 -0.002577 -0.001815   \n",
      "1997-05-22  0.005340  0.000000 -0.008728 -0.017582  0.007752 -0.014546   \n",
      "1997-05-23 -0.010624  0.013698  0.011321  0.017897  0.000000  0.016606   \n",
      "\n",
      "Ticker           WMT       JNJ  \n",
      "Date                            \n",
      "1997-05-19  0.020921  0.016597  \n",
      "1997-05-20  0.000000 -0.006122  \n",
      "1997-05-21 -0.016394 -0.024641  \n",
      "1997-05-22 -0.008332  0.002105  \n",
      "1997-05-23  0.012604  0.006303   Ticker          AMZN      MSFT      COST       JPM       BAC        ED  \\\n",
      "Date                                                                     \n",
      "2024-08-26 -0.008699 -0.007918  0.015127  0.003939  0.003772  0.002982   \n",
      "2024-08-27 -0.013561  0.000846  0.018364  0.004608 -0.006263 -0.012983   \n",
      "2024-08-28 -0.013401 -0.007829 -0.022940  0.005041  0.007058  0.008033   \n",
      "2024-08-29  0.007728  0.006137 -0.001599  0.004158  0.005507  0.004184   \n",
      "2024-08-30  0.037067  0.009731  0.006485  0.011656  0.014439  0.007440   \n",
      "\n",
      "Ticker           NEM       HAL      AAPL       MCD       XOM         C  \\\n",
      "Date                                                                     \n",
      "2024-08-26  0.004227  0.003463  0.001499 -0.002901  0.021406 -0.005632   \n",
      "2024-08-27  0.008419 -0.007844  0.003742  0.003326 -0.009511 -0.001780   \n",
      "2024-08-28 -0.016509 -0.019608 -0.006753 -0.008942 -0.009857 -0.001621   \n",
      "2024-08-29  0.026047  0.012258  0.014570  0.002822  0.013817  0.004872   \n",
      "2024-08-30  0.003949 -0.009242 -0.003438  0.002779 -0.001608  0.012282   \n",
      "\n",
      "Ticker           LMT       DIS       PFE         T       CAT        VZ  \\\n",
      "Date                                                                     \n",
      "2024-08-26  0.005567  0.013472  0.000692  0.001521  0.007893  0.006795   \n",
      "2024-08-27  0.004139 -0.009588 -0.003458 -0.005567 -0.000114 -0.000964   \n",
      "2024-08-28  0.006370 -0.015512 -0.002429  0.008651 -0.008316  0.000965   \n",
      "2024-08-29  0.005089  0.003576 -0.001044 -0.003027  0.009879 -0.005785   \n",
      "2024-08-30  0.002205  0.006347  0.010098  0.007085  0.012683  0.012849   \n",
      "\n",
      "Ticker           WMT       JNJ  \n",
      "Date                            \n",
      "2024-08-26  0.004359  0.002924  \n",
      "2024-08-27  0.001315 -0.002571  \n",
      "2024-08-28 -0.000657  0.005953  \n",
      "2024-08-29  0.004469  0.001891  \n",
      "2024-08-30  0.010599  0.009925  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from pandas_datareader import data as pdr\n",
    "\n",
    "# Assuming TrainTest is defined elsewhere in your codebase\n",
    "# from your_module import TrainTest\n",
    "\n",
    "\n",
    "def AV_yFinance(\n",
    "    start: str,\n",
    "    end: str,\n",
    "    split: List[float],\n",
    "    freq: str = \"weekly\",\n",
    "    n_obs: int = 104,\n",
    "    n_y: Optional[int] = None,\n",
    "    use_cache: bool = False,\n",
    "    save_results: bool = False,\n",
    ") -> Tuple[TrainTest, TrainTest]:\n",
    "\n",
    "    if use_cache:\n",
    "        X = pd.read_pickle(f\"./cache/factor_{freq}.pkl\")\n",
    "        Y = pd.read_pickle(f\"./cache/asset_{freq}.pkl\")\n",
    "    else:\n",
    "        # Define the list of tickers\n",
    "        tick_list = [\n",
    "            \"AAPL\",\n",
    "            \"MSFT\",\n",
    "            \"AMZN\",\n",
    "            \"C\",\n",
    "            \"JPM\",\n",
    "            \"BAC\",\n",
    "            \"XOM\",\n",
    "            \"HAL\",\n",
    "            \"MCD\",\n",
    "            \"WMT\",\n",
    "            \"COST\",\n",
    "            \"CAT\",\n",
    "            \"LMT\",\n",
    "            \"JNJ\",\n",
    "            \"PFE\",\n",
    "            \"DIS\",\n",
    "            \"VZ\",\n",
    "            \"T\",\n",
    "            \"ED\",\n",
    "            \"NEM\",\n",
    "        ]\n",
    "\n",
    "        if n_y is not None:\n",
    "            tick_list = tick_list[:n_y]\n",
    "\n",
    "        # Download asset data using yfinance\n",
    "        data = yf.download(\n",
    "            tick_list,\n",
    "            start=start,\n",
    "            end=end,\n",
    "            progress=False,\n",
    "            group_by=\"ticker\",\n",
    "            auto_adjust=True,  # Adjusted close prices\n",
    "            threads=True,  # Enable multi-threading for faster downloads\n",
    "        )\n",
    "\n",
    "        if data.empty:\n",
    "            raise ValueError(\n",
    "                \"No data downloaded. Please check the ticker symbols and date range.\"\n",
    "            )\n",
    "\n",
    "        # Extract Adjusted Close prices\n",
    "        if len(tick_list) == 1:\n",
    "            # For single ticker, data['Close'] is a Series, convert to DataFrame\n",
    "            adj_close = data[\"Close\"].to_frame()\n",
    "            adj_close.columns = tick_list\n",
    "        else:\n",
    "            # For multiple tickers, use xs to extract 'Close' for all tickers\n",
    "            try:\n",
    "                adj_close = data.xs(\"Close\", level=1, axis=1)\n",
    "            except KeyError:\n",
    "                raise KeyError(\"Close prices not found in the downloaded data.\")\n",
    "\n",
    "        # Compute daily returns as percentage change\n",
    "        Y = adj_close.pct_change().dropna()\n",
    "\n",
    "        # Download factor data from Kenneth French's data library\n",
    "        dl_freq = \"_daily\"\n",
    "\n",
    "        try:\n",
    "            # 5-Factor Model\n",
    "            factor_5 = pdr.get_data_famafrench(\n",
    "                \"F-F_Research_Data_5_Factors_2x3\" + dl_freq,\n",
    "                start=start,\n",
    "                end=end,\n",
    "            )[0]\n",
    "            rf_df = factor_5[\"RF\"]\n",
    "            factor_5 = factor_5.drop([\"RF\"], axis=1)\n",
    "\n",
    "            # Momentum Factor\n",
    "            mom_df = pdr.get_data_famafrench(\n",
    "                \"F-F_Momentum_Factor\" + dl_freq,\n",
    "                start=start,\n",
    "                end=end,\n",
    "            )[0]\n",
    "\n",
    "            # Short-Term Reversal Factor\n",
    "            st_df = pdr.get_data_famafrench(\n",
    "                \"F-F_ST_Reversal_Factor\" + dl_freq,\n",
    "                start=start,\n",
    "                end=end,\n",
    "            )[0]\n",
    "\n",
    "            # Long-Term Reversal Factor\n",
    "            lt_df = pdr.get_data_famafrench(\n",
    "                \"F-F_LT_Reversal_Factor\" + dl_freq,\n",
    "                start=start,\n",
    "                end=end,\n",
    "            )[0]\n",
    "\n",
    "            # Concatenate all factors and convert to decimal\n",
    "            X = pd.concat([factor_5, mom_df, st_df, lt_df], axis=1) / 100\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to download factor data: {e}\")\n",
    "\n",
    "        # Align factor data (X) with asset returns (Y) based on dates\n",
    "\n",
    "        # Remove timezone from Y.index if present\n",
    "        if Y.index.tz is not None:\n",
    "            Y.index = Y.index.tz_convert(None)\n",
    "\n",
    "        # Ensure X.index is also timezone-naive\n",
    "        if X.index.tz is not None:\n",
    "            X.index = X.index.tz_convert(None)\n",
    "\n",
    "        # Now, perform the alignment\n",
    "        try:\n",
    "            X = X.loc[Y.index]\n",
    "        except KeyError as e:\n",
    "            missing_dates = Y.index.difference(X.index)\n",
    "            if not missing_dates.empty:\n",
    "                print(f\"Missing dates in factor data: {missing_dates}\")\n",
    "                # Optionally, you can drop missing dates or handle them differently\n",
    "                Y = Y.loc[Y.index.intersection(X.index)]\n",
    "                X = X.loc[X.index.intersection(Y.index)]\n",
    "            else:\n",
    "                raise e  # Re-raise if no missing dates found\n",
    "\n",
    "        # Resample data if frequency is not daily\n",
    "        freq_lower = freq.lower()\n",
    "        if freq_lower in [\"weekly\", \"wk\", \"1wk\"]:\n",
    "            Y = Y.resample(\"W-FRI\").apply(lambda x: (x + 1).prod() - 1)\n",
    "            X = X.resample(\"W-FRI\").apply(lambda x: (x + 1).prod() - 1)\n",
    "        elif freq_lower in [\"monthly\", \"1mo\"]:\n",
    "            Y = Y.resample(\"M\").apply(lambda x: (x + 1).prod() - 1)\n",
    "            X = X.resample(\"M\").apply(lambda x: (x + 1).prod() - 1)\n",
    "        # Add more resampling frequencies if needed\n",
    "\n",
    "        # Handle missing values by forward and backward filling using ffill() and bfill()\n",
    "        Y = Y.ffill().bfill()\n",
    "        X = X.ffill().bfill()\n",
    "\n",
    "        # Convert the index to 'YYYY-MM-DD' format\n",
    "        X.index = X.index.strftime(\"%Y-%m-%d\")\n",
    "        X.index = pd.DatetimeIndex(X.index)\n",
    "        Y.index = Y.index.strftime(\"%Y-%m-%d\")\n",
    "        X.index = pd.DatetimeIndex(Y.index)\n",
    "\n",
    "        # Optionally save the results to cache\n",
    "        if save_results:\n",
    "            os.makedirs(\"./cache\", exist_ok=True)\n",
    "            X.to_pickle(f\"./cache/factor_{freq}.pkl\")\n",
    "            Y.to_pickle(f\"./cache/asset_{freq}.pkl\")\n",
    "    print(X.shape, Y.shape)\n",
    "    # Partition dataset into training and testing sets. Lag the data by one observation\n",
    "    # Using the provided TrainTest class\n",
    "    X_train_test = TrainTest(X[:-1], n_obs, split)\n",
    "    Y_train_test = TrainTest(Y[1:], n_obs, split)\n",
    "\n",
    "    return X_train_test, Y_train_test\n",
    "\n",
    "\n",
    "start_paddling = \"1997-04-01\"\n",
    "end_paddling = \"2024-09-01\"  # Data frequency and start/end dates\n",
    "daily_frequency = \"daily\"\n",
    "xf_train_test, yf_train_test = AV_yFinance(\n",
    "    start=start_paddling,\n",
    "    end=end_paddling,\n",
    "    split=[0.9, 0.1],\n",
    "    freq=daily_frequency,\n",
    "    n_obs=104,\n",
    "    n_y=20,\n",
    "    use_cache=False,\n",
    "    save_results=True,\n",
    ")\n",
    "print(\n",
    "    xf_train_test.train().shape,\n",
    "    yf_train_test.train().shape,\n",
    "    xf_train_test.test().shape,\n",
    "    yf_train_test.test().shape,\n",
    ")\n",
    "print(xf_train_test.train().head(), xf_train_test.test().tail())\n",
    "print(yf_train_test.train().head(), yf_train_test.test().tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
