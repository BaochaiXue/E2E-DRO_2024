{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")  # close all previous plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "cache_path: str = \"./cache/exp/\"\n",
    "data_frequency = \"weekly\"\n",
    "start = \"2000-01-01\"\n",
    "end = \"2021-09-30\"  # Data frequency and start/end dates\n",
    "split_ratio_list = [0.6, 0.4]  # Train, validation and test split percentage\n",
    "number_of_observe_per_window: int = 104\n",
    "number_of_asset: int = 20  # Number of assets n_y = 20\n",
    "AV_key: str = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTest:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        number_of_observation_per_window: int,\n",
    "        split_ratio_list: list[float],\n",
    "    ) -> None:\n",
    "        self.data: pd.DataFrame = data\n",
    "        self.number_of_observation_per_window: int = number_of_observation_per_window\n",
    "        self.split_ratio: list[float] = split_ratio_list\n",
    "\n",
    "        num_total_observations: int = self.data.shape[\n",
    "            0\n",
    "        ]  # Calculate the total number of observations in the DataFrame\n",
    "        num_observations_cumulative_split: list[float] = (\n",
    "            num_total_observations * np.cumsum(split_ratio_list)\n",
    "        )  # np.cumsum([0.7, 0.2, 0.1]) = [0.7, 0.9, 1.0]\n",
    "        self.cumulative_number_window_observation: list[int] = [\n",
    "            round(num_observation_cumulative_split)\n",
    "            for num_observation_cumulative_split in num_observations_cumulative_split\n",
    "        ]\n",
    "\n",
    "    def split_update(self, split_ratio_list: list[float]) -> None:\n",
    "        self.split_ratio: list[float] = split_ratio_list\n",
    "        num_observations_total: int = self.data.shape[0]\n",
    "        num_observations_cumulative_split: list[float] = (\n",
    "            num_observations_total * np.cumsum(split_ratio_list)\n",
    "        )  # np.cumsum([0.7, 0.2, 0.1]) = [0.7, 0.9, 1.0]\n",
    "        self.cumulative_number_window_observation = [\n",
    "            round(i) for i in num_observations_cumulative_split\n",
    "        ]\n",
    "\n",
    "    def train(self) -> pd.DataFrame:\n",
    "        return self.data[\n",
    "            : self.cumulative_number_window_observation[0]\n",
    "        ]  # Return the training subset of observations\n",
    "\n",
    "    def test(self):\n",
    "        if (\n",
    "            self.cumulative_number_window_observation[0]\n",
    "            - self.number_of_observation_per_window\n",
    "            < 0\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"The number of observations per window exceeds the number of observations of train data in the dataset.\"\n",
    "            )\n",
    "        return self.data[\n",
    "            self.cumulative_number_window_observation[0]\n",
    "            - self.number_of_observation_per_window : self.cumulative_number_window_observation[\n",
    "                1\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "    def shape(self):\n",
    "        return self.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_original_data: pd.DataFrame = pd.read_pickle(\n",
    "    \"./cache/factor_\" + data_frequency + \".pkl\"\n",
    ")\n",
    "Y_original_data: pd.DataFrame = pd.read_pickle(\n",
    "    \"./cache/asset_\" + data_frequency + \".pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mkt-RF</th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>Mom</th>\n",
       "      <th>ST_Rev</th>\n",
       "      <th>LT_Rev</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>-0.024889</td>\n",
       "      <td>-0.003948</td>\n",
       "      <td>0.005921</td>\n",
       "      <td>-0.008655</td>\n",
       "      <td>0.021933</td>\n",
       "      <td>-0.033839</td>\n",
       "      <td>0.034927</td>\n",
       "      <td>0.002774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-14</th>\n",
       "      <td>0.020696</td>\n",
       "      <td>0.013858</td>\n",
       "      <td>-0.015028</td>\n",
       "      <td>-0.028196</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.015969</td>\n",
       "      <td>-0.001553</td>\n",
       "      <td>0.008910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-21</th>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.060555</td>\n",
       "      <td>-0.025968</td>\n",
       "      <td>-0.048690</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>0.053417</td>\n",
       "      <td>-0.043407</td>\n",
       "      <td>0.020229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-28</th>\n",
       "      <td>-0.057084</td>\n",
       "      <td>0.009003</td>\n",
       "      <td>0.016956</td>\n",
       "      <td>0.013910</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>-0.030094</td>\n",
       "      <td>0.031843</td>\n",
       "      <td>-0.012653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-04</th>\n",
       "      <td>0.044559</td>\n",
       "      <td>-0.001065</td>\n",
       "      <td>-0.026655</td>\n",
       "      <td>-0.019944</td>\n",
       "      <td>-0.014198</td>\n",
       "      <td>0.037680</td>\n",
       "      <td>-0.001666</td>\n",
       "      <td>0.015425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Mkt-RF       SMB       HML       RMW       CMA    Mom     \\\n",
       "Date                                                                     \n",
       "2000-01-07 -0.024889 -0.003948  0.005921 -0.008655  0.021933 -0.033839   \n",
       "2000-01-14  0.020696  0.013858 -0.015028 -0.028196  0.000918  0.015969   \n",
       "2000-01-21  0.000378  0.060555 -0.025968 -0.048690  0.001365  0.053417   \n",
       "2000-01-28 -0.057084  0.009003  0.016956  0.013910  0.016216 -0.030094   \n",
       "2000-02-04  0.044559 -0.001065 -0.026655 -0.019944 -0.014198  0.037680   \n",
       "\n",
       "              ST_Rev    LT_Rev  \n",
       "Date                            \n",
       "2000-01-07  0.034927  0.002774  \n",
       "2000-01-14 -0.001553  0.008910  \n",
       "2000-01-21 -0.043407  0.020229  \n",
       "2000-01-28  0.031843 -0.012653  \n",
       "2000-02-04 -0.001666  0.015425  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_original_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>C</th>\n",
       "      <th>JPM</th>\n",
       "      <th>BAC</th>\n",
       "      <th>XOM</th>\n",
       "      <th>HAL</th>\n",
       "      <th>MCD</th>\n",
       "      <th>WMT</th>\n",
       "      <th>COST</th>\n",
       "      <th>CAT</th>\n",
       "      <th>LMT</th>\n",
       "      <th>JNJ</th>\n",
       "      <th>PFE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>VZ</th>\n",
       "      <th>T</th>\n",
       "      <th>ED</th>\n",
       "      <th>NEM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>-0.032195</td>\n",
       "      <td>-0.045482</td>\n",
       "      <td>-0.086300</td>\n",
       "      <td>-0.030347</td>\n",
       "      <td>-0.058169</td>\n",
       "      <td>-0.029886</td>\n",
       "      <td>0.054369</td>\n",
       "      <td>0.010932</td>\n",
       "      <td>-0.010667</td>\n",
       "      <td>-0.009113</td>\n",
       "      <td>0.019836</td>\n",
       "      <td>0.132809</td>\n",
       "      <td>-0.020110</td>\n",
       "      <td>0.034853</td>\n",
       "      <td>0.063502</td>\n",
       "      <td>0.064274</td>\n",
       "      <td>-0.038464</td>\n",
       "      <td>-0.089726</td>\n",
       "      <td>0.045217</td>\n",
       "      <td>-0.124898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-14</th>\n",
       "      <td>0.009447</td>\n",
       "      <td>0.007268</td>\n",
       "      <td>-0.076337</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.015533</td>\n",
       "      <td>0.037174</td>\n",
       "      <td>-0.014010</td>\n",
       "      <td>-0.052347</td>\n",
       "      <td>0.068455</td>\n",
       "      <td>-0.058394</td>\n",
       "      <td>0.054374</td>\n",
       "      <td>-0.025699</td>\n",
       "      <td>-0.043843</td>\n",
       "      <td>-0.029119</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.078060</td>\n",
       "      <td>-0.042510</td>\n",
       "      <td>-0.048266</td>\n",
       "      <td>-0.065724</td>\n",
       "      <td>-0.029384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-21</th>\n",
       "      <td>0.108224</td>\n",
       "      <td>-0.075724</td>\n",
       "      <td>-0.034086</td>\n",
       "      <td>-0.026897</td>\n",
       "      <td>-0.012723</td>\n",
       "      <td>-0.095248</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.099066</td>\n",
       "      <td>-0.036376</td>\n",
       "      <td>-0.031938</td>\n",
       "      <td>-0.011415</td>\n",
       "      <td>-0.081857</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>-0.040666</td>\n",
       "      <td>-0.052432</td>\n",
       "      <td>-0.024136</td>\n",
       "      <td>0.066596</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>-0.003859</td>\n",
       "      <td>0.018260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-28</th>\n",
       "      <td>-0.087054</td>\n",
       "      <td>-0.053012</td>\n",
       "      <td>-0.005962</td>\n",
       "      <td>-0.005493</td>\n",
       "      <td>0.051412</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>-0.072000</td>\n",
       "      <td>-0.155026</td>\n",
       "      <td>-0.104968</td>\n",
       "      <td>-0.117072</td>\n",
       "      <td>-0.051546</td>\n",
       "      <td>-0.081891</td>\n",
       "      <td>-0.100952</td>\n",
       "      <td>-0.059858</td>\n",
       "      <td>0.003708</td>\n",
       "      <td>0.122137</td>\n",
       "      <td>-0.040460</td>\n",
       "      <td>-0.087209</td>\n",
       "      <td>-0.029797</td>\n",
       "      <td>-0.053327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-04</th>\n",
       "      <td>0.062783</td>\n",
       "      <td>0.084580</td>\n",
       "      <td>0.273464</td>\n",
       "      <td>-0.021814</td>\n",
       "      <td>0.065980</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>0.025355</td>\n",
       "      <td>0.027925</td>\n",
       "      <td>-0.044354</td>\n",
       "      <td>0.021404</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>-0.027356</td>\n",
       "      <td>0.013242</td>\n",
       "      <td>-0.028047</td>\n",
       "      <td>0.015914</td>\n",
       "      <td>0.037551</td>\n",
       "      <td>0.016137</td>\n",
       "      <td>0.070064</td>\n",
       "      <td>-0.009521</td>\n",
       "      <td>0.196411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                AAPL      MSFT      AMZN         C       JPM       BAC  \\\n",
       "date                                                                     \n",
       "2000-01-07 -0.032195 -0.045482 -0.086300 -0.030347 -0.058169 -0.029886   \n",
       "2000-01-14  0.009447  0.007268 -0.076337  0.074074  0.015533  0.037174   \n",
       "2000-01-21  0.108224 -0.075724 -0.034086 -0.026897 -0.012723 -0.095248   \n",
       "2000-01-28 -0.087054 -0.053012 -0.005962 -0.005493  0.051412  0.001313   \n",
       "2000-02-04  0.062783  0.084580  0.273464 -0.021814  0.065980  0.002842   \n",
       "\n",
       "                 XOM       HAL       MCD       WMT      COST       CAT  \\\n",
       "date                                                                     \n",
       "2000-01-07  0.054369  0.010932 -0.010667 -0.009113  0.019836  0.132809   \n",
       "2000-01-14 -0.014010 -0.052347  0.068455 -0.058394  0.054374 -0.025699   \n",
       "2000-01-21  0.014925  0.099066 -0.036376 -0.031938 -0.011415 -0.081857   \n",
       "2000-01-28 -0.072000 -0.155026 -0.104968 -0.117072 -0.051546 -0.081891   \n",
       "2000-02-04  0.025355  0.027925 -0.044354  0.021404  0.152174 -0.027356   \n",
       "\n",
       "                 LMT       JNJ       PFE       DIS        VZ         T  \\\n",
       "date                                                                     \n",
       "2000-01-07 -0.020110  0.034853  0.063502  0.064274 -0.038464 -0.089726   \n",
       "2000-01-14 -0.043843 -0.029119  0.072464  0.078060 -0.042510 -0.048266   \n",
       "2000-01-21  0.024390 -0.040666 -0.052432 -0.024136  0.066596  0.023810   \n",
       "2000-01-28 -0.100952 -0.059858  0.003708  0.122137 -0.040460 -0.087209   \n",
       "2000-02-04  0.013242 -0.028047  0.015914  0.037551  0.016137  0.070064   \n",
       "\n",
       "                  ED       NEM  \n",
       "date                            \n",
       "2000-01-07  0.045217 -0.124898  \n",
       "2000-01-14 -0.065724 -0.029384  \n",
       "2000-01-21 -0.003859  0.018260  \n",
       "2000-01-28 -0.029797 -0.053327  \n",
       "2000-02-04 -0.009521  0.196411  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_original_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def AV(\n",
    "    start: str,\n",
    "    end: str,\n",
    "    split_ratio: list,\n",
    "    data_frequency: str = \"weekly\",\n",
    "    num_observations_per_window: int = 104,\n",
    "    num_assets=None,\n",
    "    use_cache: bool = False,\n",
    "    save_results: bool = False,\n",
    "    AV_key: str = None,\n",
    "):\n",
    "    if use_cache:\n",
    "        X: pd.DataFrame = pd.read_pickle(\"./cache/factor_\" + data_frequency + \".pkl\")\n",
    "        Y: pd.DataFrame = pd.read_pickle(\"./cache/asset_\" + data_frequency + \".pkl\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"We cannot download data from AlphaVantage without an API key.\"\n",
    "        )\n",
    "\n",
    "    # Partition dataset into training and testing sets. Lag the data by one observation, since we are predicting future returns, so we don't need the last observation that doesn't have a future return.\n",
    "    # we don't need the first Y observation that doesn't have a corresponding X observation.\n",
    "    return TrainTest(X[:-1], num_observations_per_window, split_ratio), TrainTest(\n",
    "        Y[1:], num_observations_per_window, split_ratio\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1134, 8)\n",
      "(680, 8)\n",
      "(680, 20)\n"
     ]
    }
   ],
   "source": [
    "X_data, Y_data = AV(\n",
    "    start,\n",
    "    end,\n",
    "    split_ratio_list,\n",
    "    data_frequency=data_frequency,\n",
    "    num_observations_per_window=number_of_observe_per_window,\n",
    "    num_assets=number_of_asset,\n",
    "    use_cache=True,\n",
    "    save_results=False,\n",
    "    AV_key=AV_key,\n",
    ")\n",
    "print(X_data.shape())\n",
    "print(X_data.train().shape)\n",
    "print(Y_data.train().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  8\n",
      "Number of assets:  20\n"
     ]
    }
   ],
   "source": [
    "# Number of features and assets\n",
    "n_X: int = X_data.train().shape[1]\n",
    "n_Y: int = Y_data.train().shape[1]\n",
    "print(\"Number of features: \", n_X)\n",
    "print(\"Number of assets: \", n_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low p-values (< 0.05) suggest that the factor significantly affects the stock returns.\n",
    "High p-values (> 0.05) suggest that the factor's effect on the stock returns is not statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Mkt-RF   SMB   HML   RMW   CMA  Mom     ST_Rev  LT_Rev\n",
      "AAPL    0.34  0.37  0.53  0.92  0.70    0.41    0.41    0.53\n",
      "MSFT    0.64  0.85  0.63  0.80  0.20    0.21    0.97    0.22\n",
      "AMZN    0.31  0.02  0.64  0.28  0.63    0.18    0.45    0.34\n",
      "C       0.25  0.69  0.02  0.04  0.21    0.07    0.02    0.24\n",
      "JPM     0.33  0.64  0.00  0.50  0.48    0.18    0.18    0.01\n",
      "BAC     0.16  0.91  0.01  0.16  0.56    0.15    0.06    0.19\n",
      "XOM     0.03  0.77  0.34  0.15  0.10    0.11    0.51    0.04\n",
      "HAL     0.92  0.48  0.47  0.14  0.14    0.42    0.92    0.05\n",
      "MCD     0.48  0.05  0.57  0.02  0.54    0.27    0.81    0.03\n",
      "WMT     0.00  0.01  0.25  0.00  0.40    0.62    0.04    0.03\n",
      "COST    0.00  0.22  0.85  0.01  0.38    0.66    0.39    0.05\n",
      "CAT     0.92  0.27  0.59  0.23  0.07    0.40    0.67    0.51\n",
      "LMT     0.27  0.74  0.04  0.00  0.61    0.32    0.38    0.60\n",
      "JNJ     0.00  0.91  0.39  0.09  0.84    0.82    0.19    0.06\n",
      "PFE     0.06  0.38  0.95  0.91  0.56    0.75    0.50    0.12\n",
      "DIS     0.35  0.67  0.82  0.01  0.39    0.61    0.19    0.04\n",
      "VZ      0.72  0.30  0.01  0.04  0.15    0.09    0.15    0.00\n",
      "T       0.62  0.80  0.00  0.95  0.01    0.14    0.86    0.00\n",
      "ED      0.30  0.94  0.96  0.00  0.23    0.89    0.73    0.16\n",
      "NEM     0.12  0.07  0.05  0.01  0.20    0.05    0.76    0.50\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def statanalysis(X: pd.DataFrame, Y: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Initialize an empty DataFrame to store p-values\n",
    "    # Rows correspond to assets (Y.columns) and columns correspond to features (X.columns)\n",
    "    stats = pd.DataFrame(\n",
    "        columns=X.columns, index=Y.columns\n",
    "    )  # Create an empty DataFrame to store the p-values\n",
    "    for ticker in Y.columns:\n",
    "        for feature in X.columns:\n",
    "            stats.loc[ticker, feature] = (\n",
    "                sm.OLS(Y[ticker].values, sm.add_constant(X[feature]).values)\n",
    "                .fit()\n",
    "                .pvalues[1]  # Get the p-value of the feature\n",
    "            )\n",
    "\n",
    "    return stats.astype(float).round(2)\n",
    "\n",
    "\n",
    "statistical_analysis: pd.DataFrame = statanalysis(X_data.train(), Y_data.train())\n",
    "print(statistical_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SlidingWindow(Dataset):\n",
    "    \"\"\"Sliding window dataset constructor for time series data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        XData: pd.DataFrame,\n",
    "        YData: pd.DataFrame,\n",
    "        num_observations: int,\n",
    "        performance_window: int,\n",
    "    ) -> None:\n",
    "        # Convert the feature DataFrame to a PyTorch tensor with double precision\n",
    "        self.X: torch.Tensor = torch.tensor(XData.values, dtype=torch.float64)\n",
    "        # Convert the asset return DataFrame to a PyTorch tensor with double precision\n",
    "        self.Y: torch.Tensor = torch.tensor(YData.values, dtype=torch.float64)\n",
    "        # Store the number of observations (scenarios) in the sliding window\n",
    "        self.num_observations: int = num_observations\n",
    "        # Store the number of scenarios in the performance window\n",
    "        self.perf_period: int = performance_window\n",
    "\n",
    "    def __getitem__(\n",
    "        self, index: int\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # Extract the feature window starting at 'index' and spanning 'n_obs + 1' time steps\n",
    "        x: torch.Tensor = self.X[index : index + self.num_observations + 1]\n",
    "        # Extract the realizations window starting at 'index' and spanning 'n_obs' time steps\n",
    "        y: torch.Tensor = self.Y[index : index + self.num_observations]\n",
    "        # Extract the performance window starting after the observations window and spanning 'perf_period + 1' time steps\n",
    "        y_future_performance: torch.Tensor = self.Y[\n",
    "            index\n",
    "            + self.num_observations : index\n",
    "            + self.num_observations\n",
    "            + self.perf_period\n",
    "            + 1\n",
    "        ]\n",
    "        # Return the extracted windows as a tuple\n",
    "        return x, y, y_future_performance\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # Calculate the effective length by subtracting the window sizes from the total data length\n",
    "        total_length: int = len(self.X) - self.num_observations - self.perf_period\n",
    "        # Return the calculated length\n",
    "        return total_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackTest:\n",
    "    \"\"\"Backtest object to store out-of-sample results.\"\"\"\n",
    "\n",
    "    def __init__(self, len_test: int, n_y: int, dates: pd.DatetimeIndex) -> None:\n",
    "        # Initialize the weights array with zeros; dimensions are (len_test, n_y)\n",
    "        self.weights: np.ndarray = np.zeros((len_test, n_y))\n",
    "        # Initialize the returns array with zeros; length is len_test\n",
    "        self.rets: np.ndarray = np.zeros(len_test)\n",
    "        # Store the dates corresponding to the out-of-sample period\n",
    "        self.dates: pd.DatetimeIndex = pd.DatetimeIndex(dates[-len_test:])\n",
    "\n",
    "    def stats(self) -> None:\n",
    "        # Calculate the cumulative product of returns plus one to get the total return index\n",
    "        tri: np.ndarray = np.cumprod(self.rets + 1)\n",
    "        # Calculate the geometric mean return over the out-of-sample period\n",
    "        self.mean: float = (tri[-1]) ** (1 / len(tri)) - 1\n",
    "        # Calculate the volatility (standard deviation) of the returns\n",
    "        self.vol: float = np.std(self.rets)\n",
    "        # Calculate the pseudo-Sharpe ratio, handling division by zero\n",
    "        self.sharpe: float = self.mean / self.vol if self.vol != 0 else np.nan\n",
    "        # Create a DataFrame with dates, realized returns, and total return index\n",
    "        self.returns = pd.DataFrame({\"Date\": self.dates, \"rets\": self.rets, \"tri\": tri})\n",
    "        # Set the 'Date' column as the index of the DataFrame\n",
    "        self.returns = self.returns.set_index(\"Date\")\n",
    "\n",
    "    def plot_cumulative_returns(\n",
    "        self,\n",
    "        figsize: tuple = (12, 6),\n",
    "        resample_freq: str | None = None,\n",
    "        title: str | None = None,\n",
    "    ) -> None:\n",
    "        if self.returns is None:\n",
    "            raise ValueError(\n",
    "                \"Returns DataFrame not initialized. Run 'compute_stats' after populating 'rets' and 'weights'.\"\n",
    "            )\n",
    "\n",
    "        data_to_plot = self.returns[\"tri\"]\n",
    "        label = \"Cumulative Return\"\n",
    "\n",
    "        if resample_freq:\n",
    "            # Resample the cumulative return\n",
    "            # For cumulative returns, resampling can be tricky. We'll resample the returns first,\n",
    "            # then compute cumulative returns on the resampled data.\n",
    "            resampled_rets = (\n",
    "                self.returns[\"rets\"]\n",
    "                .resample(resample_freq)\n",
    "                .apply(lambda x: (x + 1).prod() - 1)\n",
    "            )\n",
    "            data_to_plot = (resampled_rets + 1).cumprod()\n",
    "            label = f\"Cumulative Return ({resample_freq})\"\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.plot(data_to_plot, label=label, color=\"blue\")\n",
    "        plt.title(title if title else \"Cumulative Return Over Time\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Cumulative Return\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()  # Adjust layout to prevent clipping of labels\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "\n",
    "\n",
    "# Define the Sharpe loss function\n",
    "def sharpe_loss(z_star: torch.Tensor, y_perf: torch.Tensor) -> torch.Tensor:\n",
    "    loss = -torch.mean(y_perf @ z_star) / torch.std(y_perf @ z_star)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Define the portfolio variance risk function\n",
    "def p_var(z: cp.Variable, c: cp.Variable, x: cp.Expression) -> cp.Expression:\n",
    "    return cp.square(x @ z - c)\n",
    "\n",
    "\n",
    "# Define the Hellinger distance-based DRO optimization layer\n",
    "def hellinger(num_assets: int, num_observations: int, prisk) -> CvxpyLayer:\n",
    "    # Define decision variables\n",
    "    z = cp.Variable((num_assets, 1), nonneg=True)  # Portfolio weights\n",
    "    c_aux = cp.Variable()  # Centering parameter\n",
    "    lambda_aux = cp.Variable(nonneg=True)\n",
    "    xi_aux = cp.Variable()\n",
    "    beta_aux = cp.Variable(num_observations, nonneg=True)\n",
    "    tau_aux = cp.Variable(num_observations, nonneg=True)\n",
    "    mu_aux = cp.Variable()\n",
    "\n",
    "    # Define parameters\n",
    "    ep = cp.Parameter((num_observations, num_assets))  # Residuals matrix\n",
    "    y_hat = cp.Parameter(num_assets)  # Predicted returns\n",
    "    gamma = cp.Parameter(nonneg=True)  # Risk-return trade-off parameter\n",
    "    delta = cp.Parameter(nonneg=True)  # Ambiguity size parameter\n",
    "\n",
    "    # Define constraints\n",
    "    constraints = [\n",
    "        cp.sum(z) == 1,  # Total budget constraint\n",
    "        mu_aux == y_hat @ z,  # Expected return constraint\n",
    "    ]\n",
    "    for i in range(num_observations):\n",
    "        # Constraints based on the risk function\n",
    "        constraints += [xi_aux + lambda_aux >= prisk(z, c_aux, ep[i, :]) + tau_aux[i]]\n",
    "        constraints += [\n",
    "            beta_aux[i] >= cp.quad_over_lin(lambda_aux, tau_aux[i])\n",
    "        ]  # Constraint on the ambiguity set\n",
    "\n",
    "    # Define the objective function\n",
    "    objective = cp.Minimize(\n",
    "        xi_aux\n",
    "        + (delta - 1) * lambda_aux\n",
    "        + (1 / num_observations) * cp.sum(beta_aux)\n",
    "        - gamma * mu_aux\n",
    "    )\n",
    "\n",
    "    # Define the problem\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    # Create a CVXPY layer\n",
    "    cvxpylayer = CvxpyLayer(\n",
    "        problem, parameters=[ep, y_hat, gamma, delta], variables=[z]\n",
    "    )\n",
    "\n",
    "    return cvxpylayer\n",
    "\n",
    "\n",
    "# Define mappings for performance loss functions, risk functions, and optimization layers\n",
    "perf_loss_functions = {\n",
    "    \"sharpe_loss\": sharpe_loss,\n",
    "    # Add other performance loss functions here if needed\n",
    "}\n",
    "\n",
    "risk_functions = {\n",
    "    \"p_var\": p_var,\n",
    "    # Add other risk functions here if needed\n",
    "}\n",
    "\n",
    "opt_layer_functions = {\n",
    "    \"hellinger\": hellinger,\n",
    "    # Add other optimization layer functions here if needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def compute_annualized_sharpe_ratio(\n",
    "    returns: pd.Series | np.ndarray,\n",
    "    risk_free_rate: float = 0.02,\n",
    "    periods_per_year: int = 52,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute the annualized Sharpe Ratio.\n",
    "\n",
    "    Args:\n",
    "        returns (pd.Series | np.ndarray): Periodic returns of the portfolio.\n",
    "        risk_free_rate (float, optional): Annual risk-free rate (e.g., 0.02 for 2%). Defaults to 0.02.\n",
    "        periods_per_year (int, optional): Number of return periods in a year (e.g., 52 for weekly). Defaults to 52.\n",
    "\n",
    "    Returns:\n",
    "        float: Annualized Sharpe Ratio.\n",
    "    \"\"\"\n",
    "    if isinstance(returns, pd.Series):\n",
    "        returns = returns.dropna().values\n",
    "    else:\n",
    "        returns = np.array(returns)\n",
    "        returns = returns[~np.isnan(returns)]\n",
    "\n",
    "    if len(returns) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # Calculate excess returns by subtracting the per-period risk-free rate\n",
    "    excess_returns = returns - (risk_free_rate / periods_per_year)\n",
    "\n",
    "    # Calculate mean and standard deviation of excess returns\n",
    "    mean_excess_return = np.mean(excess_returns) * periods_per_year  # Annualize mean\n",
    "    std_excess_return = np.std(excess_returns, ddof=1) * np.sqrt(\n",
    "        periods_per_year\n",
    "    )  # Annualize std\n",
    "\n",
    "    # Compute Sharpe Ratio with numerical stability\n",
    "    sharpe_ratio = mean_excess_return / (\n",
    "        std_excess_return + 1e-18\n",
    "    )  # Add epsilon to prevent division by zero\n",
    "\n",
    "    return sharpe_ratio\n",
    "\n",
    "\n",
    "def analyze_returns(\n",
    "    returns: pd.DataFrame, risk_free_rate: float = 0.02, frequency: int = 52\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze the returns DataFrame to verify date index, count total weeks,\n",
    "    identify years covered, count weeks per year, and compute Sharpe Ratio per year.\n",
    "\n",
    "    Args:\n",
    "        returns (pd.DataFrame): DataFrame containing portfolio returns with a DatetimeIndex.\n",
    "        risk_free_rate (float, optional): Annual risk-free rate (default is 2%). Defaults to 0.02.\n",
    "        frequency (int, optional): Number of periods per year (default is 52 for weekly data). Defaults to 52.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary containing analysis results.\n",
    "    \"\"\"\n",
    "    analysis_results: Dict[str, Any] = {}\n",
    "\n",
    "    # 1. Verify that the DataFrame is indexed by dates\n",
    "    if not isinstance(returns.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"The DataFrame index must be a pandas DatetimeIndex.\")\n",
    "    analysis_results[\"DateIndexValid\"] = True\n",
    "\n",
    "    # 2. Count the total number of weeks\n",
    "    total_weeks: int = len(returns)\n",
    "    analysis_results[\"TotalWeeks\"] = total_weeks\n",
    "\n",
    "    # 3. Identify all the years covered in the dataset\n",
    "    years: list[int] = returns.index.year.unique().tolist()\n",
    "    years.sort()  # Sort the years in ascending order\n",
    "    analysis_results[\"YearsCovered\"] = years\n",
    "\n",
    "    # 4. Count the number of weeks for each individual year\n",
    "    weeks_per_year: Dict[int, int] = {}\n",
    "    grouped = returns.groupby(returns.index.year)\n",
    "\n",
    "    for year, group in grouped:\n",
    "        weeks_count = len(group)\n",
    "        weeks_per_year[year] = weeks_count\n",
    "\n",
    "    analysis_results[\"WeeksPerYear\"] = weeks_per_year\n",
    "\n",
    "    # 5. Compute Sharpe Ratio for each year\n",
    "    sharpe_ratios_per_year: Dict[int, float] = {}\n",
    "    for year, group in grouped:\n",
    "        sharpe = compute_annualized_sharpe_ratio(\n",
    "            returns=group[\"rets\"],\n",
    "            risk_free_rate=risk_free_rate,\n",
    "            periods_per_year=analysis_results[\"WeeksPerYear\"][year],\n",
    "        )\n",
    "        sharpe_ratios_per_year[year] = sharpe\n",
    "\n",
    "    analysis_results[\"SharpeRatiosPerYear\"] = sharpe_ratios_per_year\n",
    "\n",
    "    return analysis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E2E_net_Eps_Control(nn.Module):\n",
    "    \"\"\"End-to-end Distributionally Robust Optimization (DRO) learning neural net module.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_input_features: int,\n",
    "        num_assets: int,\n",
    "        num_observations: int,\n",
    "        optimization_layer: str = \"hellinger\",\n",
    "        prisk: str = \"p_var\",\n",
    "        performance_objective: str = \"sharpe_loss\",\n",
    "        pred_model: str = \"3layer\",\n",
    "        prediction_loss_factor: float | None = 0.5,\n",
    "        performance_period: int = 13,\n",
    "        train_pred: bool = True,\n",
    "        train_gamma: bool = True,\n",
    "        train_delta: bool = True,\n",
    "        set_seed: int | None = None,\n",
    "        cache_path: str = \"./cache/\",\n",
    "        self_overall_std_dev_factor: float = 1.0,\n",
    "        model_name: str = \"E2E_net_Eps_Control\",\n",
    "    ) -> None:\n",
    "        super(E2E_net_Eps_Control, self).__init__()\n",
    "\n",
    "        # Set random seed for reproducibility\n",
    "        if set_seed is not None:\n",
    "            torch.manual_seed(set_seed)\n",
    "            self.seed: int = set_seed\n",
    "\n",
    "        self.num_features: int = num_input_features  # Number of input features\n",
    "        self.num_assets: int = num_assets  # Number of assets\n",
    "        self.num_observations: int = (\n",
    "            num_observations  # Number of observations/scenarios\n",
    "        )\n",
    "\n",
    "        # Prediction loss function\n",
    "        if prediction_loss_factor is not None:\n",
    "            self.pred_loss_factor: float = prediction_loss_factor\n",
    "            self.pred_loss = nn.MSELoss()  # Mean squared error loss\n",
    "        else:\n",
    "            self.pred_loss = None\n",
    "\n",
    "        # Performance loss function\n",
    "        if performance_objective in perf_loss_functions:\n",
    "            self.perf_loss = perf_loss_functions[performance_objective]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown perf_loss function: {performance_objective}\")\n",
    "\n",
    "        self.perf_period: int = performance_period\n",
    "\n",
    "        # Initialize gamma parameter\n",
    "        self.gamma: nn.Parameter = nn.Parameter(\n",
    "            torch.FloatTensor(1).uniform_(0.02, 0.1)\n",
    "        )\n",
    "        self.gamma.requires_grad = train_gamma\n",
    "        self.gamma_init: float = self.gamma.item()\n",
    "\n",
    "        ub: float = (1 - 1 / (num_observations**0.5)) / 2\n",
    "        lb: float = (1 - 1 / (num_observations**0.5)) / 10\n",
    "        self.delta: nn.Parameter = nn.Parameter(torch.FloatTensor(1).uniform_(lb, ub))\n",
    "        self.delta.requires_grad = train_delta\n",
    "        self.delta_init: float = self.delta.item()\n",
    "        self.model_type = \"dro\"\n",
    "\n",
    "        self.pred_model: str = pred_model\n",
    "\n",
    "        if pred_model == \"2layer\":\n",
    "            hidden_size = int(0.5 * (num_input_features + num_assets))\n",
    "            self.pred_layer = nn.Sequential(\n",
    "                nn.Linear(num_input_features, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, num_assets),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_assets, num_assets),\n",
    "            )\n",
    "        elif pred_model == \"3layer\":\n",
    "            hidden_size1 = int(0.5 * (num_input_features + num_assets))\n",
    "            hidden_size2 = int(0.6 * (num_input_features + num_assets))\n",
    "            self.pred_layer = nn.Sequential(\n",
    "                nn.Linear(num_input_features, hidden_size1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size1, hidden_size2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size2, num_assets),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_assets, num_assets),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pred_model type: {pred_model}\")\n",
    "\n",
    "        # Define the optimization layer\n",
    "        if optimization_layer in opt_layer_functions:\n",
    "            if prisk in risk_functions:\n",
    "                self.opt_layer = opt_layer_functions[optimization_layer](\n",
    "                    num_assets, num_observations, risk_functions[prisk]\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prisk function: {prisk}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown opt_layer function: {optimization_layer}\")\n",
    "\n",
    "        self.cache_path: str = cache_path\n",
    "\n",
    "        self.overall_std_dev_factor: float = self_overall_std_dev_factor\n",
    "        self.model_name: str = model_name\n",
    "\n",
    "    def forward(\n",
    "        self, X: torch.Tensor, Y: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Generate predictions for each time step in X\n",
    "        Y_hat: torch.Tensor = self.pred_layer(X)  # Shape: (n_obs + 1, n_y)\n",
    "        # Calculate residuals\n",
    "        ep: torch.Tensor = Y - Y_hat[:-1]  # Shape: (n_obs, n_y)\n",
    "        # Calculate overall standard deviation (scalar)\n",
    "        self.overall_eps_std_dev: torch.Tensor = (\n",
    "            torch.std(ep, unbiased=True).to(\"cpu\").detach().numpy()\n",
    "        )\n",
    "\n",
    "        # Extract the last prediction\n",
    "        y_hat: torch.Tensor = Y_hat[-1]  # Shape: (n_y,)\n",
    "\n",
    "        # Solver arguments\n",
    "        solver_args: Dict[str, Any] = {\n",
    "            \"solve_method\": \"ECOS\",\n",
    "            \"max_iters\": 120,\n",
    "            \"abstol\": 1e-7,\n",
    "        }\n",
    "\n",
    "        # Optimize z_star\n",
    "        z_star: torch.Tensor\n",
    "        (z_star,) = self.opt_layer(\n",
    "            ep, y_hat, self.gamma, self.delta, solver_args=solver_args\n",
    "        )\n",
    "\n",
    "        return z_star, y_hat\n",
    "\n",
    "    def net_train(\n",
    "        self,\n",
    "        train_set: DataLoader,\n",
    "        val_set: DataLoader | None = None,\n",
    "        epochs: int | None = None,\n",
    "        lr: float | None = None,\n",
    "    ) -> float | None:\n",
    "        # Assign number of epochs and learning rate\n",
    "        if epochs is None:\n",
    "            epochs = self.epochs\n",
    "        if lr is None:\n",
    "            lr = self.lr\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        n_train: int = len(train_set)\n",
    "        self.train()\n",
    "        for epoch in range(epochs):\n",
    "            train_loss: float = 0.0\n",
    "            optimizer.zero_grad()\n",
    "            for _, (x, y, y_perf) in enumerate(train_set):\n",
    "                # Move tensors to the same device as the model\n",
    "                x = x.to(next(self.parameters()).device)\n",
    "                y = y.to(next(self.parameters()).device)\n",
    "                y_perf = y_perf.to(next(self.parameters()).device)\n",
    "\n",
    "                # Forward pass\n",
    "                z_star, y_hat = self(x.squeeze(), y.squeeze())\n",
    "\n",
    "                # Compute loss\n",
    "                if self.pred_loss is None:\n",
    "                    loss = (1 / n_train) * self.perf_loss(z_star, y_perf.squeeze())\n",
    "                else:\n",
    "                    loss = (1 / n_train) * (\n",
    "                        self.perf_loss(z_star, y_perf.squeeze())\n",
    "                        + (self.pred_loss_factor / self.num_assets)\n",
    "                        * self.pred_loss(y_hat, y_perf.squeeze()[0])\n",
    "                        + (\n",
    "                            self.overall_std_dev_factor\n",
    "                            # / self.num_assets\n",
    "                            # / self.num_observations\n",
    "                        )\n",
    "                        * self.overall_eps_std_dev\n",
    "                    )\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Accumulate loss\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "            # Ensure gamma and delta remain positive\n",
    "            for name, param in self.named_parameters():\n",
    "                if name == \"gamma\":\n",
    "                    param.data.clamp_(min=0.0001)\n",
    "\n",
    "        # Validation\n",
    "        if val_set is not None:\n",
    "            n_val: int = len(val_set)\n",
    "            val_loss: float = 0.0\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                for t, (x, y, y_perf) in enumerate(val_set):\n",
    "                    # Forward pass\n",
    "                    z_val, y_val = self(x.squeeze(), y.squeeze())\n",
    "\n",
    "                    # Compute loss\n",
    "                    if self.pred_loss is None:\n",
    "                        loss = (1 / n_val) * self.perf_loss(z_val, y_perf.squeeze())\n",
    "                    else:\n",
    "                        loss = (1 / n_val) * (\n",
    "                            self.perf_loss(z_val, y_perf.squeeze())\n",
    "                            + (self.pred_loss_factor / self.num_assets)\n",
    "                            * self.pred_loss(y_val, y_perf.squeeze()[0])\n",
    "                            + (\n",
    "                                self.overall_std_dev_factor\n",
    "                                # / self.num_assets\n",
    "                                # / self.num_observations\n",
    "                            )\n",
    "                            * self.overall_eps_std_dev\n",
    "                        )\n",
    "\n",
    "                    # Accumulate validation loss\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            return val_loss\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    # net_roll_test: Test the e2e neural net\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def net_roll_test(\n",
    "        self,\n",
    "        X: TrainTest,\n",
    "        Y: TrainTest,\n",
    "        n_roll: int,\n",
    "        lr: float,\n",
    "        epochs: int,\n",
    "        load_state: list[bool] = [False, False, False, False],\n",
    "        save_state: list[bool] = [False, False, False, False],\n",
    "    ) -> None:\n",
    "        # Initialize backtest object\n",
    "        portfolio = BackTest(\n",
    "            len(Y.test()) - Y.number_of_observation_per_window,\n",
    "            self.num_assets,\n",
    "            Y.test().index[Y.number_of_observation_per_window :],\n",
    "        )\n",
    "\n",
    "        # Initialize lists to store trained parameters\n",
    "        self.gamma_trained = []\n",
    "        self.delta_trained = []\n",
    "\n",
    "        # Store initial split\n",
    "        init_split = Y.split_ratio\n",
    "\n",
    "        # Calculate window size\n",
    "        win_size = init_split[1] / n_roll\n",
    "\n",
    "        split = [0, 0]\n",
    "        t = 0\n",
    "        for i in range(n_roll):\n",
    "\n",
    "            print(f\"Out-of-sample window: {i+1} / {n_roll}\")\n",
    "\n",
    "            split[0] = init_split[0] + win_size * i\n",
    "            if i < n_roll - 1:\n",
    "                split[1] = win_size\n",
    "            else:\n",
    "                split[1] = 1 - split[0]\n",
    "\n",
    "            X.split_update(split)\n",
    "            Y.split_update(split)\n",
    "            train_set = DataLoader(\n",
    "                SlidingWindow(\n",
    "                    X.train(), Y.train(), self.num_observations, self.perf_period\n",
    "                )\n",
    "            )\n",
    "            test_set = DataLoader(\n",
    "                SlidingWindow(X.test(), Y.test(), self.num_observations, 0)\n",
    "            )\n",
    "            if load_state[i]:\n",
    "                # Reset model parameters to initial state\n",
    "                self.load_state_dict(\n",
    "                    torch.load(\n",
    "                        self.cache_path + self.model_name + \"_model_\" + str(i) + \".pt\",\n",
    "                        weights_only=True,\n",
    "                    )\n",
    "                )\n",
    "            # Train the model\n",
    "            self.train()\n",
    "            self.net_train(train_set, lr=lr, epochs=epochs)\n",
    "            # Save the trained model\n",
    "            if save_state[i]:\n",
    "                torch.save(\n",
    "                    self.state_dict(),\n",
    "                    self.cache_path + self.model_name + \"_model_\" + str(i) + \".pt\",\n",
    "                )\n",
    "            self.gamma_trained.append(self.gamma.item())\n",
    "            self.delta_trained.append(self.delta.item())\n",
    "            # Test the model\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                for __, (x, y, y_perf) in enumerate(test_set):\n",
    "                    # Move tensors to the same device as the model\n",
    "                    x = x.to(next(self.parameters()).device)\n",
    "                    y = y.to(next(self.parameters()).device)\n",
    "                    y_perf = y_perf.to(next(self.parameters()).device)\n",
    "\n",
    "                    z_star, _ = self(x.squeeze(), y.squeeze())\n",
    "                    portfolio.weights[t] = z_star.squeeze().cpu().numpy()\n",
    "                    portfolio.rets[t] = (\n",
    "                        y_perf.squeeze().cpu().numpy() @ portfolio.weights[t]\n",
    "                    ).item()\n",
    "                    t += 1\n",
    "\n",
    "        # Reset dataset splits\n",
    "        X.split_update(init_split)\n",
    "        Y.split_update(init_split)\n",
    "\n",
    "        # Calculate portfolio statistics\n",
    "        portfolio.stats()\n",
    "        self.portfolio = portfolio\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    # load_cv_results: Load cross-validation results\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def load_cv_results(self, cv_results):\n",
    "        self.cv_results = cv_results\n",
    "\n",
    "        # Select and store the optimal hyperparameters\n",
    "        idx = cv_results.val_loss.idxmin()\n",
    "        self.lr = cv_results.lr[idx]\n",
    "        self.epochs = cv_results.epochs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "Out-of-sample window: 1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhang\\anaconda3\\envs\\py12\\Lib\\site-packages\\diffcp\\cone_program.py:371: UserWarning: Solved/Inaccurate.\n",
      "  warnings.warn(\"Solved/Inaccurate.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60, Loss: -0.0976\n",
      "Epoch 2/60, Loss: -0.0631\n",
      "Epoch 3/60, Loss: -0.0756\n",
      "Epoch 4/60, Loss: -0.0794\n",
      "Epoch 5/60, Loss: -0.0712\n",
      "Epoch 6/60, Loss: -0.0649\n",
      "Epoch 7/60, Loss: -0.0664\n",
      "Epoch 8/60, Loss: -0.0695\n",
      "Epoch 9/60, Loss: -0.0705\n",
      "Epoch 10/60, Loss: -0.0709\n",
      "Epoch 11/60, Loss: -0.0707\n",
      "Epoch 12/60, Loss: -0.0703\n",
      "Epoch 13/60, Loss: -0.0692\n",
      "Epoch 14/60, Loss: -0.0684\n",
      "Epoch 15/60, Loss: -0.0696\n",
      "Epoch 16/60, Loss: -0.0735\n",
      "Epoch 17/60, Loss: -0.0764\n",
      "Epoch 18/60, Loss: -0.0775\n",
      "Epoch 19/60, Loss: -0.0796\n",
      "Epoch 20/60, Loss: -0.0805\n",
      "Epoch 21/60, Loss: -0.0819\n",
      "Epoch 22/60, Loss: -0.0812\n",
      "Epoch 23/60, Loss: -0.0813\n",
      "Epoch 24/60, Loss: -0.0812\n",
      "Epoch 25/60, Loss: -0.0817\n",
      "Epoch 26/60, Loss: -0.0828\n",
      "Epoch 27/60, Loss: -0.0837\n",
      "Epoch 28/60, Loss: -0.0841\n",
      "Epoch 29/60, Loss: -0.0844\n",
      "Epoch 30/60, Loss: -0.0841\n",
      "Epoch 31/60, Loss: -0.0847\n",
      "Epoch 32/60, Loss: -0.0855\n",
      "Epoch 33/60, Loss: -0.0863\n",
      "Epoch 34/60, Loss: -0.0872\n",
      "Epoch 35/60, Loss: -0.0893\n",
      "Epoch 36/60, Loss: -0.0910\n",
      "Epoch 37/60, Loss: -0.0923\n",
      "Epoch 38/60, Loss: -0.0941\n",
      "Epoch 39/60, Loss: -0.0957\n",
      "Epoch 40/60, Loss: -0.0969\n",
      "Epoch 41/60, Loss: -0.0967\n",
      "Epoch 42/60, Loss: -0.0970\n",
      "Epoch 43/60, Loss: -0.0976\n",
      "Epoch 44/60, Loss: -0.0979\n",
      "Epoch 45/60, Loss: -0.0985\n",
      "Epoch 46/60, Loss: -0.0991\n",
      "Epoch 47/60, Loss: -0.0991\n",
      "Epoch 48/60, Loss: -0.0995\n",
      "Epoch 49/60, Loss: -0.1006\n",
      "Epoch 50/60, Loss: -0.1016\n",
      "Epoch 51/60, Loss: -0.1017\n",
      "Epoch 52/60, Loss: -0.1014\n",
      "Epoch 53/60, Loss: -0.1016\n",
      "Epoch 54/60, Loss: -0.1018\n",
      "Epoch 55/60, Loss: -0.1020\n",
      "Epoch 56/60, Loss: -0.1026\n",
      "Epoch 57/60, Loss: -0.1024\n",
      "Epoch 58/60, Loss: -0.1019\n",
      "Epoch 59/60, Loss: -0.1028\n",
      "Epoch 60/60, Loss: -0.1027\n",
      "Out-of-sample window: 2 / 4\n",
      "Epoch 1/60, Loss: -0.1007\n",
      "Epoch 2/60, Loss: -0.0543\n",
      "Epoch 3/60, Loss: -0.0674\n",
      "Epoch 4/60, Loss: -0.0784\n",
      "Epoch 5/60, Loss: -0.0726\n",
      "Epoch 6/60, Loss: -0.0788\n",
      "Epoch 7/60, Loss: -0.0838\n",
      "Epoch 8/60, Loss: -0.0900\n",
      "Epoch 9/60, Loss: -0.0919\n",
      "Epoch 10/60, Loss: -0.0956\n",
      "Epoch 11/60, Loss: -0.0986\n",
      "Epoch 12/60, Loss: -0.0977\n",
      "Epoch 13/60, Loss: -0.0975\n",
      "Epoch 14/60, Loss: -0.1001\n",
      "Epoch 15/60, Loss: -0.1011\n",
      "Epoch 16/60, Loss: -0.1004\n",
      "Epoch 17/60, Loss: -0.0989\n",
      "Epoch 18/60, Loss: -0.0963\n",
      "Epoch 19/60, Loss: -0.0942\n",
      "Epoch 20/60, Loss: -0.0948\n",
      "Epoch 21/60, Loss: -0.0952\n",
      "Epoch 22/60, Loss: -0.0956\n",
      "Epoch 23/60, Loss: -0.0960\n",
      "Epoch 24/60, Loss: -0.0952\n",
      "Epoch 25/60, Loss: -0.0949\n",
      "Epoch 26/60, Loss: -0.0956\n",
      "Epoch 27/60, Loss: -0.0958\n",
      "Epoch 28/60, Loss: -0.0962\n",
      "Epoch 29/60, Loss: -0.0963\n",
      "Epoch 30/60, Loss: -0.0965\n",
      "Epoch 31/60, Loss: -0.0968\n",
      "Epoch 32/60, Loss: -0.0969\n",
      "Epoch 33/60, Loss: -0.0969\n",
      "Epoch 34/60, Loss: -0.0962\n",
      "Epoch 35/60, Loss: -0.0963\n",
      "Epoch 36/60, Loss: -0.0972\n",
      "Epoch 37/60, Loss: -0.0985\n",
      "Epoch 38/60, Loss: -0.0995\n",
      "Epoch 39/60, Loss: -0.1001\n",
      "Epoch 40/60, Loss: -0.1006\n",
      "Epoch 41/60, Loss: -0.1010\n",
      "Epoch 42/60, Loss: -0.1002\n",
      "Epoch 43/60, Loss: -0.1004\n",
      "Epoch 44/60, Loss: -0.1000\n",
      "Epoch 45/60, Loss: -0.1006\n",
      "Epoch 46/60, Loss: -0.1008\n",
      "Epoch 47/60, Loss: -0.1005\n",
      "Epoch 48/60, Loss: -0.1001\n",
      "Epoch 49/60, Loss: -0.1000\n",
      "Epoch 50/60, Loss: -0.1000\n",
      "Epoch 51/60, Loss: -0.1004\n",
      "Epoch 52/60, Loss: -0.1013\n",
      "Epoch 53/60, Loss: -0.1020\n",
      "Epoch 54/60, Loss: -0.1024\n",
      "Epoch 55/60, Loss: -0.1023\n",
      "Epoch 56/60, Loss: -0.1025\n",
      "Epoch 57/60, Loss: -0.1030\n",
      "Epoch 58/60, Loss: -0.1029\n",
      "Epoch 59/60, Loss: -0.1031\n",
      "Epoch 60/60, Loss: -0.1028\n",
      "Out-of-sample window: 3 / 4\n",
      "Epoch 1/60, Loss: -0.1038\n",
      "Epoch 2/60, Loss: -0.0422\n",
      "Epoch 3/60, Loss: -0.0693\n",
      "Epoch 4/60, Loss: -0.0707\n",
      "Epoch 5/60, Loss: -0.0736\n",
      "Epoch 6/60, Loss: -0.0763\n",
      "Epoch 7/60, Loss: -0.0791\n",
      "Epoch 8/60, Loss: -0.0818\n",
      "Epoch 9/60, Loss: -0.0823\n",
      "Epoch 10/60, Loss: -0.0846\n",
      "Epoch 11/60, Loss: -0.0841\n",
      "Epoch 12/60, Loss: -0.0783\n",
      "Epoch 13/60, Loss: -0.0794\n",
      "Epoch 14/60, Loss: -0.0803\n",
      "Epoch 15/60, Loss: -0.0784\n",
      "Epoch 16/60, Loss: -0.0812\n",
      "Epoch 17/60, Loss: -0.0823\n",
      "Epoch 18/60, Loss: -0.0815\n",
      "Epoch 19/60, Loss: -0.0797\n",
      "Epoch 20/60, Loss: -0.0779\n",
      "Epoch 21/60, Loss: -0.0767\n",
      "Epoch 22/60, Loss: -0.0772\n",
      "Epoch 23/60, Loss: -0.0766\n",
      "Epoch 24/60, Loss: -0.0768\n",
      "Epoch 25/60, Loss: -0.0786\n",
      "Epoch 26/60, Loss: -0.0778\n",
      "Epoch 27/60, Loss: -0.0779\n",
      "Epoch 28/60, Loss: -0.0765\n",
      "Epoch 29/60, Loss: -0.0760\n",
      "Epoch 30/60, Loss: -0.0767\n",
      "Epoch 31/60, Loss: -0.0808\n",
      "Epoch 32/60, Loss: -0.0847\n",
      "Epoch 33/60, Loss: -0.0862\n",
      "Epoch 34/60, Loss: -0.0895\n",
      "Epoch 35/60, Loss: -0.0904\n",
      "Epoch 36/60, Loss: -0.0897\n",
      "Epoch 37/60, Loss: -0.0908\n",
      "Epoch 38/60, Loss: -0.0897\n",
      "Epoch 39/60, Loss: -0.0882\n",
      "Epoch 40/60, Loss: -0.0896\n",
      "Epoch 41/60, Loss: -0.0920\n",
      "Epoch 42/60, Loss: -0.0952\n",
      "Epoch 43/60, Loss: -0.0956\n",
      "Epoch 44/60, Loss: -0.0966\n",
      "Epoch 45/60, Loss: -0.0987\n",
      "Epoch 46/60, Loss: -0.0990\n",
      "Epoch 47/60, Loss: -0.0963\n",
      "Epoch 48/60, Loss: -0.0947\n",
      "Epoch 49/60, Loss: -0.0947\n",
      "Epoch 50/60, Loss: -0.0955\n",
      "Epoch 51/60, Loss: -0.0952\n",
      "Epoch 52/60, Loss: -0.0951\n",
      "Epoch 53/60, Loss: -0.0936\n",
      "Epoch 54/60, Loss: -0.0963\n",
      "Epoch 55/60, Loss: -0.0985\n",
      "Epoch 56/60, Loss: -0.0976\n",
      "Epoch 57/60, Loss: -0.0970\n",
      "Epoch 58/60, Loss: -0.0932\n",
      "Epoch 59/60, Loss: -0.0952\n",
      "Epoch 60/60, Loss: -0.0999\n",
      "Out-of-sample window: 4 / 4\n",
      "Epoch 1/60, Loss: -0.0878\n",
      "Epoch 2/60, Loss: -0.0455\n",
      "Epoch 3/60, Loss: -0.0522\n",
      "Epoch 4/60, Loss: -0.0557\n",
      "Epoch 5/60, Loss: -0.0592\n",
      "Epoch 6/60, Loss: -0.0650\n",
      "Epoch 7/60, Loss: -0.0576\n",
      "Epoch 8/60, Loss: -0.0495\n",
      "Epoch 9/60, Loss: -0.0474\n",
      "Epoch 10/60, Loss: -0.0474\n",
      "Epoch 11/60, Loss: -0.0460\n",
      "Epoch 12/60, Loss: -0.0420\n",
      "Epoch 13/60, Loss: -0.0416\n",
      "Epoch 14/60, Loss: -0.0474\n",
      "Epoch 15/60, Loss: -0.0501\n",
      "Epoch 16/60, Loss: -0.0492\n",
      "Epoch 17/60, Loss: -0.0481\n",
      "Epoch 18/60, Loss: -0.0456\n",
      "Epoch 19/60, Loss: -0.0459\n",
      "Epoch 20/60, Loss: -0.0480\n",
      "Epoch 21/60, Loss: -0.0487\n",
      "Epoch 22/60, Loss: -0.0492\n",
      "Epoch 23/60, Loss: -0.0509\n",
      "Epoch 24/60, Loss: -0.0517\n",
      "Epoch 25/60, Loss: -0.0510\n",
      "Epoch 26/60, Loss: -0.0496\n",
      "Epoch 27/60, Loss: -0.0479\n",
      "Epoch 28/60, Loss: -0.0461\n",
      "Epoch 29/60, Loss: -0.0453\n",
      "Epoch 30/60, Loss: -0.0448\n",
      "Epoch 31/60, Loss: -0.0446\n",
      "Epoch 32/60, Loss: -0.0447\n",
      "Epoch 33/60, Loss: -0.0455\n",
      "Epoch 34/60, Loss: -0.0463\n",
      "Epoch 35/60, Loss: -0.0476\n",
      "Epoch 36/60, Loss: -0.0481\n",
      "Epoch 37/60, Loss: -0.0486\n",
      "Epoch 38/60, Loss: -0.0484\n",
      "Epoch 39/60, Loss: -0.0478\n",
      "Epoch 40/60, Loss: -0.0487\n",
      "Epoch 41/60, Loss: -0.0494\n",
      "Epoch 42/60, Loss: -0.0519\n",
      "Epoch 43/60, Loss: -0.0529\n",
      "Epoch 44/60, Loss: -0.0534\n",
      "Epoch 45/60, Loss: -0.0548\n",
      "Epoch 46/60, Loss: -0.0546\n",
      "Epoch 47/60, Loss: -0.0544\n",
      "Epoch 48/60, Loss: -0.0561\n",
      "Epoch 49/60, Loss: -0.0555\n",
      "Epoch 50/60, Loss: -0.0555\n",
      "Epoch 51/60, Loss: -0.0544\n",
      "Epoch 52/60, Loss: -0.0540\n",
      "Epoch 53/60, Loss: -0.0542\n",
      "Epoch 54/60, Loss: -0.0541\n",
      "Epoch 55/60, Loss: -0.0540\n",
      "Epoch 56/60, Loss: -0.0541\n",
      "Epoch 57/60, Loss: -0.0548\n",
      "Epoch 58/60, Loss: -0.0549\n",
      "Epoch 59/60, Loss: -0.0557\n",
      "Epoch 60/60, Loss: -0.0549\n"
     ]
    }
   ],
   "source": [
    "n_roll: int = 4\n",
    "dr_net_eps = E2E_net_Eps_Control(\n",
    "    num_input_features=n_X,\n",
    "    num_assets=n_Y,\n",
    "    num_observations=number_of_observe_per_window,\n",
    "    prisk=\"p_var\",\n",
    "    train_pred=True,\n",
    "    train_gamma=True,\n",
    "    train_delta=True,\n",
    "    set_seed=19260817,\n",
    "    optimization_layer=\"hellinger\",\n",
    "    performance_objective=\"sharpe_loss\",\n",
    "    cache_path=\"./cache/\",\n",
    "    performance_period=13,\n",
    "    prediction_loss_factor=0.5,\n",
    "    self_overall_std_dev_factor=1,\n",
    ").double()\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "dr_net_eps.to(device)\n",
    "dr_net_eps.net_roll_test(\n",
    "    X_data,\n",
    "    Y_data,\n",
    "    n_roll=n_roll,\n",
    "    lr=0.005,\n",
    "    epochs=60,\n",
    "    load_state=[True] * (n_roll),\n",
    "    save_state=[True] * (n_roll),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAJOCAYAAABm7rQwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAADBjElEQVR4nOzdeZyN5f/H8feZDYOxL2Pfk2UikiVLdrKFEpVWIUsSSSVLROpbKkUrLUTfSMoS2UIoUSR8o7FlxlIx2caZmfP74/rdc2bMds7MOXNmzryej8c8rvvc516uM9eo8XZdn9vmcDgcAgAAAAAAALJRgK87AAAAAAAAgLyHUAoAAAAAAADZjlAKAAAAAAAA2Y5QCgAAAAAAANmOUAoAAAAAAADZjlAKAAAAAAAA2Y5QCgAAAAAAANmOUAoAAAAAAADZjlAKAAAAAAAA2Y5QCgAAP7Bnzx498MADqlq1qvLnz69ChQrpxhtv1MyZM/X333/7unvpmjRpkmw2W6bOXblypSZNmpTqe1WqVNH999+f+Y5lUps2bWSz2RK/8ufPrzp16mjq1Km6evVqpq65cOFCzZo1y7Md9bDjx49r+PDhql69uvLnz69ixYqpTZs2WrBggRwOh6+7l8j6ecvoq02bNjpy5IhsNpvmz5/v624DAOCXgnzdAQAAkDXvvvuuHn30UV133XUaO3as6tSpI7vdrp07d2ru3Lnatm2bvvjiC1930ytWrlypN998M9Vg6osvvlBYWFj2d0pStWrVtGDBAknSmTNn9N5772nChAk6duyY3nnnHbevt3DhQv36668aNWqUh3vqGVu3blW3bt1UqFAhjR07VhERETp//rw+++wz3XPPPfrqq6+0cOFCBQT4/t9DH374YXXu3DnxdVRUlHr37q0RI0ZowIABifvDwsIUHh6ubdu2qXr16r7oKgAAfo9QCgCAXGzbtm0aOnSoOnTooGXLlilfvnyJ73Xo0EFPPPGEVq9e7cMe+k7Dhg19du8CBQqoadOmia+7dOmiOnXq6MMPP9Trr7+u/Pnz+6xvSV26dEmhoaFZusa5c+fUu3dvFSlSRDt27FCZMmUS3+vZs6ciIiL01FNPqUGDBnrqqaey2mWXxcfHKy4uLtmfCUmqUKGCKlSokPj6yJEjkqRKlSolGzNLavsAAIBn+P6fqwAAQKa98MILstlseuedd1L85VuSQkJC1KNHj8TXNpst1VlF1y51mz9/vmw2m9avX69BgwapRIkSCgsL08CBA3Xx4kVFR0frzjvvVNGiRRUeHq4xY8bIbrcnnr9x40bZbDZt3Lgx2X1cXQ61ePFidezYUeHh4SpQoICuv/56PfXUU7p48WLiMffff7/efPPNxM9lfVkhQ9LPdObMGYWEhGjChAkp7nXgwAHZbDa9/vrrifuio6M1ePBgVahQQSEhIapataomT56suLi4dPudlqCgIDVo0EBXr17VuXPnEvc7HA699dZbatCggQoUKKBixYqpb9+++uOPPxKPadOmjVasWKGjR48m+5ySe9/n+++/X4UKFdLevXvVsWNHFS5cWO3atUv8/g0fPlwff/yxrr/+eoWGhuqGG27Q119/neFne++993T69GnNmDEjWSBlefLJJ1W7dm299NJLstvtXhkL6/POnDlTU6dOVdWqVZUvXz5t2LAhw/6nJ7Xvo7X8b8+ePbrjjjtUpEgRFS9eXKNHj1ZcXJwOHjyozp07q3DhwqpSpYpmzpyZ4roxMTEaM2aMqlatqpCQEJUvX16jRo1K9vMNAEBewEwpAAByqfj4eK1fv16NGjVSxYoVvXKPhx9+WL1799aiRYu0e/duPf3004l/8e7du7ceeeQRffvtt3rxxRdVrlw5jR492iP3/f3339W1a1eNGjVKBQsW1IEDB/Tiiy/qhx9+0Pr16yVJEyZM0MWLF/X5559r27ZtieeGh4enuF6pUqXUrVs3ffjhh5o8eXKyZWTz5s1TSEiI7r77bkkmBGnSpIkCAgL03HPPqXr16tq2bZumTp2qI0eOaN68eZn6TJGRkSpatKhKlSqVuG/w4MGaP3++Ro4cqRdffFF///23pkyZoubNm+uXX35RmTJl9NZbb+mRRx7R4cOHs7wM8+rVq+rRo4cGDx6sp556Klmws2LFCv3444+aMmWKChUqpJkzZ+r222/XwYMHVa1atTSvuXbtWgUGBqp79+6pvm+z2dSjRw/NnDlTP/30k5o2beq1sXj99ddVq1YtvfzyywoLC1PNmjWz8u1K15133ql77rlHgwcP1tq1azVz5kzZ7XZ9++23evTRRzVmzBgtXLhQ48aNU40aNdS7d29JZnZa69atdeLECT399NOKiIjQvn379Nxzz2nv3r369ttvM11jDQCAXMcBAABypejoaIckx1133eXyOZIcEydOTLG/cuXKjvvuuy/x9bx58xySHCNGjEh2XK9evRySHK+88kqy/Q0aNHDceOONia83bNjgkOTYsGFDsuMiIyMdkhzz5s1L3Ddx4kRHer+SJCQkOOx2u2PTpk0OSY5ffvkl8b1hw4alee61n2n58uUOSY41a9Yk7ouLi3OUK1fO0adPn8R9gwcPdhQqVMhx9OjRZNd7+eWXHZIc+/btS7OvDofD0bp1a0fdunUddrvdYbfbHVFRUY7nnnvOIckxd+7cxOO2bdvmkOT4z3/+k+z848ePOwoUKOB48sknE/fddtttjsqVK6e4lzvf5/vuu88hyfHBBx+kuI4kR5kyZRwxMTGJ+6Kjox0BAQGO6dOnp/t5a9eu7Shbtmy6x8yZM8chybF48WKHw+H5sbA+b/Xq1R1Xr15Nty/Xss596aWX0nwvtZ/Xa8etQYMGDkmOpUuXJu6z2+2OUqVKOXr37p24b/r06Y6AgADHjz/+mOz8zz//3CHJsXLlSrf6DwBAbsbyPQAAkKZu3bole3399ddLkm677bYU+48ePeqx+/7xxx8aMGCAypYtq8DAQAUHB6t169aSpP3792fqml26dFHZsmWTza755ptvdPLkST344IOJ+77++mvdeuutKleunOLi4hK/unTpIknatGlThvfat2+fgoODFRwcrPDwcE2ZMkXjx4/X4MGDk93HZrPpnnvuSXafsmXL6oYbbkixJM9T+vTpk+r+W2+9VYULF058XaZMGZUuXdoj4+r4/6fvWTOAvDUWPXr0UHBwcJb764rU/mzYbLbEvklm2WaNGjWSfQ+//vpr1atXTw0aNEj2mTp16pTqUkwAAPwZy/cAAMilSpYsqdDQUEVGRnrtHsWLF0/2OiQkJM39V65c8cg9L1y4oJYtWyp//vyaOnWqatWqpdDQUB0/fly9e/fW5cuXM3XdoKAg3XvvvXrjjTd07tw5FS1aVPPnz1d4eLg6deqUeNypU6f01VdfpRlunD17NsN7Va9eXYsWLZLD4dDRo0c1depUTZ8+XREREbrrrrsS7+NwOFKtwyQp3SVzmRUaGprmEwlLlCiRYl++fPky/H5XqlRJv//+uy5evKiCBQumeoxV58taZuqtsUht6aa3pPZnIDQ0NEUR+5CQEMXExCS+PnXqlA4dOpSlny8AAPwFoRQAALlUYGCg2rVrp1WrVunEiRPJniiWlnz58ik2NjbF/r/++sujfbP+Yn7tvVz5C/f69et18uRJbdy4MXF2lKRkBcIz64EHHtBLL72kRYsWqV+/flq+fLlGjRqlwMDAxGNKliypiIgITZs2LdVrlCtXLsP75M+fX40bN5Yk3XTTTbr11ltVt25djRo1St26dVOhQoVUsmRJ2Ww2bd68OdUi9antS+0+kuvfZ2/UKurQoYPWrFmjr776KjFwS8rhcGj58uUqXry4GjVqlLjfG2ORG2oxlSxZUgUKFNAHH3yQ5vsAAOQVhFIAAORi48eP18qVKzVo0CB9+eWXiTOZLHa7XatXr04sQl2lShXt2bMn2THr16/XhQsXPNqvKlWqSJL27NmTbObL8uXLMzzXChauDWXefvvtFMdax1y+fFkFChTI8NrXX3+9br75Zs2bN0/x8fGKjY3VAw88kOyYbt26aeXKlapevbqKFSuW4TVdUaJECc2YMUMPPPCA3njjDY0fP17dunXTjBkz9Oeff+rOO+9M9/y0Zixl5fvsKQ8//LBeeukljR8/Xm3btlXp0qWTvT9z5kwdOHBAM2bMSDY7yFdj4WvdunXTCy+8oBIlSqhq1aq+7g4AAD5FKAUAQC7WrFkzzZkzR48++qgaNWqkoUOHqm7durLb7dq9e7feeecd1atXLzGUuvfeezVhwgQ999xzat26tX777TfNnj1bRYoU8Wi/ypYtq/bt22v69OkqVqyYKleurHXr1mnp0qUZntu8eXMVK1ZMQ4YM0cSJExUcHKwFCxbol19+SXFs/fr1JUkvvviiunTposDAQEVERKQI55J68MEHNXjwYJ08eVLNmzfXddddl+z9KVOmaO3atWrevLlGjhyp6667TleuXNGRI0e0cuVKzZ0716VZadcaOHCgXnnlFb388ssaNmyYWrRooUceeUQPPPCAdu7cqVatWqlgwYKKiorSli1bVL9+fQ0dOjTxcy5dulRz5sxRo0aNFBAQoMaNG2fp++wpRYsW1dKlS9WtWzc1atRIY8eO1Q033KCYmBgtXrxYCxYsUL9+/TR27NgU5/pqLHxp1KhRWrJkiVq1aqXHH39cERERSkhI0LFjx7RmzRo98cQTuvnmm33dTQAAsgWhFAAAudygQYPUpEkTvfrqq3rxxRcVHR2t4OBg1apVSwMGDNDw4cMTjx07dqxiYmI0f/58vfzyy2rSpIk+++wz9ezZ0+P9+vjjjzVixAiNGzdO8fHx6t69uz799NPEZW1pKVGihFasWKEnnnhC99xzjwoWLKiePXtq8eLFuvHGG5MdO2DAAG3dulVvvfWWpkyZIofDocjIyMQZRKm56667NGrUKJ04cUITJ05M8X54eLh27typ559/Xi+99JJOnDihwoULq2rVqurcuXOmZ+wEBARoxowZuu222zRr1iw999xzevvtt9W0aVO9/fbbeuutt5SQkKBy5cqpRYsWatKkSeK5jz32mPbt26enn35a58+fl8PhSCwentnvsye1aNFCe/bs0YsvvqjXXntNJ06cUIECBXTDDTfok08+0YABA1JdWuersfClggULavPmzZoxY4beeecdRUZGqkCBAqpUqZLat2+f7s8uAAD+xuawfqMBAAAAAAAAskmArzsAAAAAAACAvIdQCgAAAAAAANmOUAoAAAAAAADZjlAKAAAAAAAA2Y5QCgAAAAAAANmOUAoAAAAAAADZLsjXHchuCQkJOnnypAoXLiybzebr7gAAAAAAAPgVh8Ohf//9V+XKlVNAQNrzofJcKHXy5ElVrFjR190AAAAAAADwa8ePH1eFChXSfD/PhVKFCxeWZL4xYWFhPu5Nxux2u9asWaOOHTsqODjY192BBzCm/ocx9T+Mqf9hTP0PY+p/GFP/w5j6J8bV/3hjTGNiYlSxYsXEDCYteS6UspbshYWF5ZpQKjQ0VGFhYfyB9xOMqf9hTP0PY+p/GFP/w5j6H8bU/zCm/olx9T/eHNOMyiZR6BwAAAAAAADZjlAKAAAAAAAA2Y5QCgAAAAAAANkuz9WUclV8fLzsdruvuyG73a6goCBduXJF8fHxvu4OPCA7xjQ4OFiBgYFeuTYAAAAAAJ5AKHUNh8Oh6OhonTt3ztddkWT6U7ZsWR0/fjzDAmHIHbJrTIsWLaqyZcvycwMAAAAAyJEIpa5hBVKlS5dWaGioz/9Cn5CQoAsXLqhQoUIKCGC1pT/w9pg6HA5dunRJp0+fliSFh4d7/B4AAAAAAGQVoVQS8fHxiYFUiRIlfN0dSSbAuHr1qvLnz08o5SeyY0wLFCggSTp9+rRKly7NUj4AAAAAQI5DypGEVUMqNDTUxz0Bss76Oc4JtdEAAAAAALgWoVQqfL1kD/AEfo4BAAAAADkZoRQAAAAAAACyHaEUso3NZtOyZctyzHUAAAAAAIDvEEr5kejoaI0YMULVqlVTvnz5VLFiRXXv3l3r1q3zddcyZdKkSWrQoEGK/VFRUerSpYtX712lShXZbDbZbDYVKFBAtWvX1ksvvSSHw+HyNebPn6+iRYt6r5MAAAAAAORiPH3PTxw5ckQtWrRQ0aJFNXPmTEVERMhut+ubb77RsGHDdODAAV930WPKli2bLfeZMmWKBg0apCtXrujbb7/V0KFDFRYWpsGDB2fL/ZOy2+0KDg7O9vsCAAAAAOAtzJTyE48++qhsNpt++OEH9e3bV7Vq1VLdunU1evRobd++XZIJrmw2m37++efE886dOyebzaaNGzdKkjZu3CibzaZvvvlGDRs2VIECBdS2bVudPn1aq1at0vXXX6+wsDD1799fly5dSrxOlSpVNGvWrGR9atCggSZNmpRmn8eNG6datWopNDRU1apV04QJExKfFDd//nxNnjxZv/zyS+KMpfnz50tKvnyvWbNmeuqpp5Jd98yZMwoODtaGDRskSVevXtWTTz6p8uXLq2DBgrr55psTP296ChcurLJly6pKlSp6+OGHFRERoTVr1iS+n951N27cqAceeEDnz59P7L/1vQgMDNSKFSuS3ato0aKJn88ap88++0xt2rRR/vz59cknn+j+++9Xr1699PLLLys8PFwlSpTQsGHDeLoeAAAAACBXYqZUBhwOKUn2kq1CQ1077u+//9bq1as1bdo0FSxYMMX7mVlCNmnSJM2ePVuhoaG68847deeddypfvnxauHChLly4oNtvv11vvPGGxo0b5/a1LYULF9b8+fNVrlw57d27V4MGDVLhwoX15JNPql+/fvr111+1evVqffvtt5KkIkWKpLjG3XffrZdeeknTp09PfNrc4sWLVaZMGbVu3VqS9MADD+jIkSNatGiRypUrpy+++EKdO3fW3r17VbNmzQz76XA4tGnTJu3fvz/Z8eldt3nz5po1a5aee+45HTx4UJJUqFAht74/48aN03/+8x/NmzdP+fLl06ZNm7RhwwaFh4drw4YNOnTokPr166cGDRpo0KBBbl0bAAAAAABfI5TKwKVLkptZgsdcuCAVKJDxcYcOHZLD4VDt2rU9du+pU6eqRYsWkqSHHnpI48eP1+HDh1WtWjVJUt++fbVhw4YshVLPPvts4naVKlX0xBNPaPHixXryySdVoEABFSpUSEFBQeku1+vXr58ef/xxbdmyRS1btpQkLVy4UAMGDFBAQIAOHz6sTz/9VCdOnFC5cuUkSWPGjNHq1as1b948vfDCC2lee9y4cXr22Wd19epV2e125c+fXyNHjpQkl65bpEgR2Wy2TC83HDVqlHr37p1sX7FixTR79mwFBgaqdu3auu2227Ru3TpCKQAAAABArkMo5Qes4tvWTCFPiIiISNwuU6ZM4hK7pPt++OGHLN3j888/16xZs3To0CFduHBBcXFxCgsLc+sapUqVUocOHbRgwQK1bNlSkZGR2rZtm+bMmSNJ2rVrlxwOh2rVqpXsvNjYWJUoUSLda48dO1b333+/zpw5o2eeeUZt27ZV8+bNs3xdVzVu3DjFvrp16yowMDDxdXh4uPbu3euR+wEAAAAAkJ0IpTIQGmpmLPnq3q487K1mzZqy2Wzav3+/evXqleZxAQGmhFjSJ8ilVY8oaVFtm82Wosi2zWZTQkJCsmtf+2S69Godbd++XXfddZcmT56sTp06qUiRIlq0aJH+85//pHlOWu6++2499thjeuONN7Rw4ULVrVtXN9xwgyQpISFBgYGB+umnn5KFOVLGy+lKliypGjVqqEaNGlqyZIlq1Kihpk2bqn379lm6rs1mc+l7ldpSzIzGAQAAAACA3IJQKgM2m5RKNpBtXAmlihcvrk6dOunNN9/UyJEjU4QZ586dU9GiRVWqVClJUlRUlBo2bChJyYqeZ0WpUqUUFRWV+DomJkaRkZFpHr9161ZVrlxZzzzzTOK+o0ePJjsmJCRE8fHxGd67V69eGjx4sFavXq2FCxfq3nvvTXyvYcOGio+P1+nTpxOX92VGsWLFNGLECI0ZM0a7d+926bpp9b9UqVKKjo5OfP37778nKxoPAAAAAEBewNP3/MRbb72l+Ph4NWnSREuWLNHvv/+u/fv36/XXX1ezZs0kSQUKFFDTpk01Y8YM/fbbb/ruu++S1XXKirZt2+rjjz/W5s2b9euvv+q+++5LMYMoqRo1aujYsWNatGiRDh8+rNdff11ffPFFsmOqVKmiyMhI/fzzzzp79qxiY2NTvVbBggXVs2dPTZgwQfv379eAAQMS36tVq5buvvtuDRw4UEuXLlVkZKR+/PFHvfjii1q5cqVbn3HYsGE6ePCglixZ4tJ1q1SpogsXLmjdunU6e/ZsYvB066236r333tOuXbu0c+dODRkyJMUMKAAAAABA5q1cKY0aJV254uueID2EUn6iatWq2rVrl2699VY98cQTqlevnjp06KB169Yl1leSpA8++EB2u12NGzfWY489pqlTp3rk/uPHj1erVq3UrVs3de3aVb169VL16tXTPL5nz556/PHHNXz4cDVo0EDff/+9JkyYkOyYPn36qHPnzrr11ltVqlQpffrpp2le7+6779Yvv/yili1bqlKlSsnemzdvngYOHKgnnnhC1113nXr06KEdO3aoYsWKbn3GUqVK6d5779WkSZOUkJCQ4XWbN2+uIUOGqF+/fipVqpRmzpwpSXr55ZdVvnx5tWnTRgMGDNCYMWMU6uqjFgEAAAAA6Tp4UOrTR3rtNemTT3zdG6TH5ri2uI2fi4mJUZEiRXT+/PkURbWvXLmiyMhIVa1aVfnz5/dRD5NLSEhQTEyMwsLCEmtCIXfLrjHNiT/P/sput2vlypXq2rUrs978BGPqfxhT/8OY+h/G1P8wpv4pp49rXJx0yy3Sjh3mdbdu0ldf+bZPOZ03xjS97CUpUg4AAAAAAOAXZs40gVSBAub12rW+e3gZMkYoBQAAAAAAcr0rV6Tnnzfbc+ZINWpIsbHSN9/4tl9IG6EUAAAAAADI9Y4cMcFU4cLSwIFSr15m/zXP1EIOQigFAAAAAAByvT/+MG21apLN5gylvv5astt91i2kg1AKAAAAAADkepGRpq1a1bRNm0qlS0vnz0ubNvmuX0gboVQqEhISfN0FIMv4OQYAAACQlySdKSVJgYFSjx5me+xYKSZGcjikZcukjRt90UNcK8jXHchJQkJCFBAQoJMnT6pUqVIKCQmRzWbzaZ8SEhJ09epVXblyRQEBZIj+wNtj6nA4dPXqVZ05c0YBAQEKCQnx+D0AAAAAIKe5NpSSpGeekZYvl37+Wbr9dhNUrV0rBQdLx45JZcv6pKv4f4RSSQQEBKhq1aqKiorSyZMnfd0dSSZguHz5sgoUKODzgAyekV1jGhoaqkqVKhFmAgAAAMgTrl2+J0lVqkgrV0pt2kjr1zv32+3SkiXSsGHZ2UNci1DqGiEhIapUqZLi4uIUHx/v6+7Ibrfru+++U6tWrRQcHOzr7sADsmNMAwMDFRQURJAJAAAAIE9wOFKfKSVJjRqZAKpvX6l+fenGG6XZs6XFiwmlfI1QKhU2m03BwcE5IgQKDAxUXFyc8ufPnyP6g6xjTAEAAADAs/76S/r3X7NdpUrK9zt2lM6elUJCpOPHTSi1ZYt04oRUoUK2dhVJsK4HAAAAAADkatYsqXLlpPz5Uz/GKrdbsaLUooWZXfXf/2ZP/5A6QikAAAAAAJBr/POPtHt38n1WPalrl+6lpV8/0y5e7Ll+wX2EUgAAAAAAINe4915TFyrpLKe06kmlpW9fyWaTduyQjhxJ+7j166WZM6W4uEx3F+kglAIAAAAAALlCTIz0zTdme+xY6coVs22FUkmfvJee8HCpcWOz/eOPqR+zYYPUubM0bpz07ruZ7zPSRigFAAAAAAByhW+/dc5aOnpUev11s+3u8j1JqlXLtIcPp3xv3z7p9tslu928fv556fJl6cwZ6bHHpO++y1z/kRyhFAAAAAAgx4mNlT74QDp1ytc9QU6ycqVpq1c37bRpJihyd6ZU0mtY51piY6Xu3aXz56XmzaXKlaWoKOmFF8zMqddfl4YOzdrngEEoBQAAAADIcZ56SnroIRMEAJJ5Wt7q1WZ79mypYUOznK9PH+nYMbPfnZlS1rHXzpT69lsz86p0aWn5cmnSJLN/6lRp1y6z/dtv0qFDmf4o+H+EUgAAAACAHCUqSpo712xHR/u2L8g59u6V/vxTCg2V2rSR3nlHCguTNm+W4uOlfPlMrShXpTVTaskS095xh1SihHTPPdJ115l9RYpIdeqY7eXLs/RxoBwUSk2fPl02m02jRo1K97hNmzapUaNGyp8/v6pVq6a51n+pAAAAAAB+YeZMZwHry5d92xfkHNbSvVtvlfLnN4XKt2yRypc3+6tWlQLcSDmsmVLHjjlrR9nt0pdfmu2+fU0bFCS9957UqZOZqTV4sNlPKJV1OSKU+vHHH/XOO+8oIiIi3eMiIyPVtWtXtWzZUrt379bTTz+tkSNHaokVYwIAAAAAcrWks6QkQik4rVpl2i5dnPvq15e2b5fuvluaMsW964WHm3ArIcEUTZekjRulv/+WSpWSWrZ0HnvLLSaQatpU6tHD7Nu8Wfrrr0x/HCgHhFIXLlzQ3XffrXfffVfFihVL99i5c+eqUqVKmjVrlq6//no9/PDDevDBB/Xyyy9nU28BAAAAAN708stmllRIiHlNKAXJBFKbN5vtpKGUJFWoIH3yiVlu5w6bzTlbylrCZ8156dVLCgxM/bwqVaSICBNmrVjh3G+FVsuWudePvCzI1x0YNmyYbrvtNrVv315Tp05N99ht27apY8eOyfZ16tRJ77//vux2u4KDg1OcExsbq9jY2MTXMTExkiS73S67NT8vB7P6mBv6Ctcwpv6HMfU/jKn/YUz9D2PqfxhT/5OZMb10SXr//SBJNj30ULzmzAnUpUsO2e1xXuol3OWLP6sHDkh33RUkh8Omhx5KUMWK8fLU7atWDdRvvwXo99/j1apVgr74wvz89ewZJ7vdkeZ53boFaM+eQC1blqD+/eMlSRMmBGrnzgDdfrs0dmy8Jk9OUJDPU5eMeWNMXb2WT789ixYt0q5du/Tjjz+6dHx0dLTKlCmTbF+ZMmUUFxens2fPKjyVimbTp0/X5MmTU+xfs2aNQkNDM9dxH1i7dq2vuwAPY0z9D2PqfxhT/8OY+h/G1P8wpv7HnTFdt66izp+/UWXKXFTZsj9LaqEzZy5o5cr1XusfMsfbf1YTEqRdu0rr4MHi2rixomJignX99X+pc+etWrky7bDIXTZbPUnV9e23f+j8+VM6ffoWFSp0VVeurE73PsWLF5HURitXJuizz9YqJiZEO3e2T3z/pZcCtW/fUT3yyF5J5rMcORKm228/JJvNY933KE+O6aVLl1w6zmeh1PHjx/XYY49pzZo1yp8/v8vn2a4ZPYfDkep+y/jx4zV69OjE1zExMapYsaI6duyosLCwTPQ8e9ntdq1du1YdOnRIdSYYch/G1P8wpv6HMfU/jKn/YUz9D2PqfzIzptOnm/VSw4fnV5s2N2viRCkoqJC6du3qza7CDdn1Z3XOnABNnepcP1e5skNr14apdOku6ZzlvsjIAH39tSRV18mTZi3f7bcHqUeP9O/jcEjz5zv0669BOny4U+LMrU6dEnTXXQl64IEgffttVb39dkUFBkoDBgTp0iWbHnnkOjVp4rlQzRO8MabWKrWM+CyU+umnn3T69Gk1atQocV98fLy+++47zZ49W7GxsQq8ZgFn2bJlFX3N80BPnz6toKAglShRItX75MuXT/ny5UuxPzg4OFf9zy639RcZY0z9D2PqfxhT/8OY+h/G1P8wpv7H1THds0fascM86ezhhwN16pTZf/myjZ+JHMjbf1Z//tm0t9wi9e8v3XmnTSVLev5+NWua9o8/AmQt4rrjjgAFB2dcgvupp6R77pFmzw5U4cJm3z33BOieewI0d660Y4dNH3wQLLvdLE2VpKNHg9Sihcc/hkd4ckxdvY7PQql27dpp7969yfY98MADql27tsaNG5cikJKkZs2a6auvvkq2b82aNWrcuDH/kQIAAACAXOztt03bq5dUtqxkTbSg0HnedOiQaR991IRS3lK9umn37DFtoUJShw6unduvn/TMM+bJfWfOSKGh5udXkh57TBowQHrrLSlJmWsdOeKpnvsHnz19r3DhwqpXr16yr4IFC6pEiRKqV6+eJLP0buDAgYnnDBkyREePHtXo0aO1f/9+ffDBB3r//fc1ZswYX30MAAAAAEAWXbxonp4mSYMHm7ZAAdMSSuVNhw+btkYN796nShUlq/HUvbvkaoWhoCApaRzRs6cJtSSpb1+pXDnp1Cnp3DnnMYRSyfkslHJFVFSUjh07lvi6atWqWrlypTZu3KgGDRro+eef1+uvv64+ffr4sJcAAAAAgKxYtMjMjKpeXWrb1uyzQim7XYqP913fkP0uXZJOnjTb1kwmb8mfXypf3vna3XjhwQelkiXN9r33OvcHB5tZXpZbbzXt0aOZ66e/ylEPJ9y4cWOy1/Pnz09xTOvWrbVr167s6RAAAAAAwOuspXuPPCIF/P/UCSuUksxsKWsGCvzfH3+YtmhRqXhx79+vWjXpxAnzM9e5s3vnhoZKq1dL+/alPHfwYGn2bKl0aWn8eGnDBmZKXStHhVIAAAAAAP924ULygGn3bunHH83Mkvvvd+4nlMq7rKV73p4lZaleXfruO6lLF6lgQffPb9TIfF2rZEnzWQICpKgos+/YMfPkvqRLBvOyHL18DwAAAADgPyZONLNf3njDuc+aJdW7t5lRYgkIkEJCzDZ1pfIWq8i5t+tJWYYONcvrJk70/LVDQ80SwQoVzM/0lStKfLIkmCkFAAAAAMgmK1ea+lAjR5onkpUuLS1YYN4bMiTl8QUKSFevEkrlNdk9U+qmm6T16717j+BgU7vq+HGzhK9sWe/eL7cglAIAAAAAeJ3DIR044Hw9dqxzu1EjqXXrlOcUKCCdP08olddkdyiVXapUMaHU0aNS06a+7k3OwPI9AAAAAIDXnThh6kkFBZmiz5JUsaI0fbq0dm3qNXasulKEUnlLdi/fyy6VK5uWYudOzJQCAAAAAHidNUuqRg3phRekYcOkMmVMSJUWQqm8x243M4kk/5wpJRFKJUUoBQAAAADwuv37TXv99aYtXz7jcwil8p5jx0zdsfz5pfBwX/fGs6yZUlboBpbvAQAAAACywbWhlCsIpfIea+le9ermaXX+hJlSKfnZEAMAAAAAciJCKbjCX4ucS8lnSjkcvu1LTkEoBQAAAADwOkKpnO3cOenxx6XduzN3fmSktGCBqQmVFf4cSlWqZNpLl6SzZ33bl5yCUAoAAAAA4FV//y2dPm22r7vO9fMIpbLPu+9Ks2ZJXbtKZ86YfUePSj//nP55Dof04YdS/frSPfdIX3yR9rF//SWtWZP+LKFffzWtvz15T5Ly5XPWyWIJn0EoBQAAAADwKmuWVMWKUqFCrp9HKOVZDof0/PPSokUp3/vpJ9NGR0sPPigtXmxmtd10kyk+npqzZ6UBA6T775cuXjT79u1L+/4DBkidOknLl6f+/sqVJrSy2aQWLVz+WLmKVVeKYucGoRQAAAAAwKsys3RPIpTytAMHpOeekx56SIqLS/5e0mV7X38t3XWX+b7HxUlr1zrfcziks2fza/58m+rUMQFXYKB0443m/cjI1O/9xx8mcJJM+HStf/6RBg0y26NGSTfckKmPmONVrWpaKwTM6wilAAAAAABeRSiVM1gzni5dco6JJF24IP3+u9l++mnnfmup5fr1pt29W6pRI0gPP9xJjzwSpDNnpHr1pG3bpCefNMekFUrNn+/c/u675O9dviw98oh08qRUq5Y0dWqmPl6ucPvtpn3/fenKFd/2JScI8nUHAAAAAAD+jVAqZzh50rn944+mDpQk7dljZkCFh5tAqHp1qXx5KSREattW2rDBvP/qq9Lx4zYFBCSoXj2b+vWzacwYc5xVJ+qPP1LeNz4+eSh14IB06pRUpoz07bfS4MHmvIAAc1xoqLe+A77Xq5dZxnr8uJlldv/9vu6RbzFTCgAAAADgVQcOmJZQyreShlI7dzq3raV7DRuaek4PPmhqPzVrZopzR0VJv/ziLGI+bdpW7dwZp6efNoGU5FyWdvJkyhlA69ebEKZoUefsq82bTR86dTKBVIUKptZUs2Ye/9g5SlCQNGyY2X7tNSkhQdq0yTlTLa8hlAIAAAAAeM3ly84njRFK+daffzq30wqlksqf3xkSjR5tlvlVqeJQ7dp/p7h2yZLOIvZWEW+HwwSSr7xiXg8YIHXoYLa/+0566SUTynTtKv32m3TbbVn8gLnEoEHmZ/vnn01I16aN1LKlFBvr655lP0IpAAAAAIDXHDxowokSJaRSpdw7l1AqY4cPS7VrS2+/nfGxSWdK/fKLdPWq2U4rlJLM8j3JLOGTpH79EmSzpTzOZnPOloqMNIXLa9UyQeTq1Wb/Aw9IrVqZ7WXLpCVLzPYLL0iFC2fcf39RvLh0zz1m+9Ah0546JX3zje/65CuEUgAAAAAArzlwwCQY7s6SkgilkoqKMkHPtRYtMsHfs886Q6a0JA2lrl6V9u6V7Hbp11/NvtRCqVtvTf76rrsS0ry+FUr98YcJWA4dMsv7WreW3nhDatzYzAiSzHK++HipXTv/fdJeep57zswMe/pp8zRESVq40Ld98gVCKQAAAACA1xBKue7sWWdAlFRkpFnm1ayZs6C45eefned+/XX617dCqZIlTbtzp1k2d/WqVKSIM1RKqkkTZ+HxiAipbt20r1+tmrO/W7ea7SFDpI0bpeHDzeuyZZ11pSTp8cfT77O/qlDBjNe0adLQoWbf8uXSv//6tl/ZjVAKAAAAAOA1VihVu7b75+a1UKpHD/NEvA8+SL5/wgQTVhw8KJ04kfw9K5SSpHnz0r52fLwUHW22u3Uz7c6dzvMbNFCqy/KsmU6SdPfd6fc/6fI9K5Rq0SLlcdYSvuuuk7p0Sf+aecGNN5qljpcvS19+6eveZC9CKQAAAACA1zBTyjWnTknbtpntRx5x1hfavVtasMB53J49zu2YGGdNIklaudIs80vN6dMmmAoIcBYU37LFLP+TUl+6Z5k9W3r1VWnUqPQ/gxVK7dljalZJqYdSw4dLN90kvf666U9eZ7OZIvBS3lvCx/ADAAAAALwiPt6W+Kh7T4RS77wjTZzomb7lNFYhccmER337SlOnplzeljSUsrYrVJCaNzdPsvv449Svby3dK1NGatrUbB844CxC3r172n2rVs0EUiEh6X8GK5T6/XfTl8qVpfLlUx4XESH98IPUsWP618tL+vc37Zo10pkzvu1LdiKUAgAAAAB4RXR0qOx2m0JDpUqV3D8/aSiVkCCNHClNmZJyCZs/WLfOtCNGmCfeXbhglu1t2iQFBZnZU5JzBpLkfGpegwbmyXaSWcJ3bd0pyRlKlS9vvipXdp67dq3zKXtZcW1NqubNs37NvKJWLalRIyk4WPrpJ1/3JvsQSgEAAAAAvOLEicKSTO2gzCzTShpKnT8vxcaa1+fPe6iDOcj69abt3FlasUJ6911TY6p4cfOktttvN+8nnSmVtB7UnXeamUwHDpiva1mhVLlyZrnYqlXmPj/9JLVv75nPULCgVLq083VqS/eQto8+Mss4O3f2dU+yT5CvOwAAAAAA8E9//llIUuaW7knJQ6mkS5ouXMhix3KYI0ekP/4wM6JatpTy55cefth8WaxQ6eBB6coVc4wVSjVsKIWFmdlOq1ebp7hd+z3/80/Tlitn2uuvz/y4pKdqVVO/SiKUcledOr7uQfZjphQAAAAAwCuOHzczpQil0mfNkmrSRCpcOPVjwsOlEiXMMsbffpPsdunXX817DRqYtkcP06b2BLekM6W8qVo10xYubJ4kCKSHUAoAAAAA4BXW8r2shlKSc6aPJF28mIVO5UBWPal27dI+xmaTbrjBbP/yi7R/v3T1qpkhZdVysoqVb99uloEllbSmlDdZfWnaVAoM9O69kPsRSgEAAAAAPM7hkE6c8MzyPUk6fty57U8zpRwO50ypjIqNR0SYds+e5EXObTazXaGCKZbtcEhff5383OyaKXXffSaQevJJ794H/oFQCgAAAADgcfv3S5cvBysw0KEaNTJ3jeBgZ4H0Y8ec+/0plNq+XYqOlkJDpWbN0j/WCqV+/tnUjZJMPamkevY0rfW+5dqaUt5Sq5a0bZvniqfDvxFKAQAAAAA86uJF6d57zXO12rRxKCQkc9ex2Zyzpfx1ptQHH5i2b18pX770j7VCqY0bpaVLzffHeiqfxQql1q6VLl0y27Gx0tmzZtvboRTgDkIpAAAAAIDHOBzmqXF799pUtOgVvfdefJaul1oo5S81pS5ckBYtMtsPPZTx8XXqOGeO2WzS/PlS69bJj6lf3xRFv3zZ1J6SzEwsSQoJMcXSgZyCUAoAAAAA4DEffWSClqAgh5588scsF9a2Qil/XL733/+az1KzptSyZcbHFyhglvjZbGaG1cCBKY+x2ZxPwDtxwrRJ60lZ9aeAnCDI1x0AAAAAAPgHu12aMsVsT5yYoDp1/s7yNa1Q6vRp5z5/CaXef9+0Dz7oeli0cqVZimcFT6mpUMG0ViiVXfWkAHcxUwoAAAAA4BGffCL98YdUqpQ0fHiCR66Z9Al8Fn8IpZYtk7ZuNcvxUpvxlJawsPQDKSllKGW1WZ21BngaoRQAAAAAIMvi4qRp08z2k09KBQt65rr+FkqdOyfdcYezQPkdd3h+BtO1oVRkpGmrVPHsfYCsIpQCAAAAAGTZggXS4cNmltTQoZ67bmqhVG4udD5xovT551JgoDR2rPPpe550bSh15Ihpq1b1/L2ArKCmFAAAAAAgy+bNM+3o0WaWlN3umev620ypNWtM+9FH0oAB3rlHWjOlCKWQ0zBTCgAAAACQJRcuSN9/b7b79vXstf0plIqOlg4cMEXNO3f23n2sUOrkSSk+nlAKORehFAAAAAAgSzZtMjOjqlaVqlf37LX9KZT67jvT1q8vFS/uvfuULWsKqMfFSb/95vx+Va7svXsCmUEoBQAAAADIkrVrTduhg5kF5ElJQ6lixUybW0OpTZtM26aNd+8TFCSFh5vtLVtMGx4u5c/v3fsC7iKUAgAAAABkiVUnqWNHz187aShlPT0utxY6t0Kp1q29fy9rCd/mzaZl6R5yIkIpAAAAAECmnTgh7d9vlou1bev56ycNpaxg5eJFKSHB8/fyhjlzpLlzpTNnpH37zL5Wrbx/X0Ip5AY8fQ8AAAAAkGnW0r2bbnIur/Ok1GZKORzS5cvmKX+uiImRChf2/NLCjOzbJz36qNlesMC0detKJUt6/97XPoGPUAo5ETOlAAAAAACZ5s2le1LyUKpiRWew5GpdqWXLpCJFpLff9njXMvTpp85tq7aTt+tJWaxQymIFekBOQigFAAAAAMiUixel1avNdocO3rlH0lCqdGnn7ChXQ6lvvjHtxx9n7v4nTkgrVrh/nsMhLVxotrt1c+7PjnpSUspQiplSyIkIpQAAAAAAmfLhh9K5c1L16lLz5t65R9JQqlQpqVAhs+1qsfPISNP+8EPmCqTfcYcJlayZTq7ascPcu2BBafFi870aMkTq2dP9PmQGoRRyA0IpAAAAAIDbEhKkV18126NGSYGB3rlP0lCqZElnKOXqTKkjR0wbFyd9/7179z5xQtq+3Wz/+qt751pL93r1kkJDpYEDTdHzkBD3rpNZSUOpwECz9BHIaQilAAAAAABu+/pr6dAhqWhR6f77vXefa2dKubN8LyHBGUpJ0qZN7t3766+d20ePun5eXJyZHSVJ/fu7d09PKVfOuV2xohTEY86QAxFKAQAAAADc9sorph082Dl7yRuyMlMqOlqKjXW+3rjRvXt/9ZVz+9gx18/bulU6dUoqUcJ7BeAzEhIilSljtilyjpyKUAoAAAAA4JZ9+8yso6Agafhw797LCqUKFZLy53cvlLJmSVnX+OEH6dIl1+578aK0bp3ztTuh1P/+Z9qbb5aCg10/z9OsJXzUk0JORSgFAAAAAHCL9VS5rl1TFtT2tFKlTFupkmndKXRuFTm/6SbTT7td2rbNtfuuW2dmWVm1stwJpU6eNG358q6f4w2EUsjpCKUAAAAAAC5zOJyh1N13e/9+detK8+ebp9dJ7tWUsmZKVa0qtW5ttl2tK2Ut3bOelvfnn6ZWlCtySij18MNS06bmCYJATkQoBQAAAABw2fbtJuwpVEjq1i177nnffVLjxmbbneV71kypqlWlNm3M9oYNGZ/ncEgrVpjtQYPMErz4eCkqyrX+/vmnaZMWG/eFbt3MzLDatX3bDyAthFIAAAAAAJdZs6R69ZJCQ7P//pmpKVW1qtSundnetk3655/0z/vnH2cA1bq1cxmcq0v4rJlSvg6lgJyOUAoAAAAA4JK4OGnxYrM9YIBv+pCZmVJVqphgqk4dM+Ppm2/SP+/sWdMWLmyKpFv1rAilAM8ilAIAAAAAuOTbb6UzZ6SSJaX27X3TB6umVEaFzuPjnSGSVejbWm5oLc1LixVKlSxpWndCKbtdOn3abPu6phSQ0xFKAQAAAABc8s47pr3rLlNnyRdcnSllFSYPDnbOWLrtNtOuWmVCq7SkFUodPZpx/06dMjWpgoKc5wNIHaEUAAAAACBDJ05IX35ptocM8V0/XA2lrHpSlSpJgYFmu3lzqWhR6a+/pB070j43KzOlrCLn4eFSAH/jBtLFHxEAAAAAQIbefVdKSDCFv+vW9V0/XA2lktaTsgQFSV26mO2vv0773KyEUtSTAlxHKAUAAAAASJfd7ly69+ijvu2Lu6GUVU/KYi3hW7RIevNNad06s9wuKU+EUtSTAjLm01Bqzpw5ioiIUFhYmMLCwtSsWTOtWrUqzeM3btwom82W4uvAgQPZ2GsAAAAAyFu++EKKjpbKlpV69fJtX1wtdG4t37s2lOrc2cyYioyUhg83Bds3b05+TFqh1Pnz5is9zJQCXOfTUKpChQqaMWOGdu7cqZ07d6pt27bq2bOn9u3bl+55Bw8eVFRUVOJXzZo1s6nHAAAAAJA7/fyzdPmy++dt3y4NHmy2Bw2SQkI82i23uTpTyprVVLly8v0lSkiffWY+k/Xe3r3Jj7k2lCpUSCpe3GwfP57+fQmlANf5NJTq3r27unbtqlq1aqlWrVqaNm2aChUqpO3bt6d7XunSpVW2bNnEr0Crah0AAAAAIIW1a6WGDaUmTaTTp10/b+NGM5Po3DmpRQtp7Fhv9dB1roZSZ86YtkyZlO/dfrs0d67Uo4d5fW3QdG0oJTlnSy1YILVsac5PjVXonFAKyFiOqSkVHx+vRYsW6eLFi2rWrFm6xzZs2FDh4eFq166dNmzYkE09BAAAAIDcacUK0/76q9S2rWvBVGysdNddZplchw7SN99IhQt7t5+usEKpK1ek+Pi0j0stWLpWxYqmdSeUmjFD2rJFmj079WtSUwpwXZCvO7B37141a9ZMV65cUaFChfTFF1+oTp06qR4bHh6ud955R40aNVJsbKw+/vhjtWvXThs3blSrVq1SPSc2NlaxsbGJr2NiYiRJdrtddrvd8x/Iw6w+5oa+wjWMqf9hTP0PY+p/GFP/w5j6H8bUu7ZsCZQUoKAgh/bts6lLlwRt2xYvmy3tcxYutOnUqSBVqODQkiVxCgkxBc9d5a0xNcsHgyVJ587ZFRaW8hiHQzp7NkiSTUWK2NPsd3i4TVKQjh1LkN3uTLhSO7dixQBJzlU6J044ZLfHpbjmyZPm3FKl0r5vbsafVf/jjTF19Vo2h+Pa5wxkr6tXr+rYsWM6d+6clixZovfee0+bNm1KM5i6Vvfu3WWz2bR8+fJU3580aZImT56cYv/ChQsVGhqapb4DAAAAQGbEx9sUGJg9fxW7fDlQd9/dVQkJAZoyZateeOFmXbkSpKlTt6hevb9SPcfhkMaObaVDh4rp7rt/0x13/J4tfXWFwyH16dNdCQkB+uCDb1S8+JUUx1y8GKS77zaP2Vu8+Cvly5eQ6rX27y+u8eNbqkyZi3r77W8lmbHp08es65s/f5WKFr0qSTp+vLAWL66lZs2i9NJLN0mSPv10hQoUcAZTsbEB6tevuyTpk09WqFChlKEVkBdcunRJAwYM0Pnz5xWWWnL8/3weSl2rffv2ql69ut5++22Xjp82bZo++eQT7d+/P9X3U5spVbFiRZ09ezbdb0xOYbfbtXbtWnXo0EHBwcG+7g48gDH1P4yp/2FM/Q9j6n8YU/+Tl8Z00SKbBg4M0vz5cRowwPt/HVu/3qbOnYNUqZJDhw7FaciQQH3wQYDuvjtB8+alvv5txw6bWrYMUr58Dv3xR5xKlXL/vt4c01KlgnT+vE2//mpXrVop3z98WLr++mCFhjp07lzawdCxY1KNGsEKDnbo33/jFBBgljZWqGD6e+mSXUGprC8qWTJIMTE27dljV+3azv1//CHVrh2sAgXMfdObiZZb5aU/q3mFN8Y0JiZGJUuWzDCU8vnyvWs5HI5kIVJGdu/erfDw8DTfz5cvn/Lly5dif3BwcK76A5Tb+ouMMab+hzH1P4yp/2FM/Q9j6n/ywpha//7+7rtBuu8+79/Peo7ULbfYFBwcrEGDpA8+kJYuDdCbbwaoSJGU58yZY9q77rKpXLmsjYc3xrRQIen8eSk2NlipXfr8edOWLGlL996VK0sBAZLdbtM//wSrbFnnucWKSQUKpH5uhQrSb79Jp04Fq359536ruHr58jaFhPj3z3Fe+LOa13hyTF29jk8LnT/99NPavHmzjhw5or179+qZZ57Rxo0bdffdd0uSxo8fr4EDByYeP2vWLC1btky///679u3bp/Hjx2vJkiUaPny4rz4CAAAAALjszBnp++/N9vffO0OMrPjxRyksTErrr0Vbtpj2lltMe/PN0vXXS5cvS4sWpTz+2DHpv/812yNGZL1/3lCwoGnTegKfK0XOJSkoSLLmOFjFzl05t0IF0544kXw/T94D3OPTUOrUqVO69957dd1116ldu3basWOHVq9erQ4dOkiSoqKidOzYscTjr169qjFjxigiIkItW7bUli1btGLFCvXu3dtXHwEAAAAAXPb116YmkmRa66l4aUlIkN5/X1q1ymxfKz5eGjpU+vdf6c03pXXrkr8fFydt22a2rVDKZpMeeshsv/9+ymtOmGAKmt96q9SokeufLTtZT+C7eDH19//6/1JZGYVSUson8GU2lDp1Stq922wTSgGu8enyvfdT+y9gEvPnz0/2+sknn9STTz7pxR4BAAAAgPdYz2cqXlz6+2/z+v770z5+0SLp4YfNds2a0sSJ0v8vLJFkluH99JPz9dCh0p49Uv785vUvv5jgpkgRqW5d53H33is99ZSZZXXggBLrIv3yi/Txx2b7xRez9FG9ygqlsjpTSjKh1PbtWQulRoyQZs92vl++fMb3BeDjmVIAAAAAkFdcviytWWO2Z8407Zo10pWUD49LtHixc/v336X77jOzoiTpn3+kp58225Mnm2Vov/8udesmtWwp1aol3XGHeb9FC1M7yVK6tJkJJSWfXfXUU2YG1513SjfdlPnP6m2eWr4neWamlDVOZctKrVpJDzyQ8X0BEEoBAAAAQLZYt066dMmEIA88YGbTXLwobdiQ+vH//it9843Z/v57E4TExztnRs2aZQKUOnWk8eOl11933mfLFhNQRUaafR07prx+69am3bTJtJs3S6tXmzpL06Z55CN7TYECpr18OfX3szOUOnXK1AYLCDBP/du0KfmsNABpy3FP3wMAAAAAfxEfLz37rAmerNCjRw8TYHTvLs2dK335pdSlS8pzV6yQYmPNsr2mTU2B8hMnzJK7Nm2cs67GjpWCg6U+faTXXjOFym+4QapUyfkkudSunzSUcjikTz4xrwcOlGrU8Oi3weNCQ03rjVDKlXpUSUOpPXvMdo0azn4BcA2hFAAAAAB4gcNhag3NmZN8/113mbZnTxNK/fe/0iuvpAw0liwxbZ8+pjh5kyZm3w8/mBlXO3ea961wyWaTRo50vX833WRqT50+Le3bJy1bZvb36+fWx/QJ63t16VLq71uhVIkSGV8rKzOl/vrLjIck1a+f8b0AJMfyPQAAAADwgunTTSBls0mvvmpmPu3e7XwKXocOUpUqpuD5woVm37PPmqVf77wjrVxp9vXta1qrxtOPP0o7dpgn65Uvb66RGfnySc2ame0XXjDhVLFizlpTOZk3lu+dPGm+p66cW7SoMxhbvdq0hFKA+wilAAAAAMDD5s+XnnnGbL/+ujRqlNS1q9SggfOYwEBp+HDnMWvWmFpOv/0mDR5sZgFVqSLdeKM5plEjE3AdPeqcRdWypdmXWdYsq08/NW2PHmYpYE7n6kwpV0KpMmXMZ05IkKKiXDvXZnOGWdu2mZZQCnAfoRQAAAAAeNDq1dLDD5vtceOcwVNqHnzQBCx795on3knm6W1hYWa7f39n6BQWJtWubbbnzTNty5ZZ62urVslf9+mTtetlF2umVGqhVEKCmX0muRZKBQSYGWeSWcLnaqBlLeGLjzdtRETG9wKQHKEUAAAAAHjIvn1muV18vHTPPWZZXHqKFTOFxSVTlLxKFbPM73//M7WmJk1Kfry1hM8KY7IaSjVtKoWEmO1ChcySwtwgvULn5887gyJXakpJzllPhw9LMTFm29VQyupPtWqu3QuAE6EUAAAAAK/ZvNn5dLK84KOPpIsXzQyk9983s3AykrQ4+TvvmHCoTBkTblmBkaVJE+d2sWKm/lRWFCjgvOZtt5nC57lBesv3rJlOhQubulmusEKpXbtMGxBg6kalJ2koVbeua2MNIDmevgcAAADAK6KjpXbtpCJFTBHp3FCrKKtOnDBtjx4pA6W0XH+9qenkcGQ8U8maKSVJLVp4JggZNcrUUnriiaxfK7ukV+jcnXpSlqpVTfvaa6YtUSLj723SUIp6UkDmkOUCAAAA8IqDByW73YQE1gyU3Gj8eDNz6eefMz42Ksq04eHu3eOuu0z9qIzccIMz3Mvq0j1Lnz7SoUPJA6+czpWZUu6EUsOGSR07mmDQ1XOThlLUkwIyh1AKAAAAgFccO+bc3rTJd/3Iiu3bpRkzpNOnpREjnKFFWk6eNG25ct7pT758ZjZVcLBZbpdXuTJTytV6UpIJEVevlr76ysxAe/TRjM9hphSQdYRSAAAAALzi6FHn9saNPutGpsXFJQ8ntmyRli5N/xxvh1KSWer3v/9lvZ5UbubpmVKSecpht25mnNN7YqLFqkMlEUoBmUUoBQAAAMArks6U2rLFhDy5yZw50u7dpqD40KFm37hxUmxs6sdfuCD9+6/Zdnf5njvCwsxT+vIya6aUJ0Mpd5UoIU2eLE2bJpUq5d17Af6KUAoAAACAR/zwg/TII87ZQklnSv37rwl4courV6WJE8329OnSzJlS2bLS4cMmhEiNVU+qUCHz5Dd4jzVTKrXle3/9ZVpvh1KS9Nxz0tNPe/8+gL8ilAIAAACQZb/8Ymodvfuu9PbbZp81U6p4cdPmprpSW7dK//wjlS4tPfywCZpeesm89/zz0rx5Kc/JjqV7MLyxfA9A9iOUAgAAAJAlkZFS585STIx5vXevKQhuzZS66y7T5qa6UqtWmbZzZykw0Gzfc49ZvidJgwZJ//mP+azx8WZfZp+8B/e5UuicUArI+QilAAAAAGTJgw9K0dGm9pIk/fqrCQaswOCee0y7ebMzwMnpkoZSSU2fLt13n/kcY8ZIERFSw4aS3c5MqexkzZS6ejVlrTJCKSD3IJQCAAAAkGkOh6klJUmffWbaQ4ekgwfNdtmyUpMmpjh3TIz022++6ac7TpwwwVpAgNSxY/L3bDazRHHGDKltWzOLau9eaf9+QqnsZM2UklLOliKUAnIPQikAAAAAmXb2rKnrY7NJLVuaIMDhkFavNu9XrmyCm0qVzOvTp33XV1dZfW/SxDxh7VrBwWYZ37p1UrNmZt+ePSzfy0758zu3k4ZS8fHS33+b7dTGDkDOQigFAAAAINMiI01brpyUL59Ur555vXKlaa0wylraZwUGOZm1dK9Ll4yPjYgw7d69zJTKTgEBzmAqabHzf/4xoahEKAXkBoRSAAAAADLtyBHTVq1qWiuU2r3btJUrm9Z6At8//2Rb1zLFbpe+/dZsX1tPKjX165uWUCr7WXWlks6UspbuFSliZrQByNkIpQAAAABkmjVTqkoV01qhlCW3zZTassXUvipZUmrcOOPjrVAq6fI9QqnsYYVSSWdKUU8KyF2CfN0BAAAAALlXWjOlLNfOlMrpodTnn5u2Rw+zRCwj1uf980/nPmpKZQ+r2HnSmVJ//WVaQikgd2CmFAAAAIBMu3amVN26yd+3ZkrltOV7X3whzZ3rrD8kmSLZS5aY7b59XbtOkSLO4E2SCheWChXyXD+RNmZKAbkfM6UAAAAAZNq1M6WKFpUqVJBOnDCvrcAmJy3f+/df6a67pKtXpRo1pPbtzf6tW6VTp8xnaNfO9evVry8dPWq2WbqXfayZUoRSQO7FTCkAAAAAmZKQ4AylrJlSknNJW+HCJuCRctbyvU2bTCAlSdOnO/f/97+m7dlTCglx/XpWXSmJpXvZKb1C54RSQO5AKAUAAAAgU06dkmJjpcBAqWJF534rlKpUSbLZzLY1UyonLN9bu9a5vX699MMPJmCzlu7dcYd714uIcG4zUyr7sHwPyP1YvgcAAAAgU6x6UhUqSEFJ/mbRpIlp69Rx7stJM6WsUKpiRen4cWnqVKlLF/P0vLAw53I+VyWdKUUolX1SK3RuhVIlSmR/fwC4j1AKAAAAQKZcW0/K0ru3tHix1LKlc19OCaVOnJD27zdP1vv0U+mWW6SvvjJfktSnj5Qvn3vXrFVLCg6W7HaW72UnZkoBuR/L9wAAAABkyrVP3rMEBkp33pk8oLGW71286Kzn5Avffmvaxo2lFi2k/v3N64oVpbFjpddfd/+awcHOpw5WqOCZfiJj6c2UIpQCcgdmSgEAAADIlNSKnKelSBFTX8rhMHWlypTxZs/SZi3d69DBtPPmSZMmmafwBWThn+ynT5c+/1zq2jXLXYSLUpsp9ddfpiWUAnIHQikAAAAAmWLNlLp2+V5qAgNNMHXunO9CqYQE50wpK5TKl88sv8uqzp3NF7KPNVPKCqXi4pyF9AmlgNyB5XsAAAAAMsWdmVKS7+pKORzS22+bguSnT0sFC0rNmmVvH+B51kwpa/me9XNlszmXiwLI2QilAAAAALgtPl46dsxsuzJTSsq+UGroUDMTygor5s6VhgyRfvvNBFKvviqFhHi3D/C+a5fvWfWkihVL/jRIADkXoRQAAAAAtx07Zp42FxwslSvn2jnW7BVriZU3REWZEOrbb6UVK8y+hQtN++ij0p9/SoMGee/+yD7XFjqnyDmQ+xBKAQAAAHDb7t2mrVfP1ItyRXbMlFqzxrn93/+a5Xpbt5rXTz1l6lrBP6Q1U6pECd/0B4D7CKUAAACAPC4+3ixtczhcP2fXLtM2auT6OdkdSn39tbR4sflcjRtLFSt6777IfsyUAnI/QikAAAAgj3vlFaluXem991w/xwqlbrzR9XO8vXwvIUFau9Zsh4SYGTTPPWde9+rlnXvCd9KaKUUoBeQehFIAAABAHrdqlWmXLXPteIdD+ukns+1OKOXtmVI//yydOSMVKmTqR0nSuXOmJZTyP4RSQO5HKAUAAADkYQ6Hc9bT99+b2UYZiYoytZoCA6WICNfvZc2U8lYoZS3da9tWGjDAub9GDalOHe/cE75z7fK9v/4yLaEUkHsQSgEAAAB52OHD0vnzZvvcOWn//ozPsWZJXX+9MxhwhTVTylvL9775xrQdO5oaUlWqmNe9ekk2m3fuCd9hphSQ+xFKAQAAAHmYFTBZrCfVpScz9aQk7y7fu3DB2fdOnUwI9cILUtOm0rBhnr8ffI9C50DuRygFAAAA5GFWKBXw/38z8GYo5c3le2++KdntUs2aUvXqZl///tK2bc4ZU/AvSWdKORyEUkBuRCgFAAAA5GFWKHX77abNjplS//xjQgRP+ftvafp0sz1hAkv18gprplR8vAkkrVCqRAnf9QmAewilAAAAgDwqaZHz4cNNmHP4sHTqlAmrfvvNeeznn0ulSpn6TCdOmGMbNHDvftZMqfh46d9/PfEJjBdeMHWxIiKSFziHf7NmSklm/GNizDYzpYDcg1AKAAAAyKP++MMUNw8JkZo3l+rVM/vvu88UCm/ZUoqLM/sWLjQzUb780ryuVUsqXNi9+xUoIOXLZ7Y9tYTv6FHpjTfM9osvmicCIm8ICXEuOz1+3LQBAVLRoj7rEgA3EUoBAAAAeZQ1S6p+ffMX/BYtzGvrKXZ//y0dOWK2f//dtG3bSmXLSoMGuX8/m83zT+BbtEi6elVq1coUOEfeYbM5l/BZoVTx4gSTQG5CKAUAAADkUVY9qUaNTNu6tWmDg51L7Q4elBISnKHUu+9KUVHSE09k7p6eLnb+yy+m7dyZWlJ5kbWE73//M22ZMr7rCwD3EUoBAAAAeZDDIW3ZYratUKpvX+mVV8wT6zp0MPsOHDCzUGJjTVhVuXLW7uvpmVJ79pg2IsIz10PuYs2U+v570/JzAOQuQb7uAAAAAIDs99575kl7QUFmSZ5kth9/3Gxfd51pDx50zkKpXj3rS6M8OVMqNtb0TyKMyKusmVJWKNWwoe/6AsB9zJQCAAAA8ph9+6SRI832Cy9INWqkPCZpKGUt3atVK+v3tp6Mdvp01q914IApxF60qFShQtavh9zHmill/TwRSgG5C6EUAAAAkIfEx0t33SVduWIKg6dVG6p2bdMeOOCcKeWJUMoKj06cyPq1rKV79etTTyqvsmZKWQilgNyFUAoAAADIQ3bskH79VQoLkz78UApI428EVgB1+rT0ww9mu2bNrN+/YkXTWk9Ly4q9e03L0r28K2koVbGiVKKE7/oCwH2EUgAAAEAe8tVXpu3aNf0nlRUuLJUrZ7a3bzdtTpgp9cor0tCh0tWrFDmHc/mexCwpIDei0DkAAACQh1ihVPfuGR9bu7Z08qR5Up/kmVDK3ZlSJ05I5cub5Xm7dzuXG950k3OmVP36We8XcqekM6UaNPBZNwBkEjOlAAAAgDwiMtIUOQ8MlDp3zvh4q9i5JBUsKIWHZ70PVij1zz/SxYvpHzt5sjm+Z09TC+uZZ5zvTZpkAjNJqlcv6/1C7sRMKSB3I5QCAAAA8oivvzZtixZS8eIZH580lKpZ0zPFxMPCpEKFzHZ6S/jeeMMET5KZ3dWzp7RqlRQUJBUp4pxpVa2aWWqIvCnpTClCKSD3IZQCAAAA8gh3lu5JzifwSZ5ZuieZYMuaLZVWKLVsmTRypNnu0cO0K1aY9qGHpNGjncdSTypvs0KpYsWkSpV82xcA7iOUAgAAAPKAmBhp40az7Woode1MKU+xip2nVVfq5ZdNO2SICagmTDCv8+c328OHO2dbUU8qb7OW7zVs6JmZfACyF4XOAQAAAD+3f780cKBkt5twKWnYlJ5KlUwQdOWK52ZKSenPlDp/3vm0v3HjTNAwaZJUtqxUo4Ypei5JL7wgTZ0q3XGH5/qF3KdRI9O6GrQCyFl8OlNqzpw5ioiIUFhYmMLCwtSsWTOtWrUq3XM2bdqkRo0aKX/+/KpWrZrmzp2bTb0FAABATmc9JQ5GQoI0a5Z0443Szp1midObb7p+fkCAdPPNJhi66SbP9Su9J/Bt3GhTfLwJwapUcfbj0Ueljh2dx40YIZ06xUypvK5HD+ncOWnUKF/3BEBm+DSUqlChgmbMmKGdO3dq586datu2rXr27Kl9+/alenxkZKS6du2qli1bavfu3Xr66ac1cuRILVmyJJt7DgAAgJxm61apalWpSRPzhLm87sQJqV076fHHzUynTp2kvXulDh3cu84XX0h79kjXX++5vlnL91KbKfXtt2YNVtIACkhPkSK+7gGAzPLp8r3u18yxnDZtmubMmaPt27erbt26KY6fO3euKlWqpFmzZkmSrr/+eu3cuVMvv/yy+vTpkx1dBgAAQA7jcEhz55rC2HFx0tGjZknPrFmmJlFe9cADpoZUaKj0yivSI49kruZOsWLmy5PSmyn17bfm380JpQDA/7kdSsXHx2v+/Plat26dTp8+rYSEhGTvr1+/PlMdiY+P13//+19dvHhRzZo1S/WYbdu2qeM1/3fq1KmT3n//fdntdgUHB2fq3gAAAMi9li0zS7skqW9f6eJFadUqs699e1OHKK+5elX67juzvWmT1Lixb/tzrbQKnUdFherwYZuCgqQ2bbK9WwCAbOZ2KPXYY49p/vz5uu2221SvXj3ZsviIg71796pZs2a6cuWKChUqpC+++EJ16tRJ9djo6GiVKVMm2b4yZcooLi5OZ8+eVXh4eIpzYmNjFRsbm/g6JiZGkmS322W327PU9+xg9TE39BWuYUz9D2PqfxhT/8OY5l5nz0rPPBOou+5K0K23OgtGJR3TN94IlBSgIUPi9dpr5h9MGzcO0t69Nu3ZE6fKlfNeoaldu2y6ejVIxYo5FBERp5z2o1+2rCQF69w56Z9/7CpUyIzlL7+UliQ1a5ag/Pnjc1y/4R7+2+ufGFf/440xdfVabodSixYt0meffaauXbu63anUXHfddfr555917tw5LVmyRPfdd582bdqUZjB1bQjm+P9qlmmFY9OnT9fkyZNT7F+zZo1CQ0Oz2Pvss3btWl93AR7GmPofxtT/MKb+hzHNfd588watXVtFK1de1pw53yrgmoqoH320VRs2dJDN5tCNN67TqlWXJUkFC94kqZxWrfpNgYGR2d9xH1u1qoqkG1S58hmtWrXN191JVWhoV126FKyFC79ThQoXJEk//2yqqVeqdFArV/7Pl92DB/HfXv/EuPofT47ppUuXXDrO7VAqJCRENTw4Bzrp9Ro3bqwff/xRr732mt5+++0Ux5YtW1bR0dHJ9p0+fVpBQUEqUaJEqtcfP368Ro8enfg6JiZGFStWVMeOHRUWFuaxz+Etdrtda9euVYcOHVie6CcYU//DmPofxtT/MKa50//+J61fb35dPXWqoPLlu00dOph/kLTG9I8/2kiS2rd36P77b008d+PGAG3fLhUqVFddu7pfoTsqyhRPzkX/hpnMF18ESpI6dy7hsX9M9rTKlYO0f79UvXprtWvn0A8/xGnnznySpBEjaujGG/Pguks/w397/RPj6n+8MabWKrWMuB1KPfHEE3rttdc0e/bsLC/dS43D4Ui23C6pZs2a6auvvkq2b82aNWrcuHGa37h8+fIpX758KfYHBwfnqj9Aua2/yBhj6n8YU//DmPofxjTnO3lS+v13qXlz6fnnpfh4KSjIFDB/770gJc1X4uNtWrDA/Do7aFCAgoOd06iqVjXt8eOBCg4OdKsPX30l9eljnuK3ZUuWP5JP/PSTaZs2df/zZ5dKlaT9+6WoqCDZ7dKDDwYpLs6mXr0S1KRJcKaKsiNn4r+9/olx9T+eHFNXr+N2KLVlyxZt2LBBq1atUt26dVPcaOnSpS5f6+mnn1aXLl1UsWJF/fvvv1q0aJE2btyo1atXSzKznP7880999NFHkqQhQ4Zo9uzZGj16tAYNGqRt27bp/fff16effuruxwAAAEAOEx8vtW0rHTxoag5FR5unxX30kTRggLR8uQmtypUzx+/aVVonT9pUsqTUo0fya1WubNqjR93rw8aN0h13SHa7tHt3lj+ST1y4IP32m9lu0sS3fUmPVez8wAFpxAjpf/+zqXjxy5ozJ0g2W0D6JwMA/ILboVTRokV1++23e+Tmp06d0r333quoqCgVKVJEERERWr16tTp06CBJioqK0rFjxxKPr1q1qlauXKnHH39cb775psqVK6fXX39dffr08Uh/AAAA4DvLlplASjKBlCT172++3nrLzFp6/31pwgTz3ooV1SRJ994rXTsxPjOh1P/+J3XvLlmT9i9dMl+5bQnfrl1SQoJUvryUynOAcoyKFU07c6Zz38iRu1WixE2+6RAAINu5FUrFxcWpTZs26tSpk8qaR2Zkyfvvv5/u+/Pnz0+xr3Xr1tq1a1eW7w0AAICc5eWXTTtunJnh8+OP0pgxZt/gwSaUmjtXeuwxad8+m37+ubQCAx0aPjzlOi8rlDpzRrp4USpYMOP7//e/ZpbRzTebWVJXr5rzrWvlFj/+aNqbcni2ExHh3K5RQxo7Nk5lypzxXYcAANnOrVAqKChIQ4cO1f79+73VHwAAAORB338vbd9uZjw9/rhUpozUu7fz/b59peeekyIjpbFjpagos7yrf3+HqlVLGUoVLSqFhUkxMdKxY9L1LtQ6//1303bvLp04If35Z84PpTZvlh56SCpd2oQ8Xbua76OUs5fuSVKvXtLXX5tlfBERUlycQytX+rpXAIDs5PZi7Ztvvlm7c+sCewAAAORI1iype+81gdS18ueXPvjAbL/zjvTVVwGy2Rx68sn4VK9ns7m/hO/QIdPWqCGVKmW2z55N/dgLF6TOnU0xdl+aP9+EaVu3SnPmmEDt88/Nezl9ppTNJt12m3TDDaKoOQDkUW7XlHr00Uf1xBNP6MSJE2rUqJEKXjMXOiLpPFwAAAAgA3/8YepJSdLo0Wkf16aNKYj9xhvmdfPmJ1W7duk0j69cWdq7N2uh1Jk0VpN99ZX0zTdmSeEzz0gBPqrLbRU0Hz7cBDuffCL984+Zcda4sW/6BACAq9wOpfr16ydJGjlyZOI+m80mh8Mhm82m+PjU/7UKAAAAedvs2dKKFdJrr0m1ajn3v/mm5HCYmUcZLbObPl1au1b64w+H7rjjoKT0QylJOnIk4779+6906pTZdiWUWrPGtBcvmutXq5bxPTzN4XCGUoMHS/XqSTNmmIAvPNwsYQQAICdzO5SKjIz0Rj8AAADg555/Xjp92hQS//xzqV07E+pYy/JGjMj4GgULSjt2SGfOxOnXX/9N99gqVUzrykypw4dNW6qUVKRI+qGUw+EMpSQzG8sXodTJk6ZmVmCgVLOm2RcaKg0YkP19AQAgM9wOpSrn5EqPAAAAyJFOnzZfknTunNSpk5k5FRBgXlevbmZKuSIsTCpQQPr11/SPc6emVNKle1L6odRvv5lAyPLrr1LPnhnfw9OsWVI1apjlegAA5DZuh1IfffRRuu8PHDgw050BAACAf9q717RVqkgtWkgLFkhDh5qZT5I0bJjn6zJlFEpt2SJduiR17Oh88p4VSpUsadrUQqlvvkn+2vps2c0KperU8c39AQDIKrdDqcceeyzZa7vdrkuXLikkJEShoaGEUgAAAEjBCm4aNpQ+/tgEKc88Y5bvhYZKDzzg+XtaodTJk9LVq1JIiHkdHS2NGiUtXmyKg//8s3szpayle+3bS99+SygFAEBmuf3vUf/880+yrwsXLujgwYO65ZZb9Omnn3qjjwAAAMjlrOCmfn0TBD39tLRkiVm29/zz3inKXbq0lD+/qQF1/LjZd/Gi1KSJCaQk896XX6YdSp09m/yaV65ImzaZ7SeeMO3Bg1JsbMr7//OP1KyZ9OyznvtMSRFKAQByO49Mkq5Zs6ZmzJiRYhYVAAAAIEl79pi2fn3nvt69TRg0erR37mmzpVzCt2ePCaiKFZMef9zs+/pr12dKbdligqly5UxdrCJFpPh4E0xda+lSaft26dVXpbg4z342h0Pat89sZ/TEQgAAciqPrdwPDAzUyaQVHwEAAACZ0MYKUJKGUtnBCqWOHDFtVJRpr7tOGjvWbP/wg7Nw+bWh1Llzkt3uvN6bb5q2c2cTetWrZ17v3WtmUA0ZYs6RpFWrTHvpkjOU85TTp81MLJvNfBYAAHIjt2tKLV++PNlrh8OhqKgozZ49Wy1atPBYxwAAAOAf/vhDunzZLKWzQp/sUqmSaU+cMK0VSoWHm6/GjaWdO82+4sXNl2RmUgUESAkJZglfeLgJr5YtM/vHjDHH1a8vbd1qAqmlS6W//jLLBidMkNaudfZj+3bpxhtT7+Nnn5lwy51leNbSvapVTU0uAAByI7dDqV69eiV7bbPZVKpUKbVt21b/+c9/PNUvAAAA+AmrnlSdOlJgYPbeu3x50/75p2mThlKS1K2bM5RKGpgFBEglSpjle2fOmOOfftq8N3Cgc8mcNfPr3Xed577/vtSmjRQT49y3bZv06KMp+7dtm9Svn+lnZKQUHJz654iPN0v2gv7/t3fqSQEA/IHboVRCQoI3+gEAAAA/ZYVSERHZf+8KFUyb2kwpyYRSkyaZ7WtncZUq5Qyl1q0zX8HB0sSJzmOs5XuWQoXMUkCrTlb58iYQ27Yt9f798INp//xT+vxzqX//lMf8+afUoIGZsVW0qOmnVaOKUAoAkJu5XVNqypQpunTpUor9ly9f1pQpUzzSKQAAAPiPpE/ey24ZzZRq2NC5nVooJZlQ6vXXzfaQIVKVKs5jkn6mhx6Shg4127/8Ytrx4017+HDKoumS9Ouvzu3XXkv9M6xd63wK4LlzZmbXzz+b1xQ5BwDkZm6HUpMnT9aFCxdS7L906ZImT57skU4BAADAf6T25L3sklEoFRAgPfKI2e7UKfm5Vih1+rS0ebPZvvfe5McUKyb16GHCoRdflAYNcr5ns5mleVZwtH17yv5ZgZ0k7diR+jHW9+/hh6X9+6X586U+faQuXaTbb0/1YwMAkCu4HUo5HA7ZbLYU+3/55RcVtypDAgAAAJLOn5cOHTLbvgilrOV7Z89KV66kDKUk6bnnpAsXpObNk59rhVJbtpgn3RUoYJbRXevLL83TBUuUkGrWlNq2NftvukkqWVJq2tS8vjZwSkhwPpXQOubll51L8yxWcNW0qVS7tnTffWap38qVUpEiLn0bAADIkVwOpYoVK6bixYvLZrOpVq1aKl68eOJXkSJF1KFDB915553e7CsAAABymXnzTIHu66+XypTJ/vsXK2ae+idJx4+bWU9S8lAqIEAqWDDluVYotWqVaZs0SbsQedJ/s500SSpbVnrsMfO6WTPTXltX6uhRE4aFhEhvvGH2LVlivk8PPSRdvGj2+XL5IwAA3uRyofNZs2bJ4XDowQcf1OTJk1UkyT/LhISEqEqVKmpm/R8XAAAAeV5cnLNO0qhRyYOb7GKzmSV8hw9Lu3aZgCwgwBk4padkSdNalStatHDtni1bOmdkSc5ZUD/8INntzmDLCptq15YaN5ZeeMHMlPr7b+mDD0yY1aOHdOqU+Rx167p2fwAAcguXQ6n77rtPklS1alW1aNFCQUFuP7gPAAAAeciyZdKRI2ZZ27W1mLJThQomlNq507wuU0YKDMz4vGuDK1dDqWvVqeN8kt9//iM99ZTZbxU5t2ZAjR8vjR0rPfmk9Oqr0po1UtWq5r1q1VKfzQUAQG7mdk2p1q1b6+jRo3r22WfVv39/nf7/OdCrV6/WPmtRPAAAAPK8V14x7dChph6Tr1jFzq1QKunSvfRcG0pZM57cFRgovfSS2Z40STpwwGxbM6Xq1XMeGxQk9e1rttetcz7FLyIic/cGACAnczuU2rRpk+rXr68dO3Zo6dKliU/i27NnjyZOnOjxDgIAACB3cTik2bNNDaWQEGnYMN/2xwqlfvrJtJkJperUkbLyTJ+BA6XOnaXYWFMvKj4+7VpRTZpIYWFmGd+HH6Z+DAAA/sDtUOqpp57S1KlTtXbtWoWEhCTuv/XWW7Xt2uqNAAAAyFMuXTJL9UaMMK+HDzdFv33JegLfv/+aNjOhVGaX7llsNuntt6VChaTvv5cmTJAOHjTvXRs4BQVJt95qtvfsSf0YAAD8gduh1N69e3X77ben2F+qVCn99ddfHukUAAAAcqfp06UFC8yStZdfNl++Zs2UsrgaSlmFziWpefOs96NSJTODTDLfp7g4MyOqYsWUx3bokPw1oRQAwB+5HUoVLVpUUUkfJ/L/du/erfLX/h8fAAAAudaHH0rvv+/eOd9+a9rZs6UnnvDNE/euldlQKjjYBEYBAVLr1p7py333mScRWurVS/17lDSUyp9fqlHDM/cHACAncTuUGjBggMaNG6fo6GjZbDYlJCRo69atGjNmjAYOHOiNPgIAACCb/f67dP/90sMPS5s3u3ZObKy0a5fZvnamjy9Zy/csroZSkvT118mfgucJL73k/P40bpz6MTVrSpUrm+26dV17WiAAALmN26HUtGnTVKlSJZUvX14XLlxQnTp11KpVKzVv3lzPPPOMN/oIAACAbPbxx87tsWNN8fKM7N4tXb1qlr1Vq+a9vrmrbFkz2ynpa1dFREjt2nm2P0FB0tKl0rx50nPPpX6MzeYMrnjyHgDAXwW5e0JwcLAWLFigKVOmaPfu3UpISFDDhg1Vs2ZNb/QPAAAA2SwhIXkotWOHtGSJ1Ldv+udZz7xp1ixnLNuzBAVJZcpIVgUKd2ZKeUuhQmYmWnqefdbMPhs3Llu6BABAtnN7ppSlevXq6tu3r+68807VrFlTS5cuVQT/jAMAAJDrbd0qHTkiFS5sZklJ0vjxkt2e/nnbt5u2aVOvdi9Tki7h8/XTAF1VubL00UfSddf5uicAAHiHW6HUu+++qzvuuEMDBgzQjh07JEnr169Xw4YNdc8996hZs2Ze6SQAAACyjzVLqm9facIEqXRp6dAhacaM9M9LOlMqp7GKnRcvLuXL59u+AAAAw+VQ6uWXX9awYcMUGRmpL7/8Um3bttULL7ygO++8U7169dKxY8f09ttve7OvAAAA8LLLl6XPPjPb995rZku9+qp5PWWK9NNPqZ/355/S8eOmdtNNN2VPX91hhVI5YekeAAAwXA6l3n//fc2dO1c7d+7UihUrdPnyZa1fv16HDh3SxIkTVbJkSW/2EwAAANlg3jzp/HmpYkWpdWuzr39/6Y47pLg4E1RdvpzyPGvpXv36pl5STmMt3yOUAgAg53A5lDp69Kjat28vSWrTpo2Cg4M1bdo0FS1a1Ft9AwAAQDb65x/n0+CefNL5xDqbTZozx9Ri2r9feumllOfm5HpSknmSXfHiUs+evu4JAACwuBxKXblyRfnz5098HRISolKlSnmlUwAAAMh+U6ZIf/0l1akjDRmS/L0SJaRXXjHbs2dLV64kf/+770ybE+tJSVKjRtLZs9Lw4b7uCQAAsAS5c/B7772nQv8/HzsuLk7z589PsWxv5MiRnusdAAAAssWPP5qwSTI1pIJS+S3xjjukp56Sjh2TFiyQHnrI7P/1V+mHH6TAQDMjKaey2XzdAwAAkJTLoVSlSpX07rvvJr4uW7asPrYezfL/bDYboRQAAEAucvasNH689MEHUkKC1K2b1LFj6scGBUkjRkhjx5rg6sEHTdBjPeumZ0+pXLns6zsAAMjdXA6ljhw54sVuAAAAwBf695e+/dZs9+ljakel5+GHpUmTpH37zHnNmkkffWTeGzrUq10FAAB+xuWaUgAAAPAvR4+aYMlmkzZulD7/XMqoZGjRomaGlGRCqKeflmJipBo1pLZtvd1jAADgTwilAAAA8qhFi0zburX5ctXYsVJ4uHT4sPTGG2bfkCHOp/UBAAC4gl8dAAAA8qiFC007YIB751WsaIqb33+/eV24sHMbAADAVYRSAAAAedC+fdKePVJwsKkl5a7ixaV586Tdu82T90qU8HwfAQCAf3O50DkAAAD8x6efmrZLFxMwZVaDBh7pDgAAyIMyNVPq8OHDevbZZ9W/f3+dPn1akrR69Wrt27fPo50DAACA59nt0oIFZrt/f9/2BQAA5F1uh1KbNm1S/fr1tWPHDi1dulQXLlyQJO3Zs0cTJ070eAcBAADgWS+/LB05YmZIde/u694AAIC8yu1Q6qmnntLUqVO1du1ahYSEJO6/9dZbtW3bNo92DgAAAJ514IA0ebLZnjVLKljQp90BAAB5mNuh1N69e3X77ben2F+qVCn99ddfHukUAAAAnM6fl5o3l8aPz9p14uKkhx+WYmOlzp2le+7xTP8AAAAyw+1QqmjRooqKikqxf/fu3SpfvrxHOgUAAACnr76Stm2TXnlFungxc9c4e9YEUVu3SoUKSW+/Ldlsnu0nAACAO9wOpQYMGKBx48YpOjpaNptNCQkJ2rp1q8aMGaOBAwd6o48AAAB52saNpr161bntit9/l8aNkx58ULrxRmndOrNcb8ECqVIlb/QUAADAdUHunjBt2jTdf//9Kl++vBwOh+rUqaP4+HgNGDBAzz77rDf6CAAAkKdt2uTc/uYb6bbbMj7nzBmpVSspOtq5r3p1adkyqV49j3cRAADAbW6HUsHBwVqwYIGmTJmi3bt3KyEhQQ0bNlTNmjW90T8AAIA87c8/pUOHnK+/+Sb5+wkJ0pYtUrNmUnCw2edwSIMGmUCqVi3p/vulsmWl3r2lIkWyresAAADpcjuU2rRpk1q3bq3q1aurevXq3ugTAAAA/p81S+q666TDh6X//U+KjJSqVjX733xTGjlSGj5ceuMNs++996QvvzQh1eLFUoMGPuk6AABAutyuKdWhQwdVqlRJTz31lH799Vdv9AkAAAD/z6ohddttZjaUlHy21Gefmfb996W//5ZOn5ZGjzb7XniBQAoAAORcbodSJ0+e1JNPPqnNmzcrIiJCERERmjlzpk6cOOGN/gEAAORp1kypNm2kTp3MthVK/f239P33ZvvyZRNMvfCCdOGC1LixM5wCAADIidwOpUqWLKnhw4dr69atOnz4sPr166ePPvpIVapUUdu2bb3RRwAAgDzp5EmzXM9mk1q2dIZS69ZJsbHSmjWmplTA//9G9+qr0pw5Znv6dOd+AACAnChLv6pUrVpVTz31lGbMmKH69etrU9JHwwAAACBLVq40bYMGUtGi0o03SuXLS//+a+pGWe8PGyYVLy5FRUlXr0q33iq1a+erXgMAALgm06HU1q1b9eijjyo8PFwDBgxQ3bp19fXXX3uybwAAAHnWTz9Jo0aZ7Z49TRsQID39tNl+/nlp1Sqz3aeP9MgjznNfeMHMrgIAAMjJ3H763tNPP61PP/1UJ0+eVPv27TVr1iz16tVLoaGh3ugfAABAnhMZaQqbX7wotW8vjR/vfO/hh6WXXzbHSFJYmNS8uVSrlrRsmdS2rdS0qU+6DQAA4Ba3Q6mNGzdqzJgx6tevn0qWLOmNPgEAAORpQ4dKp05JERHSkiVSSIjzvZAQafJkaeBA87pjRyk4WAoPl/bv901/AQAAMsPtUOp76xEvAAAA8Lg//jBP17PZTCAVFpbymAEDpJkzpV9/dS7tAwAAyG1cCqWWL1+uLl26KDg4WMuXL0/32B49enikYwAAAHnRe++ZtmNHqUaN1I8JDDRFzjduNAEVAABAbuRSKNWrVy9FR0erdOnS6tWrV5rH2Ww2xcfHe6pvAAAAeYrdLn3wgdlOWrg8NRUrSvfe6/0+AQAAeItLT99LSEhQ6dKlE7fT+nI3kJo+fbpuuukmFS5cODHwOnjwYLrnbNy4UTabLcXXgQMH3Lo3AABATvPVV6aWVJkyUvfuvu4NAACAd7kUSiX10UcfKTY2NsX+q1ev6qOPPnLrWps2bdKwYcO0fft2rV27VnFxcerYsaMuXryY4bkHDx5UVFRU4lfNmjXdujcAAEBO8847pn3wQVO8HAAAwJ+5Xej8gQceUOfOnRNnTln+/fdfPfDAAxpoPQrGBatXr072et68eSpdurR++ukntWrVKt1zS5curaJFi7p8LwAAAG+xJosHBmb+Gvv3OwucP/ywZ/oFAACQk7k9U8rhcMhms6XYf+LECRUpUiRLnTl//rwkqXjx4hke27BhQ4WHh6tdu3basGFDlu4LAACQWQkJUvv2UoUKZuldZr36qml79JCqVfNM3wAAAHIyl2dKNWzYMLF+U7t27RQU5Dw1Pj5ekZGR6ty5c6Y74nA4NHr0aN1yyy2qV69emseFh4frnXfeUaNGjRQbG6uPP/5Y7dq108aNG1OdXRUbG5tsuWFMTIwkyW63y263Z7q/2cXqY27oK1zDmPofxtT/MKb+x5tjumiRTRs3mt+L3nwzXhMmJLh9jdOnpY8+CpJk06hRcbLbHR7upf/hz6n/YUz9D2PqnxhX/+ONMXX1WjaHw+HSbz2TJ09ObJ944gkVKlQo8b2QkBBVqVJFffr0UUhISCa6Kw0bNkwrVqzQli1bVKFCBbfO7d69u2w2m5YvX57ivUmTJiX2PamFCxcqNDQ0U30FAACQpLg4m4YPb6voaPN7UbFiV/TOO2sUHOxeqPTpp9dp8eLaqlnzH82c+Z1SmZQOAACQa1y6dEkDBgzQ+fPnFRYWluZxLodSlg8//FD9+vVT/vz5s9xJy4gRI7Rs2TJ99913qlq1qtvnT5s2TZ988on279+f4r3UZkpVrFhRZ8+eTfcbk1PY7XatXbtWHTp0UDAVT/0CY+p/GFP/w5j6H2+N6bvvBmjYsECVLu1QYKAUFWXThx/GqX9/569XsbFSSIjSDJouXJBq1QrS2bM2LVgQpzvuYJaUK/hz6n8YU//DmPonxtX/eGNMY2JiVLJkyQxDKbcLnd93331Z6lhSDodDI0aM0BdffKGNGzdmKpCSpN27dys8PDzV9/Lly6d8+fKl2B8cHJyr/gDltv4iY4yp/2FM/Q9j6n88OaaXL0vTppntZ56x6dw5aeJE6a23gtS6tbRxo7RokbR2rVSpkjR7ttS1a8rrPPGEdPasVLWqdOedQQpy+7ezvI0/p/6HMfU/jKl/Ylz9jyfH1NXruP1rT3x8vF599VV99tlnOnbsmK5evZrs/b///tvlaw0bNkwLFy7Ul19+qcKFCys6OlqSVKRIERUoUECSNH78eP3555/66KOPJEmzZs1SlSpVVLduXV29elWffPKJlixZoiVLlrj7UQAAADJt3jzp5EkTOA0eLJ07J02dKu3YIVWpkvzYyEjpttuke+8151lP6VuwwLwOCDAtgRQAAMhL3H763uTJk/XKK6/ozjvv1Pnz5zV69Gj17t1bAQEBmjRpklvXmjNnjs6fP682bdooPDw88Wvx4sWJx0RFRenYsWOJr69evaoxY8YoIiJCLVu21JYtW7RixQr17t3b3Y8CAACQKXa7NHOm2R43TsqXTypTRnrgAbMvKEhq3FiaNEn6+WdpzBgTRH38sfTNN+aYP/6Qhgwx2xMmSK1bZ/enAAAA8C23/z1uwYIFevfdd3Xbbbdp8uTJ6t+/v6pXr66IiAht375dI0eOdPlarpSzmj9/frLXTz75pJ588kl3uw0AAOAxn34qHT0qlS7tDKIk6Y03pGHDpBo1pKTPU7nhBikuTpo1S/rgA7OMb9IkU0+qVSsTSgEAAOQ1bs+Uio6OVv369SVJhQoV0vnz5yVJ3bp104oVKzzbOwAAgBwmIUGaPt1sjx4t/X/FAUmmoHlERPJAyvLgg6ZdvlzatcsEW5L0n/84l/MBAADkJW6HUhUqVFBUVJQkqUaNGlqzZo0k6ccff0y1oDgAAIA/WbJEOnBAKlJEGjrU9fPq1zdL+ux2qXt3M3OqbVuzDwAAIC9yO5S6/fbbtW7dOknSY489pgkTJqhmzZoaOHCgHrT+CRAAAMAPXb0qjR9vth97TErnCcepsn5VOnnStFQkAAAAeZnbNaVmzJiRuN23b19VqFBB33//vWrUqKEePXp4tHMAAAA5yZw50uHDpqj5mDHun9+/v1nyd+WKqTPVsaPn+wgAAJBbZPnBw02bNlXTpk090RcAAIAc659/pClTzPaUKVLhwu5fo2hR6f77pblzpYkTJZvNkz0EAADIXVwKpZYvX+7yBZktBQAA/NErr0h//y3VqeNchpcZr78uPfWUVLmy5/oGAACQG7kUSvXq1culi9lsNsXHx2elPwAAADnSqlWmfeopKSgLc82DgwmkAAAAJBdDqYSEBG/3AwAAIMe6dEn65Rez3aqVb/sCAADgL9x++h4AAEBes3OnFBcnhYdLlSr5ujcAAAD+we3J51OsCp9peO655zLdGQAAgJzo++9N27w5xckBAAA8xe1Q6osvvkj22m63KzIyUkFBQapevTqhFAAA8Dvbtpm2WTPf9gMAAMCfuB1K7d69O8W+mJgY3X///br99ts90ikAAICcwuEglAIAAPAGj9SUCgsL05QpUzRhwgRPXA4AACDH+OMP6cwZ89S8G2/0dW8AAAD8h8cKnZ87d07nz5/31OUAAAByBGuW1I03Svnz+7YvAAAA/sTt5Xuvv/56stcOh0NRUVH6+OOP1blzZ491DAAAICdg6R4AAIB3uB1Kvfrqq8leBwQEqFSpUrrvvvs0fvx4j3UMAAAgJ7BCqebNfdsPAAAAf+N2KBUZGemNfgAAAOQ4Fy5Iv/xitpkpBQAA4FkeqykFAADgb378UUpIkCpUMF8AAADwHLdnSl25ckVvvPGGNmzYoNOnTyshISHZ+7t27fJY5wAAAHyJelIAAADe43Yo9eCDD2rt2rXq27evmjRpIpvN5o1+AQAA+ByhFAAAgPe4HUqtWLFCK1euVIsWLbzRHwAAgBzB4ZC2bzfbFDkHAADwPLdrSpUvX16FCxf2Rl8AAAByjEOHpLNnpXz5pIYNfd0bAAAA/+N2KPWf//xH48aN09GjR73RHwAAgBzBWrrXqJEUEuLbvgAAAPgjt5fvNW7cWFeuXFG1atUUGhqq4ODgZO///fffHuscAACAr1BPCgAAwLvcDqX69++vP//8Uy+88ILKlClDoXMAAOCXvv/etIRSAAAA3uF2KPX9999r27ZtuuGGG7zRHwAAAJ+LiZF+/dVsE0oBAAB4h9s1pWrXrq3Lly97oy8AAAA5wn//KyUkSNddJ5Ur5+veAAAA+Ce3Q6kZM2boiSee0MaNG/XXX38pJiYm2RcAAEBu9847pn34Yd/2AwAAwJ+5vXyvc+fOkqR27dol2+9wOGSz2RQfH++ZngEAAPjAzz9LP/wgBQdL993n694AAAD4L7dDqQ0bNnijHwAAADnCu++atndvqVQp3/YFAADAn7kdSrVu3dob/QAAAPC5ixelTz4x24884tu+AAAA+Du3Q6nvvvsu3fdbtWqV6c4AAAD40rx55sl7NWpIbdr4ujcAAAD+ze1Qqk0qv6HZbLbEbWpKAQCAnOjPPwvqllsC1b279MwzUpJfXySZMGrKFLP9+ONSgNuPgwEAAIA73A6l/vnnn2Sv7Xa7du/erQkTJmjatGke6xgAAIAnzZtXTzt3BuiHH6S//pJeeSV5MPXSS9KZM1KtWtKgQb7rJwAAQF7hdihVpEiRFPs6dOigfPny6fHHH9dPP/3kkY4BAAD/4HBIjz0m7dkj1a0rtWgh9e+fcqaSp5w8Ka1eLW3eLNnt0qxZ0oEDNu3cWVYBAQ4lJNg0a5YUGSndfrtUr550/rz0n/+Y82fMME/eAwAAgHe5HUqlpVSpUjp48KCnLgcAAPzEDz9Ib7xhtjdtkt56SypSRLrtNs/f6/x5qU4d01p275aKFDFr8QYOdKhFC5seeUT68kvzlVSLFlKvXp7vFwAAAFJyO5Tas2dPstcOh0NRUVGaMWOGbrjhBo91DAAA+If5803bpo2ZHbVhg7RkiXdCqXXrTCBVsqT00EPSxx9Lv/0mSQEKCkrQM8/Eq0aNANWrJy1bZkKyY8ekfPmk4sVNYOatGVwAAABIzu1QqkGDBrLZbHI4HMn2N23aVB988IHHOgYAAHKeqChp9mxTc6lKlYyPv3JF+vRTs/3ss6Z4+IYN0vLlUlycFOSxOdvG2rWm7d/fLMMbNEhq1046elTq1OmIKleuKElq2tR8AQAAwHfc/lUwMjIy2euAgACVKlVK+fPn91inAABAznPlitStm7Rrl7RqlbRjR8a1l7780sxcqlhRuvVWKSHBzEj66y/p+++lVq3McVu2SJMmmf3Nm0udO0vdu7vfRyuU6tDBtNWrm+WD33wTpwIFfpVU0f2LAgAAwCvcfthx5cqVk31VrFiRQAoAAD/ncEjDhplASjJ1mmbMyPg8a+neffeZWVJBQc6wadkyKSZG6tdPatnSLL37+WezhK5HD+mbb9zrY2SkdPiwuUebNs79pUtLd93lUHCwI81zAQAAkP1cDqXWr1+vOnXqKCYmJsV758+fV926dbV582aPdg4AAOQMb78tffCBCZaGDTP7nn/ePFEvLSdPSmvWmO3773futwqJL11qAqrPPjPXfeQRs23Ncpo7170+WrOkmjaVChd271wAAABkP5dDqVmzZmnQoEEKCwtL8V6RIkU0ePBgvfLKKx7tHAAA8L3ly51B1LRp5kl6vXpJdrs0cmTa523ebJbrNW5sltFZOnaUChQwdZ6++04KCzNL+d5+W7rjDmnWLHPcV1+ZYMtV1y7dAwAAQM7mcij1yy+/qHPnzmm+37FjR/30008e6RQAAMgZtm41y+sSEqQHHpDGjTNPp3vtNfP+pk3Sn3+mfu6BA6a99uG8oaEmmJJMOPX119LNNzvfr1NHuuUWKT5emjfPtX7Gx5vlfxKhFAAAQG7hcih16tQpBadTzTQoKEhnzpzxSKcAAIDvxcVJffs6C5y/844JpCSpUiWpWTOz/cUXqZ9vhVK1a6d8b/x4U/dp+XJTT+pajzxi2nffNfWrpk2Tfvwx7b7+9JP0zz9SkSLSTTe59PEAAADgYy6HUuXLl9fevXvTfH/Pnj0KDw/3SKcAAIDv/fCDFB1tnpa3eLEpIJ5Unz6mXbo09fP37zdtaqHUzTdLGzZI7dunfm7fvlKxYmaJ3403Ss8+K7VubWpUXbkivfyy9Nhj0uXL5ngrGGvfPmU/AQAAkDO5HEp17dpVzz33nK5cuZLivcuXL2vixInq1q2bRzsHAAB8xypS3r69WXJ3rd69Tbtpk3TmjLRkifTQQ9LFi2a538GD5v3UQqmMFCggDRpktkNCpOuuMwFU9+5med/YsdLrr0vvv2+eDLhokTm2Xz/37wUAAADfcPnfEp999lktXbpUtWrV0vDhw3XdddfJZrNp//79evPNNxUfH69nnnnGm30FAADZyAqlrPpP16pa1cxi2rVLGjJEWrbMhFHNmpkg68oVEyhVrZq5+0+dKnXpIjVoYEKxAQNM8BUZKeXLJ8XGSm++KTVqJB05IhUqJN12W+buBQAAgOzncihVpkwZff/99xo6dKjGjx8vh8MhSbLZbOrUqZPeeustlSlTxmsdBQAA2efcOWnHDrOdViglmdlSu3YlX8L33XdShQpmu1YtKTAwc30IDjZ1pyyLFpnaUpL08MNmBtaBA9KIEWZfz56pz+gCAABAzuRW1YXKlStr5cqV+ueff3To0CE5HA7VrFlTxYoV81b/AACAD6xfb2Y9XX+9VLFi2sf16WPqPUlS3brSvn1mOd+NN5p9mVm6l5agIGniROfr++4zM6Wsh//edZfn7gUAAADvy1Qp0GLFiukmHm0DAIDfymjpnqV2bempp8yT7154QSpTRjp2TFq92vm+twwbZkIpyRRFz6ivAAAAyFl4Pg0AAEjG4ZC++cZsuxL0TJ/u3G7UyCz7s0Itb4ZS118vtW1rZnX17m3qVwEAACD3cPnpewAAIG/46SdTODw4WGrd2r1zW7Uy7f+XnvRqKCVJc+ZIgwdLU6Z49z4AAADwPEIpAACQ6Nw5Z22mXr2kggXdO//aEOu66zzRq7TVqiXNnSuVK+fd+wAAAMDzWL4HAEAe53BIp09LFy5Ijz8uHT4sVa5sZiG5q0ULyWYz16xQQSpUyPP9BQAAgH9gphQAAHncsGFS2bJSjRrSV19J+fJJS5dKJUq4f62iRaUGDcy2t5fuAQAAIHcjlAIAIA/bts05I6pQIalKFemTT6Qbb8z8Ndu2Ne0NN2S5ewAAAPBjLN8DACCPio+XRoww2w8+KL3/vmeu+9xzpsbTffd55noAAADwT4RSAADkUfPmmSfthYVJL7zgueuGhUmjR3vuegAAAPBPLN8DACAPcjjMjCZJmjRJKlPGp90BAABAHkQoBQBAHnT4sBQVZYqaP/qor3sDAACAvIhQCgCAPOinn0wbEWGCKQAAACC7+TSUmj59um666SYVLlxYpUuXVq9evXTw4MEMz9u0aZMaNWqk/Pnzq1q1apo7d2429BYAAP9hhVKNGvm2HwAAAMi7fBpKbdq0ScOGDdP27du1du1axcXFqWPHjrp48WKa50RGRqpr165q2bKldu/eraefflojR47UkiVLsrHnAADkbjt3mpZQCgAAAL7i06fvrV69OtnrefPmqXTp0vrpp5/UqlWrVM+ZO3euKlWqpFmzZkmSrr/+eu3cuVMvv/yy+vTp4+0uAwCQ6zkc0q5dZptQCgAAAL6So2pKnT9/XpJUvHjxNI/Ztm2bOnbsmGxfp06dtHPnTtntdq/2DwAAf3D4sHT+vBQSItWt6+veAAAAIK/y6UyppBwOh0aPHq1bbrlF9erVS/O46OholbnmudVlypRRXFyczp49q/Dw8GTvxcbGKjY2NvF1TEyMJMlut+eKEMvqY27oK1zDmPofxtT/+NOYfvaZTZ98EqDZs+NVqZLZt2OHTVKQ6tdPkM0WLz/4mBnypzGFwZj6H8bU/zCm/olx9T/eGFNXr5VjQqnhw4drz5492rJlS4bH2my2ZK8dDkeq+yVTTH3y5Mkp9q9Zs0ahoaGZ7G32W7t2ra+7AA9jTP0PY+p/cvuY/u9/xfT007coLi5A/fqd1rPP7pDNJn3+eR1JNVWy5FGtXLnH193MVrl9TJESY+p/GFP/w5j6J8bV/3hyTC9duuTScTkilBoxYoSWL1+u7777ThUqVEj32LJlyyo6OjrZvtOnTysoKEglSpRIcfz48eM1evToxNcxMTGqWLGiOnbsqLCwMM98AC+y2+1au3atOnTooODgYF93Bx7AmPofxtT/+MOYnjkjDR8epLg48w82P/1UVgkJt6l7d4deey1QktSrV0V17Zr+/3f9hT+MKZJjTP0PY+p/GFP/xLj6H2+MqbVKLSM+DaUcDodGjBihL774Qhs3blTVqlUzPKdZs2b66quvku1bs2aNGjdunOo3L1++fMqXL1+K/cHBwbnqD1Bu6y8yxpj6H8bU/+TmMR00SDpxQqpVS+rUSXrjDWn06CC1bi3t3m2OadIkSLn042Vabh5TpI4x9T+Mqf9hTP0T4+p/PDmmrl7Hp4XOhw0bpk8++UQLFy5U4cKFFR0drejoaF2+fDnxmPHjx2vgwIGJr4cMGaKjR49q9OjR2r9/vz744AO9//77GjNmjC8+AgAAOc7Bg9LKlVJgoLR0qTR9ulSxonT0qFSypHTunClynk4JRwAAAMDrfBpKzZkzR+fPn1ebNm0UHh6e+LV48eLEY6KionTs2LHE11WrVtXKlSu1ceNGNWjQQM8//7xef/119enTxxcfAQCAHGfBAtN26mSerlewoDR3rpR04nCfPiaYAgAAAHzF58v3MjJ//vwU+1q3bq1du3Z5oUcAAORuDoczlLr7buf+rl2lixelK1cku10qUsQ3/QMAAAAsOaLQOQAA8IwdO6Q//jCzo3r2TP5eYKDZDwAAAOQEPl2+BwAAPOuTT0zbqxcBFAAAAHI2QikAAPyE3S5ZZRnvuce3fQEAAAAyQigFAICf+PRT6exZqXRpqX17X/cGAAAASB+hFAAAfsBulyZPNtujR0tBVI0EAABADkcoBQCAH5g/3xQ4L11aGj7c170BAAAAMkYoBQBADuRwSOfOuXZsbKz0/PNme/x4CpwDAAAgdyCUAgAgBxo3TipRQvryy/SPczjMcr3jx6Xy5aUhQ7KnfwAAAEBWEUoBAJDDHD0qzZolJSRIY8aYelGpcTikUaOkt96SbDbp1Vel/Pmzs6cAAABA5hFKAQCQw0yb5gyiDh2SPvww5TEJCdLIkdLrr5vX770n3XFH9vURAAAAyCpCKQAAcpDISGnePLN9112mnTxZunLFeczVq9I990izZ5vXb78tPfhg9vYTAAAAyCoeGA0AQA4RG2tqScXFSR06mHBqyxbpxAmpalUpKMi8d/mydP68ef3hh9KAAb7uOQAAAOA+ZkoBAJADbNkiNWgg/fe/5vWkSaY+1LRp5nV0tAmnoqNNIFWokPTVVwRSAAAAyL2YKQUAgI8dP25mRl25IpUpI73xhtS8uXlv4EDpxhulS5fMzCjrq0IFKSzMt/0GAAAAsoJQCgAAH/vqKxNINWggbdggFS2a/P169XzRKwAAAMC7WL4HAICPrVpl2n79UgZSAAAAgL8ilAIAwIdiY6X16812586+7QsAAACQnQilAADwoc2bTb2o8HDphht83RsAAAAg+xBKAQDgQ9bSvc6dJZvNt30BAAAAshOhFAAAPmSFUl26+LYfAAAAQHbj6XsAAPjApUvS999L+/dLAQFS+/a+7hEAAACQvZgpBQBANvvgA/OUvQ4dzOtmzaRixXzaJQAAACDbMVMKAIBsdPq09Nhjkt0ulS4tNWggTZzo614BAAAA2Y9QCgCAbPT889KFC1LjxtKOHWbpHgAAAJAX8aswAADZ5NAhae5csz1zJoEUAAAA8jZ+HQYA5BlxcdIrr0g//5z9946Kkh55xPShSxfp1luzvw8AAABATkIoBQDIMz78UHriCWno0Oy977vvStddJ23YIIWESC++mL33BwAAAHIiQikAQJ7x3/+adtcuU2g8O/zvf2aG1L//Sk2aSFu3SvXrZ8+9AQAAgJyMUAoAkCf884+0bp3ZvnpV+u237Lnv+vWmbdFC2rbNFDgH8H/t3XmcjeX/x/H3mcUsDDIagwhJJbJVpLJl3/JV6hvZvkpoEZVKQpsQUlosCUlJWStNpgyVbDFjK4osYezLYDLbuX9/XL8zY4xhzMy5jznn9Xw8zuO+z33f576vaz6O4fO4rs8FAABAUgoA4CMWLTL1nFxiY+157rJlZtusGYXNAQAAgHPxz2MAQIGWlGRGQP37b9ZzEyZIjRpJW7dKX31ljoWGmu369e5vm2VJy5eb/UaN3P88AAAAoCAhKQUAKLASE81Kdk2bSvXrSzt3ZpyzLGnECJMUatBAWrLEHO/f32ztSEr9+ad04IAUFCTVrev+5wEAAAAFCUkpAECB9O+/0r33mhXtJCkuTqpTR/r5Z/P+n39MQkiSDh82daRuukl6+OGM69PS3NtG1yipevWk4GD3PgsAAAAoaEhKAQAKpP79pR9+kAoXlubMkW67zRQz79vXnF+92mxvuskUGZekLl2kG26QQkKkM2ek7dvd20ZXUqphQ/c+BwAAACiISEoBgI+xLGnBAmn+fOnECU+3JncsS1q40OzPmiV16iRFRZlC4lu2mFFSrqRUw4am5tSPP0rPPSf5+0s1a5pz7pzCZ1kZRc6pJwUAAABkRVIKAHxIaqrUr5/0n/9IHTtK4eFSmzamWHhBEh8vHTpkEkzNm5tjJUqY0VKSFB2dkZSqW9fUdGrSRCpUyByrXdtsz01KffaZFBFhRl3lhx07pP37zTPr1cufewIAAADehKQUAPiIf/81iaiJEyWHQ6pcWXI6pcWLzcipgiQ21mxvvNFMxXNp0cJsFy+W1q0z+xcqMF6rltm6klL//is984ypPdW9e8b982L69Iznn9tGAAAAAAZJKQDwImfPSrfeakY/HTuWcTw1VXrwQenrr03B7blzpb/+kgYPNuenTfNMe3PLlUxyJZdcXKOmFiwwiaZixUwNqfO5RkqtWycdPWoSda6i6GfPSp06BejkyUK5bt8ff0ijR5t912p/AAAAADIjKQUAXmTjRpNoWbzYFPfetcvUNurXLyMh9f33ZvqeJPXsabbR0dK+fZf/vIULpc2b8635OeYayeRKLrncfrtUtGjGqnq33WbqTJ2vWjWpYkXp5EmpbVtp1ChzfMwY6frrpT17HJo2rVqu2mZZUp8+UkqKSQ527Jir2wAAAABej6QUAHiRPXsy9rduNYmXoCBpyhSTnPn8c6lBg4xrKleW7r7bTOP75JPLe9bKlVKHDubzrlFGdslupFRgoKkd5XKhqXuu6775RrrqKmnVKungQfOzeuop6YMPzDVbtoTnqm2ffCL99JMUGiq9956ZKgkAAAAgK5JSAOBFdu8228aNpTp1zH5Kiim2/eGHJol0vh49zHb6dDPKJ6e++spsT5yQnnwyd+3NjWPHMvrpWkXvXK4pfFL2SSlJqlrVJKaCg837IUNMsurWW837w4dDL3t1wrNnzX0kadgwqUKFy/s8AAAA4EtISgGAF3GNlLr9dmntWjOC6Z9/zEig3r0v/JlOncyonj//lH79NWfPsazMxdG/+kqaPz9PTc8x19S9SpWk4sWznncVO5fMz+Fi6teXYmLMiCZXcq54cal8eZOd27z58oY5TZ4s7d0rXXONGXUFAAAAIHskpQDAi7iSUuXLm2ljpUqZBMmFkjcuYWGmCLokvf12xvFff5W2bbvwZzZvlv7+24wyciVfunWT7rtPmjTJFFZ3F1dS6vypey6VKknvv2+Kl5cqden71asnPf545tpT1aqZpNSmTTlPSp05I73xhtl/+eWMEVgAAAAALoykFAB4kXOTUpdj4ECznTfPJKIWLDCF0m++2UxHS0oyo6NcBcRdo6SaNTNFwm+7TTp92ny+Tx9pxgxz/vhxM3LJtRJdfnDVkzq/yPm5+vWTHnss98+oXt2VlMr+GtfPwmXCBOnQIZMUcxWQBwAAAJA9klIA4EVym5SqVs2sQmdZ0uDBGQmdtDQz+qd4ccnfXypRwoxAck3V69DBjAhascK8Hn7YHJ8yxWw//FBassTcw+nMa++MS42Uyg+upFR20/fWrDE/i7ZtpaNHTZJu6FBzbvhwU5sKAAAAwMWRlAIAL5GYKB05YvYvNyklSS+8YLbz5pkRP9WqmdX6IiJMAW/LkhISpL59TWLIz09q1858JjDQ1GcaM0YKCJBWr5bi4jJWsktIMDWr8mrbtowphe5MSrmm723e7LhgMm3YMNOnb7+VatSQ7r/fFJR/4AGpc2f3tQsAAADwJiSlAMBL/POP2YaFScWKXf7n77zTvCSTWJoxQ/rvf81Kd9u2SfHxZqqeq/bSnXdKV1+d+R6lSmUkqh56SNq3L+Pc6tWX36bzDRpkkmNt20qRkXm/X3aqVJECAtJ0+rRDu3ZlPrdpkxQVZX4OFSuaPqalmVFis2aZEWUAAAAALo2kFAB4ifOLnOfGyJFSyZJmxJOrZlNwsEnSREaapNCSJVLTphnT1c73yCNmu3Wr2YaFme2aNblrk8uyZdKiRSbp89ZbebvXpQQESOXLn5IkbdyY+dyYMWbbsaOpb9Wnj/lZTJ9uPgcAAAAgZ/jnMwB4id27zTY3U/dc7rpLOnz44tfcc495ZadFC6lsWTOCKCBAevVVacCAvCWlnM6MYuyPPSbdeGPu75VT116boL//Lq6NG03tLEnau1f67DOz/9xzptbWhx+6vy0AAACAN2KkFAB4idwWOc9v/v7So4+a/f/+V7r3XrO/YYOpTZUbCxeaOlZFi5pC4naoUCFBUuaRUuPHS6mpUoMG0u2329MOAAAAwFsxUgoAvIQrKXXttZ5th2RW8LvxRql1a6lIETMl8MgRk5iqW/fy7+cqmN6vX9Y6Vu7iSkpt2GDeHz6cMSrq+eftaQMAAADgzRgpBcDn/fWXmZKVlubpluTNlTJSSjKr8T34oKkn5XBkJKIuNYXv55+lBQsyx2LbNumHH8x9HnvMbU3OomLFk/Lzs7R9u/TFF9K4cWaFw1tvlVq1sq8dAAAAgLciKQXApzmdUvv2UpcupvZRQXYlJaXO55rqtmaNlJwsnTiR9ZrNm6UmTaT//EeqXl366iuz0t7EieZ8mzZShQp2tVgqWjRZzz3nlGSmI773njk+dGjuC8kDAAAAyEBSCoBPi4rKWCXutdekmBjPtie3nE7pn3/M/pWclPrmG1MEvWRJUyPKxemU+vY19Zok6Y8/pE6dTO2m6dPNsX79bG2yJGnYMKfuvFM6dUo6fVqqVUtq29b+dgAAAADeiKQUAJ82frzZhoebUTldukiHDrn/ubNnm8RMUJBZoa5xY2n+/MufQuh0mlX3Dh40I5D8/KQyZdzT5ry47TazPXHC1JZKS5O+/jrj/IwZ0i+/SKGh0qZN0ssvSyEh5tiJE1LFimZVP7sFBJipnVddZd4zSgoAAADIPySlAPiszZul6GiTyPn5Z+mmm6T4eKldOykhwX3PTUyUBgyQjh41iaS0NGnZMqljR6lZM5NoyqmnnjJT2po3N+/LlDH1nK404eHS669L998vdetmjrnqS504IQ0aZPaHD5eqVTNTKbdtM6v3BQRIr7xi4uQJ5cub5Nj8+VKHDp5pAwAAAOCNSEoB8EmWJY0da/Y7djQJqblzTfJkzRqTmEpMdM+z33tPOnDAJJN27TLJlxdfNCODYmLMlMKciI3NWJVu82azvRKn7rm89JL05ZcZ0/DWrDFxWLTIjJ6qUkV6+umM68uVkz7/XEpKkrp29UiT01WtSkIKAAAAyG8kpQAUWL//blZEu1DR7Iv59FOpRo2MWkUDBpjtTTdJ338vFS0q/fST9Pjj+dla4+RJadQosz98uHTttSYZM2KEqakkZUwpdNm6VapcWRozJuOYZUnPPGO2bdpI995rjterl/9tzm81apjRXIcPm6mH0dHm+H33XXiUl6dGSAEAAABwL/6pD8B2Z85Iv/6as2vPnpUWLpSOH898/OBBs1LbM8+YldqWLs3Z/aKizKibTZtM/aKXX5buuCPjfJ06ZjSPZLbJyTm7b06NGycdOybdeKP08MOZzz35pEnAREdnjHySzIiuHTvMFLfly82xRYvMqKrgYOn996UFC8zqe2+9lb/tdYfgYJOYkqTVqzOSUs2aea5NAAAAAOxHUgqA7R58ULrzTlPs+2ISE80ooA4dpFtvzVglz+mUunc3iSlJ2rtXuuceUyz7UqZMyWjD3r2mdtH5haubNpWuvtokz1atuqyuXZRlSZMnm/1XXpH8/TOfr1BB+s9/zL5rtNSZMxk/J8syCbVJk6SePc2xgQPNaCvJTHcrKKOKXKvxffyxiWNoqFS/vmfbBAAAAMBeBeS/LwC8xc8/S99+a/ZdCaILSUw0dZ1cI6D+/tuMaBo2TOrVy0yzCwkxI2169DDXPP/8xetAHTmSseLb4MEZK6qdz8/PJLkk6Ycfcty1S9q0ydSSCg3NmG53PtdUwk8/NSOfvvpKOn1aqlTJTOH75x+pTx8zcuy226QXXsi/9tnJlZRassRsGzY0KxECAAAA8B0kpQC4hWWZQtaff24Ke//2mzn28ssZ18TEmNFKF9Kzp0lIhYWZ6Xv165vaUa++mlELavx4k9yYNMmMMjp4UJo4Mfs2ff65lJIi1a4t3XLLxdvvmkrmmlqWH1z3atAg+wRM/fpmFFlSklmpbtIkc7xXL2nWLFNzKShIevNNacUK8/MpiG67LfN7pu4BAAAAvsejSamffvpJ7dq1U5kyZeRwOLRgwYKLXr9s2TI5HI4sr62uOT0ArgiWZVZRq1tX6tzZ1Eq6/Xazyt3y5VKhQlK1aua6zz7L+vlvv5XmzDHT2777TmrfXvrxR5OI6d1b+t//TJ2lRx811xcqJA0ZYvZHjjRT3i5k2jSzdY2suhhXkmTNmssvpJ4dV1KqefPsr3E4pJkzzSiutWullSvNyK1u3czP8I8/zKixF164cFHwguKGGzIn1EhKAQAAAL7Ho0mpM2fOqEaNGnrvvfcu63Pbtm1TfHx8+uv66693UwsB5MaQIdK775r9hg1NjSbLMsW4Jemxx6SnnjL7n36a+bOJiRmr3g0caEYNSaY49gsvmJFDU6eac+fWgurWzUxxO3zYFP4+34YNUmysSeQ89NCl+1CunEmcOJ1mRFdenT1rVvSTLp2AqVjRjOpy9a9FC+maa8z+dddJZcrkvT2e5u9v6oRJUunS0s03e7Y9AAAAAOzn0aRUq1at9Prrr6tjx46X9bmIiAhFRkamv/zPrxYMwHaHD5uEUevW0ogR5tgHH0jLlpkRQlFRUvnyUtmy0osvSp06mRFOmzaZhJHLq69Ku3eba4cNy/nzAwOloUPN/ujR0qlTmc/PmWO27dpJJUvm7J5Nm5ptftSVWrFC+vffnCdgWrQwK/WFhxfculGX4ips3qJF1mLzAAAAALxfgKcbkBu1atXS2bNnVbVqVQ0ZMkSNGzfO9tqkpCQlJSWlv09ISJAkpaSkKCUlxe1tzStXGwtCW5Ez3hBTp1P65huHqla1VLmy9P33DnXv7q9jxzIyCyNGpOmRR5xydbNJE+mvv0ytpOBgc6xNG3/Nn++nwYOdmj07TfPnOzR6tL8kh95+O1WFClm6nB/TAw9Ir78eoO3bHXrnnTQ9/7wz/dzPP/tL8lOLFqlKSbFydL/GjR16//0ALVliKSUlNdvrchLTqCg/Sf665x6nUlPTcvT8xx/PGDVWgP+4ZGvAACk42E89ezqvuP55w/cUmRFT70NMvQ8x9T7E1DsRV+/jjpjm9F4Oy7Jy9r8zN3M4HJo/f746dOiQ7TXbtm3TTz/9pDp16igpKUkzZ87UxIkTtWzZMjVo0OCCnxk+fLheeeWVLMc/++wzhYaG5lfzAZ/yxRdV9PnnN0mSrr/+uLZvLy7LcqhcuQQ1aLBPt98er2uvPXWJu0hbt16lIUPuUmqqnypWPKHdu4vK6fRT69Z/q3fvTblq27Jl12j8+DoqUiRZkydHKzQ0VSkpDnXp0kbJyf56770fdc01p3N0r8TEAHXp0lqW5dC0aVG66qqkS3/oHAcOhOrVV+9Q4cIpOnYsWEePhujpp9epUaNsqrsDAAAAgBdITExU586ddfLkSRUtWjTb6wpUUupC2rVrJ4fDoUWLFl3w/IVGSpUrV05Hjhy56A/mSpGSkqLo6Gg1a9ZMgQW5qjHSFfSY7tol3XJLgM6ezTzfqnfvNI0d68x2VbnsxMQ41KmTvxISzP06d3bq44/T5JfLycVpaVKNGgH680+Hhg1L00svObVmjUN33RWg8HBL+/enXtZUsRo1AvTHHw7Nn5+qNm0y/3VpWWbaWXYxHTzYT2PGZJ5evGdPiiIjc9c32Kegf0+RFTH1PsTU+xBT70NMvRNx9T7uiGlCQoJKlix5yaRUgZy+d6569erp0/MrJZ8jKChIQRf4X3JgYGCB+gIVtPbi0gpqTJ9/3hTtbtzYrGb3+edSlSpSx47+ki6/vlvz5tLPP5sV8WrVkiZO9FNgYO7L3QUGSq+8YoqZv/OOv555xl+rV5tz9es7VKjQ5f3MXSvexcUF6Nyc+S+/SPffbwqsv/GG69kZMbUsaf58c7xPH+nkSal2balcuYIXc19WUL+nyB4x9T7E1PsQU+9DTL0TcfU++RnTnN6nwCelYmNjVbp0aU83A/AJS5aYRIu/vzRhgnTttflThPuWW6T16/N+H5dOnUzR87/+kj77zBQZlzJW8rsct94qzZgh/fZbxrFt26T27aXjx6Xx4zPqPqWlmdpPoaGmePuOHaZ+1ltvSUWK5LlbAAAAAOBVPJqUOn36tLZv357+fufOnYqLi1OJEiVUvnx5vfjii9q3b58++eQTSdL48eNVoUIF3XzzzUpOTtann36quXPnau7cuZ7qAuAz9u2Tunc3+088kbMV5DzF31/q188U0v7gA+ngQXM8t0kpySSlLEs6dEhq1cokpCSThJo40U+33SZ16OCvFSvMaoNff23Ot2pFQgoAAAAALsSjSanffvst08p5AwcOlCR1795d06dPV3x8vPbs2ZN+Pjk5Wc8++6z27dunkJAQ3Xzzzfr222/VunVr29sO+JKzZ6WOHaUDB6Rq1aTXX/d0iy6te3dp8GBp40bzvlChjATT5ahRwyS5Dh40iblXX5V27pSuu0569lmpb19pyhQ/7d5dRd9/b6YdPvCAFPD/f7t26pRPHQIAAAAAL+PRpFSjRo10sTrr06dPz/R+0KBBGjRokJtbBeB8TzwhrVkjlSghLVxYMEb+XHWV1LmzNHWqeV+njplKd7lCQkwibsMGadkyU0NLkqZMke6+Wxo5Utq925G+GmGxYtI//5hrgoKkNm3y3hcAAAAA8Ea5ryYMwCfExJjEjsMhffGFVKmSp1uUc/36ZezXr5/7+7hGWA0dKp0+LVWuLDVqZEZDPfVUxnVt2zq1bJnSVyBs0UIqAIt8AgAAAIBHkJQCkK2UFDNKSjLT1Jo29Wx7Llft2tJdd5n9Fi1yfx9XUmrnTrPt1csk6Vz7pUtbCg//VxMnpqlmTbMq4XXXSc88k/tnAgAAAIC3K/Cr7wFwn3fflX7/XSpZsmDUkbqQ+fOlTZukc8rXXbZza1H5+2cUfJfMdL1Nm1IVHb1UERHNJUkPPWReAAAAAIDsMVIKQBaWZabsDR1q3o8aZWo0FUQlS+YtISVJ1atLgYFmv00bqXTpzOeLFpVCQlLz9hAAAAAA8DEkpQBksn27dM890iOPSImJUvPmUo8enm6VZwUFZdSk6tvXs20BAAAAAG9BUgqAJCk11YyIql7dFDcPCZHGjpW+/Vby428KzZplVt9r2dLTLQEAAAAA70BNKQCSzAigjz4y+02bSpMmFayV9tytbFnzAgAAAADkD5JSABQdbRJSDoc0ZYr0v/9lrC4HAAAAAIA7kJQCfNyZM1Lv3mb/8celXr082x4AAAAAgG+gUgzg44YOlXbtksqVk0aM8HRrAAAAAAC+gqQU4MOOH5fef9/sT5wohYV5tj0AAAAAAN9BUgrwYbNnS0lJZsW9Vq083RoAAAAAgC8hKQX4sGnTzLZnTwqbAwAAAADsRVIK8FGbNklr10oBAdLDD3u6NQAAAAAAX0NSCvBySUnSX39JcXGSZWUcd42Sat9euvpqjzQNAAAAAODDSEoBXiotzUzLCw6WqlSRatWSxo415/79V5o50+z/73+eayMAAAAAwHeRlAK81IsvStOnm/3gYLMdNkzas0caPFg6ckQqV05q0cJjTQQAAAAA+DCSUoAXmjZNeustsz9rlnTmjHT33VJiotShgzR+vDk3aZKpKQUAAAAAgN1ISgFe5u+/pT59zP7LL0udO0t+ftIHH5gEVGysOffoo1KrVp5rJwAAAADAt5GUAvLBunXS0qWZC4nb5e+/pUWLpORk837YMLPfpIk0fHjGddWqSU8/bfYrVMioLwUAAAAAgCeQlALy6J9/pDvvlO65R6pfX/rlF/c/07KkyZOlqlWl666T7r1X6thRWr/eTNeTpNGjzQipc73+upm69/33UliY+9sJAAAAAEB2qCYD5NFbb0lJSWZ/1SqpQQPpxx+lxo3z9zm//y4VKiSVLSs98YT08cfmuL+/ST59+60UE2MSVvffL9Wpk/UeQUFS//752y4AAAAAAHKDkVJAHhw8KE2ZYvZnzpTatTNJocGDM0/lsyxp4kRp6tTcPSc6Wrr5Zun6680Ip48/NomoESPMKnpff20SVomJ5vhrr+W9bwAAAAAAuBMjpYA8GDdOOntWqltX6tJFatpUqlTJjJj67jupdWvJ6ZT69jXT7SSpeHFHphXvEhOljz4yq+PVqnXh57z9dsZ+WppUvLg0e7bUooU51qKF9OWXpg2PPSbdeKNbugsAAAAAQL4hKQVcplOnpHnzzCp2H31kjr30kuRwSJGR0uOPS2PGSEOHmpFNr71mRlG5PPmkv956y3z1/vpLuu8+adMmKTjY3Pf8FfF27pSiosz+H3+Y6Xply0qhoZmva99eOnkyax0pAAAAAACuRCSlgBz691/pgw+kkSPNlDmX2rWltm0z3g8aJH34oVmRr0oVc8zf34yUGjVK+vNPh959t7ZWrvTTzJlSQoI5f/asKVj+2WemJpTL5Mlm+l+zZpceAUVCCgAAAABQUPBfWCAHYmPN1LpnnzUJqcqVpQEDpGnTzEp2DkfGtVdfLT33nNkPDJSaNDE1n/73v4yRVWvWlNb77/srIcGs3Ldjh/Tgg1JKitS1q3T0qLkuKSmjDlXfvvb1FwAAAAAAd2OkFJANy5I2bJDmzDHT8VJSpNKlpddfl7p1U6a6UOd7+WUzeqpKFVOY3OXuu6XXXkvT1Kmn1aJFEbVo4a+2bU3yatYsaetW88zPPzcr7M2bJx0+bKbrtWvn/j4DAAAAAGAXklLABezeLbVsaZJELh06mJFO4eGX/ryfn1SnzoXPPf+8U9WrL1Pr1q0VGOifftzf34ym6t9fmj7djIwaMcKc69374kkwAAAAAAAKGqbvAec5e9YUH9+6VQoJMQXEZ882o5ZykpDKi86dzaipdeukIUOkzZvNSntPPune5wIAAAAAYDfGXgDnefppkxQKD5fWr5fKl7fv2SVLmml68+aZguqSqU911VX2tQEAAAAAADswUgo4x7x50qRJpnD5rFn2JqRcevbM2I+IkJ56yv42AAAAAADgbiSlgP9nWdIrr5j955+XWrTwTDtatpQiI83+iy9KRYp4ph0AAAAAALgT0/eA/xcdLW3cKBUubKbMeUpAgPTFF9Kvv0qPP+65dgAAAAAA4E4kpVCgnDkjFSpkioHntzFjzLZXL6lEify//+Vo0MC8AAAAAADwVkzfQ4Hx99/SNddId90lJSXl773j4sxIKX9/acCA/L03AAAAAADIiqQUCoyhQ6UTJ6Q1a6QhQ8yxw4elRYukDRvylqhyrXTXqZNUoUJeWwoAAAAAAC6F6XsoEDZskD77LOP92LFmVNPkydLx4+ZYoUJm5bwePS7v3itWmBpODocpcA4AAAAAANyPkVIoEF56yayO98AD0qOPmv1Ro0xCqnx5qXhxKTlZeuIJaf9+85kNG6QlSySn07yfNUuqU0eaMSPjvk6n1L+/2f/f/6SaNe3sFQAAAAAAvouRUrjiRUVJ335rRka9/rpUurS0apW0fbv06qvS009Lfn6m1tTKldILL0idO0v33msSVbVqSddfL82ZY+7Xo4dJRvXsKU2dKq1bJxUtKo0Y4cleAgAAAADgW0hK4Yq2dq2p8yRJffqY5JLruNMphYRkXPvOO9Ltt0szZ5oEVHKySVbFxpqXn590993S8uVmhb2RI6U//zSfHTZMioiwt28AAAAAAPgypu/hivXHH1KrVtLp09I995g6Ui5BQZkTUpJ0221m9JNkip7fe6+0d6/03HNSgwZSTIx59e1rpv/9+adJVHXqZKb9AQAAAAAA+zBSClekEyekdu2ko0fN6KcFC0wi6lLefFPaskW66SZT9DwoSBo9OvM1770n3XGHFBwsNW0qXXWVO3oAAAAAAAAuhqQUrjhOp9Stm7Rjh3TttaaeVJEiOftsqVLS6tUXv8bPT+raNe/tBAAAAAAAucf0PVxxRo6Uvv7ajHKaO1cqWdLTLQIAAAAAAPmNpBSuKFu3mqLjkvT++1KdOp5tDwAAAAAAcA+SUrhiWJb05JNSaqqpJ9Wrl6dbBAAAAAAA3IWkFPLF2bNmhNPtt0uVKkkVKkg//3x595g7V/rhBzNtb/x4d7QSAAAAAABcKSh0jjxxOqXly6V+/czUu3M99JC0aVPOVrf7919p4ECz/8ILJrEFAAAAAAC8FyOlkCunTkm9e5vV7po0MQmpyEjp44/NCKnrr5f27ZP69jXT8k6fltLSsr/f9OnSP/9I5ctLzz9vWzcAAAAAAICHkJRCrrz0kjRlinTkiBQWJj36qLRli9Szp3TXXdKnn0r+/tIXX0hlyphrypWT3nnHjIo6V1qaNHas2X/uOSkkxP7+AAAAAAAAe5GUwmXbvl368EOzP3OmdPSoNHmyVKJExjW3356xit6BA2YbHy89/bRUtap08GDGtfPnSzt2mM/37GlLFwAAAAAAgIeRlMJle+kls0Jey5bSww9LgYEXvm7IECkqSlqxwiShJk0yo6Z27ZJeftlcY1nS6NFm/4knpMKFbekCAAAAAADwMAqd47KsXSvNmSM5HNKoURe/1uGQWrTIeN+7txkldffd0kcfSY8/bgqhr10rBQebpBQAAAAAAPANjJRCtizLJKB++MG8dzql/v3Nfvfu0i23XP4977pLeuABc+927aSuXc3xfv2kq6/On3YDAAAAAIArHyOlkO7MGbOqXqlS5v3zz0tvvWVGPH3zjXTokLRypVSkiPT667l/zqhR0sKFZrU9ydSZck3hAwAAAAAAvoGklI9zOk3y6aOPpBMnzLFq1aQqVaR588x7y5I6d5YKFTLvhw6VypbN/TMrVDDJrtdek155RerbNy89AAAAAAAABRFJKR/mdEqPPWYSUufavNm8JOmdd8wUvhUrzPsbb8yYwpcXTz5pakg5HHm/FwAAAAAAKHioKeWjnE6pTx+TkPLzkz7+WDp5Ujp+XJo82dR7mjlTeuopae5c6ZprTAJpwoSMEVN5RUIKAAAAAADfxUgpH+R0msLiU6aYhNTMmWZ6nsujj5qXS6lS0rp10v79Us2atjcXAAAAAAB4IZJSPsayzLS5SZPMSKUZMzInpLITEWFeAAAAAAAA+cGj0/d++ukntWvXTmXKlJHD4dCCBQsu+Znly5erTp06Cg4OVqVKlTRx4kT3N9SLDBkiffihSUhNny49/LCnWwQAAAAAAHyRR0dKnTlzRjVq1FDPnj113333XfL6nTt3qnXr1nr00Uf16aefasWKFerXr5+uvvrqHH3e2x07Jg0aJF19tdSpk9muXm1W1WvUSPrhB2nECHPtlClSt26ebC0AAAAAAPBlHk1KtWrVSq1atcrx9RMnTlT58uU1fvx4SdJNN92k3377TWPGjPH5pJTTKXXtKi1ebN6PHJn9tcOHS7162dIsAAAAAACACypQNaVWrlyp5s2bZzrWokULTZ06VSkpKQoMDMzymaSkJCUlJaW/T0hIkCSlpKQoJSXFvQ3OB642Xqqto0f7afFifwUFWWrRwlJ0tEPJyVL16lJYmKVVqxxKSXGoZ0+nXnwxTQWg614rpzFFwUFMvQ8x9T7E1PsQU+9DTL0PMfVOxNX7uCOmOb2Xw7IsK9+emgcOh0Pz589Xhw4dsr2mSpUq6tGjhwYPHpx+7Ndff9Wdd96p/fv3q3Tp0lk+M3z4cL3yyitZjn/22WcKDQ3Nl7Z72saNJTV8eH05nQ49/nismjXbo5QUPzmdUlCQU5L0778B2revsK677qQcDg83GAAAAAAAeK3ExER17txZJ0+eVNGiRbO9rkCNlJJM8upcrpza+cddXnzxRQ0cODD9fUJCgsqVK6fmzZtf9AdzpUhJSVF0dLSaNWt2wZFgsbFS164Bcjodevhhp8aNqyaHo5oHWoqculRMUfAQU+9DTL0PMfU+xNT7EFPvQ0y9E3H1Pu6IqWuW2qUUqKRUZGSkDhw4kOnYoUOHFBAQoPDw8At+JigoSEFBQVmOBwYGFqgv0IXa++efUtu20qlTUsOG0pQpfipUyKMLKuIyFLQ/g7g0Yup9iKn3Iabeh5h6H2LqfYipdyKu3ic/Y5rT+xSoDMYdd9yh6OjoTMeWLFmiW2+91ee+DKdOSc2bS4cPS7VrS4sWScHBnm4VAAAAAABAzng0KXX69GnFxcUpLi5OkrRz507FxcVpz549kszUu27duqVf36dPH+3evVsDBw7UH3/8oY8//lhTp07Vs88+64nme1RYmDRggHTjjdJ330kFYCYiAAAAAABAOo8mpX777TfVqlVLtWrVkiQNHDhQtWrV0tChQyVJ8fHx6QkqSapYsaIWL16sZcuWqWbNmnrttdf07rvv6r777vNI+z2tf39TUyoiwtMtAQAAAAAAuDwerSnVqFEjXWzxv+nTp2c51rBhQ61fv96NrSpYmLIHAAAAAAAKogJVUwoAAAAAAADegaQUAAAAAAAAbEdSCgAAAAAAALYjKQUAAAAAAADbkZQCAAAAAACA7UhKAQAAAAAAwHYkpQAAAAAAAGA7klIAAAAAAACwHUkpAAAAAAAA2I6kFAAAAAAAAGxHUgoAAAAAAAC2IykFAAAAAAAA25GUAgAAAAAAgO1ISgEAAAAAAMB2JKUAAAAAAABgO5JSAAAAAAAAsB1JKQAAAAAAANiOpBQAAAAAAABsR1IKAAAAAAAAtgvwdAPsZlmWJCkhIcHDLcmZlJQUJSYmKiEhQYGBgZ5uDvIBMfU+xNT7EFPvQ0y9DzH1PsTU+xBT70RcvY87YurKubhyMNnxuaTUqVOnJEnlypXzcEsAAAAAAAC816lTp1SsWLFszzusS6WtvIzT6dT+/fsVFhYmh8Ph6eZcUkJCgsqVK6d//vlHRYsW9XRzkA+Iqfchpt6HmHofYup9iKn3Iabeh5h6J+LqfdwRU8uydOrUKZUpU0Z+ftlXjvK5kVJ+fn665pprPN2My1a0aFG+8F6GmHofYup9iKn3Iabeh5h6H2LqfYipdyKu3ie/Y3qxEVIuFDoHAAAAAACA7UhKAQAAAAAAwHYkpa5wQUFBGjZsmIKCgjzdFOQTYup9iKn3Iabeh5h6H2LqfYip9yGm3om4eh9PxtTnCp0DAAAAAADA8xgpBQAAAAAAANuRlAIAAAAAAIDtSEoBAAAAAADAdiSl3OzNN9/UbbfdprCwMEVERKhDhw7atm1bpmssy9Lw4cNVpkwZhYSEqFGjRtqyZUumayZPnqxGjRqpaNGicjgcOnHiRJZntW/fXuXLl1dwcLBKly6trl27av/+/e7snk+yM6YuSUlJqlmzphwOh+Li4tzQK99mZ0wrVKggh8OR6fXCCy+4s3s+ye7v6bfffqu6desqJCREJUuWVMeOHd3VNZ9lV0yXLVuW5Tvqeq1du9bd3fQpdn5P//zzT917770qWbKkihYtqjvvvFMxMTHu7J5PsjOm69evV7NmzVS8eHGFh4erd+/eOn36tDu755PyI6bHjh3Tk08+qRtuuEGhoaEqX768nnrqKZ08eTLTfY4fP66uXbuqWLFiKlasmLp27XrRfx8j9+yM6xtvvKH69esrNDRUxYsXt6N7PsmumO7atUu9evVSxYoVFRISouuuu07Dhg1TcnJyrttOUsrNli9frscff1yrVq1SdHS0UlNT1bx5c505cyb9mtGjR2vcuHF67733tHbtWkVGRqpZs2Y6depU+jWJiYlq2bKlBg8enO2zGjdurDlz5mjbtm2aO3euduzYofvvv9+t/fNFdsbUZdCgQSpTpoxb+gP7Y/rqq68qPj4+/TVkyBC39c1X2RnTuXPnqmvXrurZs6c2bNigFStWqHPnzm7tny+yK6b169fP9P2Mj4/XI488ogoVKujWW291ez99iZ3f0zZt2ig1NVVLly7VunXrVLNmTbVt21YHDhxwax99jV0x3b9/v5o2barKlStr9erVioqK0pYtW9SjRw93d9Hn5EdM9+/fr/3792vMmDHatGmTpk+frqioKPXq1SvTszp37qy4uDhFRUUpKipKcXFx6tq1q6399RV2xjU5OVmdOnVS3759be2jr7Erplu3bpXT6dSkSZO0ZcsWvf3225o4cWKO/k+bLQu2OnTokCXJWr58uWVZluV0Oq3IyEhr5MiR6decPXvWKlasmDVx4sQsn4+JibEkWcePH7/ksxYuXGg5HA4rOTk539qPrNwd08WLF1s33nijtWXLFkuSFRsb645u4BzujOm1115rvf322+5qOrLhrpimpKRYZcuWtT766CO3th9Z2fX7NDk52YqIiLBeffXVfG0/snJXTA8fPmxJsn766af0YwkJCZYk64cffnBPZ2BZlvtiOmnSJCsiIsJKS0tLPxYbG2tJsv766y/3dAaWZeU9pi5z5syxChUqZKWkpFiWZVm///67JclatWpV+jUrV660JFlbt251U2/g4q64nmvatGlWsWLF8r3tuDA7YuoyevRoq2LFirluKyOlbOYa+laiRAlJ0s6dO3XgwAE1b948/ZqgoCA1bNhQv/76a66fc+zYMc2aNUv169dXYGBg3hqNi3JnTA8ePKhHH31UM2fOVGhoaP41Ghfl7u/pqFGjFB4erpo1a+qNN97I03BX5Iy7Yrp+/Xrt27dPfn5+qlWrlkqXLq1WrVplmYqC/GfX79NFixbpyJEjjMCwgbtiGh4erptuukmffPKJzpw5o9TUVE2aNEmlSpVSnTp18rcTyMRdMU1KSlKhQoXk55fxX5mQkBBJ0i+//JIfTUc28iumJ0+eVNGiRRUQECBJWrlypYoVK6a6deumX1OvXj0VK1YsT3+HI2fcFVd4jp0xPXnyZPpzcoOklI0sy9LAgQN11113qVq1apKUPmy8VKlSma4tVapUroaUP//88ypcuLDCw8O1Z88eLVy4MO8NR7bcGVPLstSjRw/16dOHKSM2cvf3tH///po9e7ZiYmL0xBNPaPz48erXr1/+NB4X5M6Y/v3335Kk4cOHa8iQIfrmm2901VVXqWHDhjp27Fg+9QDns+P3qcvUqVPVokULlStXLvcNxiW5M6YOh0PR0dGKjY1VWFiYgoOD9fbbbysqKor6Jm7kzpg2adJEBw4c0FtvvaXk5GQdP348fepIfHx8PvUA58uvmB49elSvvfaaHnvssfRjBw4cUERERJZrIyIimGbrZu6MKzzDzpju2LFDEyZMUJ8+fXLdXpJSNnriiSe0ceNGff7551nOORyOTO8ty8pyLCeee+45xcbGasmSJfL391e3bt1kWVau24yLc2dMJ0yYoISEBL344ot5bidyzt3f0wEDBqhhw4a65ZZb9Mgjj2jixImaOnWqjh49mqd2I3vujKnT6ZQkvfTSS7rvvvtUp04dTZs2TQ6HQ19++WXeGo5s2fH7VJL27t2r77//Pkt9DOQ/d8bUsiz169dPERER+vnnn7VmzRrde++9atu2LQkMN3JnTG+++WbNmDFDY8eOVWhoqCIjI1WpUiWVKlVK/v7+eW47Liw/YpqQkKA2bdqoatWqGjZs2EXvcbH7IP+4O66wn10x3b9/v1q2bKlOnTrpkUceyXV7SUrZ5Mknn9SiRYsUExOja665Jv14ZGSkJGXJTh46dChLFjMnSpYsqSpVqqhZs2aaPXu2Fi9erFWrVuWt8bggd8d06dKlWrVqlYKCghQQEKDKlStLkm699VZ17949H3qA89n1PT1XvXr1JEnbt2/P031wYe6OaenSpSVJVatWTT8WFBSkSpUqac+ePXlpOrJh5/d02rRpCg8PV/v27XPfYFySHb9Pv/nmG82ePVt33nmnateurQ8++EAhISGaMWNG/nQCmdjxPe3cubMOHDigffv26ejRoxo+fLgOHz6sihUr5r0DyCI/Ynrq1Cm1bNlSRYoU0fz58zOVGImMjNTBgwezPPfw4cN5/rcWsufuuMJ+dsV0//79aty4se644w5Nnjw5T20mKeVmlmXpiSee0Lx587R06dIsvygrVqyoyMhIRUdHpx9LTk7W8uXLVb9+/Tw/WzLz7pF/7Irpu+++qw0bNiguLk5xcXFavHixJOmLL77QG2+8kT+dgSTPfk9jY2MlZSQ3kD/simmdOnUUFBSUacndlJQU7dq1S9dee23eO4J0dn9PLcvStGnT1K1bN/6B7SZ2xTQxMVGSMtUfcr13jXZE/vDE79NSpUqpSJEi+uKLLxQcHKxmzZrlqQ/ILL9impCQoObNm6tQoUJatGiRgoODM93njjvu0MmTJ7VmzZr0Y6tXr9bJkyfz/G8tZGVXXGEfO2O6b98+NWrUSLVr19a0adOy/H7NTePhRn379rWKFStmLVu2zIqPj09/JSYmpl8zcuRIq1ixYta8efOsTZs2WQ899JBVunRpKyEhIf2a+Ph4KzY21poyZUr6CjKxsbHW0aNHLcuyrNWrV1sTJkywYmNjrV27dllLly617rrrLuu6666zzp49a3u/vZldMT3fzp07WX3PTeyK6a+//mqNGzfOio2Ntf7++2/riy++sMqUKWO1b9/e9j57Ozu/p/3797fKli1rff/999bWrVutXr16WREREdaxY8ds7bO3s/vv3h9++MGSZP3++++29dHX2BXTw4cPW+Hh4VbHjh2tuLg4a9u2bdazzz5rBQYGWnFxcbb325vZ+T2dMGGCtW7dOmvbtm3We++9Z4WEhFjvvPOOrf31BfkR04SEBKtu3bpW9erVre3bt2e6T2pqavp9WrZsad1yyy3WypUrrZUrV1rVq1e32rZta3uffYGdcd29e7cVGxtrvfLKK1aRIkWs2NhYKzY21jp16pTt/fZmdsV03759VuXKla0mTZpYe/fuzXRNbpGUcjNJF3xNmzYt/Rqn02kNGzbMioyMtIKCgqwGDRpYmzZtynSfYcOGXfQ+GzdutBo3bmyVKFHCCgoKsipUqGD16dPH2rt3r4299Q12xfR8JKXcx66Yrlu3zqpbt65VrFgxKzg42LrhhhusYcOGWWfOnLGxt77Bzu9pcnKy9cwzz1gRERFWWFiY1bRpU2vz5s029dR32P1370MPPWTVr1/fhp75LjtjunbtWqt58+ZWiRIlrLCwMKtevXrW4sWLbeqp77Azpl27drVKlChhFSpUyLrlllusTz75xKZe+pb8iGlMTEy299m5c2f6dUePHrW6dOlihYWFWWFhYVaXLl2s48eP29dZH2JnXLt3737Ba2JiYuzrsA+wK6bTpk3L9prccvx/BwAAAAAAAADbUFMKAAAAAAAAtiMpBQAAAAAAANuRlAIAAAAAAIDtSEoBAAAAAADAdiSlAAAAAAAAYDuSUgAAAAAAALAdSSkAAAAAAADYjqQUAAAAAAAAbEdSCgAAAAAAALYjKQUAAGCDHj16yOFwyOFwKDAwUKVKlVKzZs308ccfy+l05vg+06dPV/Hixd3XUAAAAJuQlAIAALBJy5YtFR8fr127dum7775T48aN1b9/f7Vt21apqamebh4AAICtSEoBAADYJCgoSJGRkSpbtqxq166twYMHa+HChfruu+80ffp0SdK4ceNUvXp1FS5cWOXKlVO/fv10+vRpSdKyZcvUs2dPnTx5Mn3U1fDhwyVJycnJGjRokMqWLavChQurbt26WrZsmWc6CgAAkAMkpQAAADyoSZMmqlGjhubNmydJ8vPz07vvvqvNmzdrxowZWrp0qQYNGiRJql+/vsaPH6+iRYsqPj5e8fHxevbZZyVJPXv21IoVKzR79mxt3LhRnTp1UsuWLfXXX395rG8AAAAX47Asy/J0IwAAALxdjx49dOLECS1YsCDLuf/+97/auHGjfv/99yznvvzyS/Xt21dHjhyRZGpKPf300zpx4kT6NTt27ND111+vvXv3qkyZMunHmzZtqttvv10jRozI9/4AAADkVYCnGwAAAODrLMuSw+GQJMXExGjEiBH6/ffflZCQoNTUVJ09e1ZnzpxR4cKFL/j59evXy7IsValSJdPxpKQkhYeHu739AAAAuUFSCgAAwMP++OMPVaxYUbt371br1q3Vp08fvfbaaypRooR++eUX9erVSykpKdl+3ul0yt/fX+vWrZO/v3+mc0WKFHF38wEAAHKFpBQAAIAHLV26VJs2bdKAAQP022+/KTU1VWPHjpWfnyn9OWfOnEzXFypUSGlpaZmO1apVS2lpaTp06JDuvvtu29oOAACQFySlAAAAbJKUlKQDBw4oLS1NBw8eVFRUlN588021bdtW3bp106ZNm5SamqoJEyaoXbt2WrFihSZOnJjpHhUqVNDp06f1448/qkaNGgoNDVWVKlXUpUsXdevWTWPHjlWtWrV05MgRLV26VNWrV1fr1q091GMAAIDssfoeAACATaKiolS6dGlVqFBBLVu2VExMjN59910tXLhQ/v7+qlmzpsaNG6dRo0apWrVqmjVrlt58881M96hfv7769OmjBx98UFdffbVGjx4tSZo2bZq6deumZ555RjfccIPat2+v1atXq1y5cp7oKgAAwCWx+h4AAAAAAABsx0gpAAAAAAAA2I6kFAAAAAAAAGxHUgoAAAAAAAC2IykFAAAAAAAA25GUAgAAAAAAgO1ISgEAAAAAAMB2JKUAAAAAAABgO5JSAAAAAAAAsB1JKQAAAAAAANiOpBQAAAAAAABsR1IKAAAAAAAAtiMpBQAAAAAAANv9H/Zq8kGoFiTLAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "portfolio = dr_net_eps.portfolio\n",
    "portfolio.plot_cumulative_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe Ratio:  0.13675062229574486\n",
      "Mean Return:  0.00294184663780217\n",
      "Volatility:  0.02151249177820896\n",
      "Annualized Sharpe Ratio:  0.9861227612778343\n"
     ]
    }
   ],
   "source": [
    "# print the sharpe ratio\n",
    "print(\"Sharpe Ratio: \", portfolio.sharpe)\n",
    "print(\"Mean Return: \", portfolio.mean)\n",
    "print(\"Volatility: \", portfolio.vol)\n",
    "print(\"Annualized Sharpe Ratio: \", portfolio.sharpe * np.sqrt(52))\n",
    "analyze = analyze_returns(portfolio.returns, risk_free_rate=0, frequency=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Results:  {2013: 2.442172737407541, 2014: 1.055081805435834, 2015: 0.8832971787244454, 2016: 1.989062317441327, 2017: 3.185439763423227, 2018: 0.1513975336177479, 2019: 2.420584826243656, 2020: 0.21915947115773615, 2021: -0.01177712330745581}\n",
      "Mean Sharpe Ratio:  1.3704909455715621\n"
     ]
    }
   ],
   "source": [
    "print(\"Analysis Results: \", analyze[\"SharpeRatiosPerYear\"])\n",
    "print(\"Mean Sharpe Ratio: \", np.mean(list(analyze[\"SharpeRatiosPerYear\"].values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_21160\\4089635158.py:90: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  factor_5 = pdr.get_data_famafrench(\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_21160\\4089635158.py:99: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  mom_df = pdr.get_data_famafrench(\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_21160\\4089635158.py:106: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  st_df = pdr.get_data_famafrench(\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_21160\\4089635158.py:113: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  lt_df = pdr.get_data_famafrench(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6868, 8) (6868, 20)\n",
      "(6180, 8) (6180, 20) (791, 8) (791, 20)\n",
      "            Mkt-RF     SMB     HML     RMW     CMA  Mom     ST_Rev  LT_Rev\n",
      "Date                                                                      \n",
      "1997-05-16 -0.0113  0.0108  0.0037 -0.0044  0.0047 -0.0036  0.0113  0.0069\n",
      "1997-05-19  0.0027 -0.0004 -0.0028 -0.0001  0.0006  0.0024  0.0016  0.0000\n",
      "1997-05-20  0.0091 -0.0055 -0.0021  0.0030 -0.0079 -0.0001 -0.0050 -0.0078\n",
      "1997-05-21 -0.0013  0.0073 -0.0073 -0.0013 -0.0006 -0.0056  0.0022  0.0010\n",
      "1997-05-22 -0.0027  0.0064  0.0041 -0.0008  0.0008 -0.0011  0.0029  0.0005             Mkt-RF     SMB     HML     RMW     CMA  Mom     ST_Rev  LT_Rev\n",
      "Date                                                                      \n",
      "2024-08-23  0.0129  0.0190  0.0085 -0.0048  0.0068  0.0013  0.0078 -0.0011\n",
      "2024-08-26 -0.0034  0.0033  0.0017  0.0013 -0.0006 -0.0045  0.0020  0.0060\n",
      "2024-08-27  0.0005 -0.0090  0.0002  0.0027  0.0023  0.0053 -0.0080 -0.0016\n",
      "2024-08-28 -0.0067 -0.0022  0.0114  0.0055 -0.0016  0.0030  0.0029  0.0068\n",
      "2024-08-29  0.0008  0.0067  0.0028 -0.0015 -0.0122 -0.0078  0.0067  0.0021\n",
      "Ticker           DIS        ED         C       XOM       JNJ       MCD  \\\n",
      "Date                                                                     \n",
      "1997-05-19  0.029366  0.012931  0.009050  0.017204  0.016597  0.002403   \n",
      "1997-05-20  0.003003  0.000000  0.033632  0.000000 -0.006122 -0.016787   \n",
      "1997-05-21 -0.016467 -0.008510 -0.030369  0.006343 -0.024641 -0.007317   \n",
      "1997-05-22  0.000000 -0.017168 -0.006711 -0.016807  0.002105  0.000000   \n",
      "1997-05-23  0.013699  0.013101  0.018017  0.023504  0.006303  0.004914   \n",
      "\n",
      "Ticker           BAC       HAL      COST      AAPL        VZ       JPM  \\\n",
      "Date                                                                     \n",
      "1997-05-19 -0.016563 -0.009967  0.038910 -0.014492  0.000000 -0.010681   \n",
      "1997-05-20  0.012631  0.006712  0.029962  0.014705  0.000000  0.041836   \n",
      "1997-05-21 -0.027027  0.008334 -0.040000 -0.021738 -0.001815 -0.034974   \n",
      "1997-05-22  0.000000  0.001653 -0.020834 -0.014814 -0.014546  0.016108   \n",
      "1997-05-23  0.008547  0.000000 -0.013540  0.015037  0.016606  0.005283   \n",
      "\n",
      "Ticker          MSFT       WMT       PFE       LMT       NEM       CAT  \\\n",
      "Date                                                                     \n",
      "1997-05-19 -0.002708  0.020921 -0.003722  0.016643  0.016835 -0.005076   \n",
      "1997-05-20  0.034745  0.000000  0.004982  0.013643  0.003311 -0.010204   \n",
      "1997-05-21  0.010493 -0.016393 -0.006196  0.008075 -0.003300 -0.002577   \n",
      "1997-05-22  0.002077 -0.008333 -0.008728  0.005341  0.000000  0.007751   \n",
      "1997-05-23  0.018653  0.012605  0.011321 -0.010624  0.013246  0.000000   \n",
      "\n",
      "Ticker          AMZN         T  \n",
      "Date                            \n",
      "1997-05-19 -0.012040  0.002179  \n",
      "1997-05-20 -0.042685  0.006522  \n",
      "1997-05-21 -0.127392 -0.017279  \n",
      "1997-05-22 -0.021891 -0.017582  \n",
      "1997-05-23  0.074622  0.017897   Ticker           DIS        ED         C       XOM       JNJ       MCD  \\\n",
      "Date                                                                     \n",
      "2024-08-26  0.013472  0.002982 -0.005632  0.021406  0.002924 -0.002901   \n",
      "2024-08-27 -0.009588 -0.012983 -0.001780 -0.009511 -0.002571  0.003326   \n",
      "2024-08-28 -0.015512  0.008033 -0.001621 -0.009857  0.005953 -0.008942   \n",
      "2024-08-29  0.003576  0.004184  0.004872  0.013817  0.001891  0.002822   \n",
      "2024-08-30  0.006347  0.007440  0.012282 -0.001608  0.009925  0.002779   \n",
      "\n",
      "Ticker           BAC       HAL      COST      AAPL        VZ       JPM  \\\n",
      "Date                                                                     \n",
      "2024-08-26  0.003772  0.003463  0.015127  0.001499  0.006795  0.003939   \n",
      "2024-08-27 -0.006263 -0.007844  0.018364  0.003742 -0.000964  0.004608   \n",
      "2024-08-28  0.007058 -0.019608 -0.022940 -0.006753  0.000965  0.005041   \n",
      "2024-08-29  0.005507  0.012258 -0.001599  0.014570 -0.005785  0.004158   \n",
      "2024-08-30  0.014439 -0.009242  0.006485 -0.003438  0.012849  0.011656   \n",
      "\n",
      "Ticker          MSFT       WMT       PFE       LMT       NEM       CAT  \\\n",
      "Date                                                                     \n",
      "2024-08-26 -0.007918  0.004359  0.000692  0.005567  0.004227  0.007893   \n",
      "2024-08-27  0.000846  0.001315 -0.003458  0.004139  0.008419 -0.000114   \n",
      "2024-08-28 -0.007829 -0.000657 -0.002429  0.006370 -0.016509 -0.008316   \n",
      "2024-08-29  0.006137  0.004469 -0.001044  0.005089  0.026047  0.009879   \n",
      "2024-08-30  0.009731  0.010599  0.010098  0.002205  0.003949  0.012683   \n",
      "\n",
      "Ticker          AMZN         T  \n",
      "Date                            \n",
      "2024-08-26 -0.008699  0.001521  \n",
      "2024-08-27 -0.013561 -0.005567  \n",
      "2024-08-28 -0.013401  0.008651  \n",
      "2024-08-29  0.007728 -0.003027  \n",
      "2024-08-30  0.037067  0.007085  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from pandas_datareader import data as pdr\n",
    "\n",
    "# Assuming TrainTest is defined elsewhere in your codebase\n",
    "# from your_module import TrainTest\n",
    "\n",
    "\n",
    "def AV_yFinance(\n",
    "    start: str,\n",
    "    end: str,\n",
    "    split: List[float],\n",
    "    freq: str = \"weekly\",\n",
    "    n_obs: int = 104,\n",
    "    n_y: Optional[int] = None,\n",
    "    use_cache: bool = False,\n",
    "    save_results: bool = False,\n",
    ") -> Tuple[TrainTest, TrainTest]:\n",
    "\n",
    "    if use_cache:\n",
    "        X = pd.read_pickle(f\"./cache/factor_{freq}.pkl\")\n",
    "        Y = pd.read_pickle(f\"./cache/asset_{freq}.pkl\")\n",
    "    else:\n",
    "        # Define the list of tickers\n",
    "        tick_list = [\n",
    "            \"AAPL\",\n",
    "            \"MSFT\",\n",
    "            \"AMZN\",\n",
    "            \"C\",\n",
    "            \"JPM\",\n",
    "            \"BAC\",\n",
    "            \"XOM\",\n",
    "            \"HAL\",\n",
    "            \"MCD\",\n",
    "            \"WMT\",\n",
    "            \"COST\",\n",
    "            \"CAT\",\n",
    "            \"LMT\",\n",
    "            \"JNJ\",\n",
    "            \"PFE\",\n",
    "            \"DIS\",\n",
    "            \"VZ\",\n",
    "            \"T\",\n",
    "            \"ED\",\n",
    "            \"NEM\",\n",
    "        ]\n",
    "\n",
    "        if n_y is not None:\n",
    "            tick_list = tick_list[:n_y]\n",
    "\n",
    "        # Download asset data using yfinance\n",
    "        data = yf.download(\n",
    "            tick_list,\n",
    "            start=start,\n",
    "            end=end,\n",
    "            progress=False,\n",
    "            group_by=\"ticker\",\n",
    "            auto_adjust=True,  # Adjusted close prices\n",
    "            threads=True,  # Enable multi-threading for faster downloads\n",
    "        )\n",
    "\n",
    "        if data.empty:\n",
    "            raise ValueError(\n",
    "                \"No data downloaded. Please check the ticker symbols and date range.\"\n",
    "            )\n",
    "\n",
    "        # Extract Adjusted Close prices\n",
    "        if len(tick_list) == 1:\n",
    "            # For single ticker, data['Close'] is a Series, convert to DataFrame\n",
    "            adj_close = data[\"Close\"].to_frame()\n",
    "            adj_close.columns = tick_list\n",
    "        else:\n",
    "            # For multiple tickers, use xs to extract 'Close' for all tickers\n",
    "            try:\n",
    "                adj_close = data.xs(\"Close\", level=1, axis=1)\n",
    "            except KeyError:\n",
    "                raise KeyError(\"Close prices not found in the downloaded data.\")\n",
    "\n",
    "        # Compute daily returns as percentage change\n",
    "        Y = adj_close.pct_change().dropna()\n",
    "\n",
    "        # Download factor data from Kenneth French's data library\n",
    "        dl_freq = \"_daily\"\n",
    "\n",
    "        try:\n",
    "            # 5-Factor Model\n",
    "            factor_5 = pdr.get_data_famafrench(\n",
    "                \"F-F_Research_Data_5_Factors_2x3\" + dl_freq,\n",
    "                start=start,\n",
    "                end=end,\n",
    "            )[0]\n",
    "            rf_df = factor_5[\"RF\"]\n",
    "            factor_5 = factor_5.drop([\"RF\"], axis=1)\n",
    "\n",
    "            # Momentum Factor\n",
    "            mom_df = pdr.get_data_famafrench(\n",
    "                \"F-F_Momentum_Factor\" + dl_freq,\n",
    "                start=start,\n",
    "                end=end,\n",
    "            )[0]\n",
    "\n",
    "            # Short-Term Reversal Factor\n",
    "            st_df = pdr.get_data_famafrench(\n",
    "                \"F-F_ST_Reversal_Factor\" + dl_freq,\n",
    "                start=start,\n",
    "                end=end,\n",
    "            )[0]\n",
    "\n",
    "            # Long-Term Reversal Factor\n",
    "            lt_df = pdr.get_data_famafrench(\n",
    "                \"F-F_LT_Reversal_Factor\" + dl_freq,\n",
    "                start=start,\n",
    "                end=end,\n",
    "            )[0]\n",
    "\n",
    "            # Concatenate all factors and convert to decimal\n",
    "            X = pd.concat([factor_5, mom_df, st_df, lt_df], axis=1) / 100\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to download factor data: {e}\")\n",
    "\n",
    "        # Align factor data (X) with asset returns (Y) based on dates\n",
    "\n",
    "        # Remove timezone from Y.index if present\n",
    "        if Y.index.tz is not None:\n",
    "            Y.index = Y.index.tz_convert(None)\n",
    "\n",
    "        # Ensure X.index is also timezone-naive\n",
    "        if X.index.tz is not None:\n",
    "            X.index = X.index.tz_convert(None)\n",
    "\n",
    "        # Now, perform the alignment\n",
    "        try:\n",
    "            X = X.loc[Y.index]\n",
    "        except KeyError as e:\n",
    "            missing_dates = Y.index.difference(X.index)\n",
    "            if not missing_dates.empty:\n",
    "                print(f\"Missing dates in factor data: {missing_dates}\")\n",
    "                # Optionally, you can drop missing dates or handle them differently\n",
    "                Y = Y.loc[Y.index.intersection(X.index)]\n",
    "                X = X.loc[X.index.intersection(Y.index)]\n",
    "            else:\n",
    "                raise e  # Re-raise if no missing dates found\n",
    "\n",
    "        # Resample data if frequency is not daily\n",
    "        freq_lower = freq.lower()\n",
    "        if freq_lower in [\"weekly\", \"wk\", \"1wk\"]:\n",
    "            Y = Y.resample(\"W-FRI\").apply(lambda x: (x + 1).prod() - 1)\n",
    "            X = X.resample(\"W-FRI\").apply(lambda x: (x + 1).prod() - 1)\n",
    "        elif freq_lower in [\"monthly\", \"1mo\"]:\n",
    "            Y = Y.resample(\"M\").apply(lambda x: (x + 1).prod() - 1)\n",
    "            X = X.resample(\"M\").apply(lambda x: (x + 1).prod() - 1)\n",
    "        # Add more resampling frequencies if needed\n",
    "\n",
    "        # Handle missing values by forward and backward filling using ffill() and bfill()\n",
    "        Y = Y.ffill().bfill()\n",
    "        X = X.ffill().bfill()\n",
    "\n",
    "        # Convert the index to 'YYYY-MM-DD' format\n",
    "        X.index = X.index.strftime(\"%Y-%m-%d\")\n",
    "        X.index = pd.DatetimeIndex(X.index)\n",
    "        Y.index = Y.index.strftime(\"%Y-%m-%d\")\n",
    "        X.index = pd.DatetimeIndex(Y.index)\n",
    "\n",
    "        # Optionally save the results to cache\n",
    "        if save_results:\n",
    "            os.makedirs(\"./cache\", exist_ok=True)\n",
    "            X.to_pickle(f\"./cache/factor_{freq}.pkl\")\n",
    "            Y.to_pickle(f\"./cache/asset_{freq}.pkl\")\n",
    "    print(X.shape, Y.shape)\n",
    "    # Partition dataset into training and testing sets. Lag the data by one observation\n",
    "    # Using the provided TrainTest class\n",
    "    X_train_test = TrainTest(X[:-1], n_obs, split)\n",
    "    Y_train_test = TrainTest(Y[1:], n_obs, split)\n",
    "\n",
    "    return X_train_test, Y_train_test\n",
    "\n",
    "\n",
    "start_paddling = \"1997-04-01\"\n",
    "end_paddling = \"2024-09-01\"  # Data frequency and start/end dates\n",
    "daily_frequency = \"daily\"\n",
    "xf_train_test, yf_train_test = AV_yFinance(\n",
    "    start=start_paddling,\n",
    "    end=end_paddling,\n",
    "    split=[0.9, 0.1],\n",
    "    freq=daily_frequency,\n",
    "    n_obs=104,\n",
    "    n_y=20,\n",
    "    use_cache=False,\n",
    "    save_results=True,\n",
    ")\n",
    "print(\n",
    "    xf_train_test.train().shape,\n",
    "    yf_train_test.train().shape,\n",
    "    xf_train_test.test().shape,\n",
    "    yf_train_test.test().shape,\n",
    ")\n",
    "print(xf_train_test.train().head(), xf_train_test.test().tail())\n",
    "print(yf_train_test.train().head(), yf_train_test.test().tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
