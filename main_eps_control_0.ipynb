{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")  # close all previous plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "cache_path: str = \"./cache/exp/\"\n",
    "data_frequency = \"weekly\"\n",
    "start = \"2000-01-01\"\n",
    "end = \"2021-09-30\"  # Data frequency and start/end dates\n",
    "split_ratio_list = [0.6, 0.4]  # Train, validation and test split percentage\n",
    "number_of_observe_per_window: int = 104\n",
    "number_of_asset: int = 20  # Number of assets n_y = 20\n",
    "AV_key: str = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTest:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        number_of_observation_per_window: int,\n",
    "        split_ratio_list: list[float],\n",
    "    ) -> None:\n",
    "        self.data: pd.DataFrame = data\n",
    "        self.number_of_observation_per_window: int = number_of_observation_per_window\n",
    "        self.split_ratio: list[float] = split_ratio_list\n",
    "\n",
    "        num_total_observations: int = self.data.shape[\n",
    "            0\n",
    "        ]  # Calculate the total number of observations in the DataFrame\n",
    "        num_observations_cumulative_split: list[float] = (\n",
    "            num_total_observations * np.cumsum(split_ratio_list)\n",
    "        )  # np.cumsum([0.7, 0.2, 0.1]) = [0.7, 0.9, 1.0]\n",
    "        self.cumulative_number_window_observation: list[int] = [\n",
    "            round(num_observation_cumulative_split)\n",
    "            for num_observation_cumulative_split in num_observations_cumulative_split\n",
    "        ]\n",
    "\n",
    "    def split_update(self, split_ratio_list: list[float]) -> None:\n",
    "        self.split_ratio: list[float] = split_ratio_list\n",
    "        num_observations_total: int = self.data.shape[0]\n",
    "        num_observations_cumulative_split: list[float] = (\n",
    "            num_observations_total * np.cumsum(split_ratio_list)\n",
    "        )  # np.cumsum([0.7, 0.2, 0.1]) = [0.7, 0.9, 1.0]\n",
    "        self.cumulative_number_window_observation = [\n",
    "            round(i) for i in num_observations_cumulative_split\n",
    "        ]\n",
    "\n",
    "    def train(self) -> pd.DataFrame:\n",
    "        return self.data[\n",
    "            : self.cumulative_number_window_observation[0]\n",
    "        ]  # Return the training subset of observations\n",
    "\n",
    "    def test(self):\n",
    "        if (\n",
    "            self.cumulative_number_window_observation[0]\n",
    "            - self.number_of_observation_per_window\n",
    "            < 0\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"The number of observations per window exceeds the number of observations of train data in the dataset.\"\n",
    "            )\n",
    "        return self.data[\n",
    "            self.cumulative_number_window_observation[0]\n",
    "            - self.number_of_observation_per_window : self.cumulative_number_window_observation[\n",
    "                1\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "    def shape(self):\n",
    "        return self.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_original_data: pd.DataFrame = pd.read_pickle(\n",
    "    \"./cache/factor_\" + data_frequency + \".pkl\"\n",
    ")\n",
    "Y_original_data: pd.DataFrame = pd.read_pickle(\n",
    "    \"./cache/asset_\" + data_frequency + \".pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mkt-RF</th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>Mom</th>\n",
       "      <th>ST_Rev</th>\n",
       "      <th>LT_Rev</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>-0.024889</td>\n",
       "      <td>-0.003948</td>\n",
       "      <td>0.005921</td>\n",
       "      <td>-0.008655</td>\n",
       "      <td>0.021933</td>\n",
       "      <td>-0.033839</td>\n",
       "      <td>0.034927</td>\n",
       "      <td>0.002774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-14</th>\n",
       "      <td>0.020696</td>\n",
       "      <td>0.013858</td>\n",
       "      <td>-0.015028</td>\n",
       "      <td>-0.028196</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.015969</td>\n",
       "      <td>-0.001553</td>\n",
       "      <td>0.008910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-21</th>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.060555</td>\n",
       "      <td>-0.025968</td>\n",
       "      <td>-0.048690</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>0.053417</td>\n",
       "      <td>-0.043407</td>\n",
       "      <td>0.020229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-28</th>\n",
       "      <td>-0.057084</td>\n",
       "      <td>0.009003</td>\n",
       "      <td>0.016956</td>\n",
       "      <td>0.013910</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>-0.030094</td>\n",
       "      <td>0.031843</td>\n",
       "      <td>-0.012653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-04</th>\n",
       "      <td>0.044559</td>\n",
       "      <td>-0.001065</td>\n",
       "      <td>-0.026655</td>\n",
       "      <td>-0.019944</td>\n",
       "      <td>-0.014198</td>\n",
       "      <td>0.037680</td>\n",
       "      <td>-0.001666</td>\n",
       "      <td>0.015425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Mkt-RF       SMB       HML       RMW       CMA    Mom     \\\n",
       "Date                                                                     \n",
       "2000-01-07 -0.024889 -0.003948  0.005921 -0.008655  0.021933 -0.033839   \n",
       "2000-01-14  0.020696  0.013858 -0.015028 -0.028196  0.000918  0.015969   \n",
       "2000-01-21  0.000378  0.060555 -0.025968 -0.048690  0.001365  0.053417   \n",
       "2000-01-28 -0.057084  0.009003  0.016956  0.013910  0.016216 -0.030094   \n",
       "2000-02-04  0.044559 -0.001065 -0.026655 -0.019944 -0.014198  0.037680   \n",
       "\n",
       "              ST_Rev    LT_Rev  \n",
       "Date                            \n",
       "2000-01-07  0.034927  0.002774  \n",
       "2000-01-14 -0.001553  0.008910  \n",
       "2000-01-21 -0.043407  0.020229  \n",
       "2000-01-28  0.031843 -0.012653  \n",
       "2000-02-04 -0.001666  0.015425  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_original_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>C</th>\n",
       "      <th>JPM</th>\n",
       "      <th>BAC</th>\n",
       "      <th>XOM</th>\n",
       "      <th>HAL</th>\n",
       "      <th>MCD</th>\n",
       "      <th>WMT</th>\n",
       "      <th>COST</th>\n",
       "      <th>CAT</th>\n",
       "      <th>LMT</th>\n",
       "      <th>JNJ</th>\n",
       "      <th>PFE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>VZ</th>\n",
       "      <th>T</th>\n",
       "      <th>ED</th>\n",
       "      <th>NEM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>-0.032195</td>\n",
       "      <td>-0.045482</td>\n",
       "      <td>-0.086300</td>\n",
       "      <td>-0.030347</td>\n",
       "      <td>-0.058169</td>\n",
       "      <td>-0.029886</td>\n",
       "      <td>0.054369</td>\n",
       "      <td>0.010932</td>\n",
       "      <td>-0.010667</td>\n",
       "      <td>-0.009113</td>\n",
       "      <td>0.019836</td>\n",
       "      <td>0.132809</td>\n",
       "      <td>-0.020110</td>\n",
       "      <td>0.034853</td>\n",
       "      <td>0.063502</td>\n",
       "      <td>0.064274</td>\n",
       "      <td>-0.038464</td>\n",
       "      <td>-0.089726</td>\n",
       "      <td>0.045217</td>\n",
       "      <td>-0.124898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-14</th>\n",
       "      <td>0.009447</td>\n",
       "      <td>0.007268</td>\n",
       "      <td>-0.076337</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.015533</td>\n",
       "      <td>0.037174</td>\n",
       "      <td>-0.014010</td>\n",
       "      <td>-0.052347</td>\n",
       "      <td>0.068455</td>\n",
       "      <td>-0.058394</td>\n",
       "      <td>0.054374</td>\n",
       "      <td>-0.025699</td>\n",
       "      <td>-0.043843</td>\n",
       "      <td>-0.029119</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.078060</td>\n",
       "      <td>-0.042510</td>\n",
       "      <td>-0.048266</td>\n",
       "      <td>-0.065724</td>\n",
       "      <td>-0.029384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-21</th>\n",
       "      <td>0.108224</td>\n",
       "      <td>-0.075724</td>\n",
       "      <td>-0.034086</td>\n",
       "      <td>-0.026897</td>\n",
       "      <td>-0.012723</td>\n",
       "      <td>-0.095248</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.099066</td>\n",
       "      <td>-0.036376</td>\n",
       "      <td>-0.031938</td>\n",
       "      <td>-0.011415</td>\n",
       "      <td>-0.081857</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>-0.040666</td>\n",
       "      <td>-0.052432</td>\n",
       "      <td>-0.024136</td>\n",
       "      <td>0.066596</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>-0.003859</td>\n",
       "      <td>0.018260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-28</th>\n",
       "      <td>-0.087054</td>\n",
       "      <td>-0.053012</td>\n",
       "      <td>-0.005962</td>\n",
       "      <td>-0.005493</td>\n",
       "      <td>0.051412</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>-0.072000</td>\n",
       "      <td>-0.155026</td>\n",
       "      <td>-0.104968</td>\n",
       "      <td>-0.117072</td>\n",
       "      <td>-0.051546</td>\n",
       "      <td>-0.081891</td>\n",
       "      <td>-0.100952</td>\n",
       "      <td>-0.059858</td>\n",
       "      <td>0.003708</td>\n",
       "      <td>0.122137</td>\n",
       "      <td>-0.040460</td>\n",
       "      <td>-0.087209</td>\n",
       "      <td>-0.029797</td>\n",
       "      <td>-0.053327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-04</th>\n",
       "      <td>0.062783</td>\n",
       "      <td>0.084580</td>\n",
       "      <td>0.273464</td>\n",
       "      <td>-0.021814</td>\n",
       "      <td>0.065980</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>0.025355</td>\n",
       "      <td>0.027925</td>\n",
       "      <td>-0.044354</td>\n",
       "      <td>0.021404</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>-0.027356</td>\n",
       "      <td>0.013242</td>\n",
       "      <td>-0.028047</td>\n",
       "      <td>0.015914</td>\n",
       "      <td>0.037551</td>\n",
       "      <td>0.016137</td>\n",
       "      <td>0.070064</td>\n",
       "      <td>-0.009521</td>\n",
       "      <td>0.196411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                AAPL      MSFT      AMZN         C       JPM       BAC  \\\n",
       "date                                                                     \n",
       "2000-01-07 -0.032195 -0.045482 -0.086300 -0.030347 -0.058169 -0.029886   \n",
       "2000-01-14  0.009447  0.007268 -0.076337  0.074074  0.015533  0.037174   \n",
       "2000-01-21  0.108224 -0.075724 -0.034086 -0.026897 -0.012723 -0.095248   \n",
       "2000-01-28 -0.087054 -0.053012 -0.005962 -0.005493  0.051412  0.001313   \n",
       "2000-02-04  0.062783  0.084580  0.273464 -0.021814  0.065980  0.002842   \n",
       "\n",
       "                 XOM       HAL       MCD       WMT      COST       CAT  \\\n",
       "date                                                                     \n",
       "2000-01-07  0.054369  0.010932 -0.010667 -0.009113  0.019836  0.132809   \n",
       "2000-01-14 -0.014010 -0.052347  0.068455 -0.058394  0.054374 -0.025699   \n",
       "2000-01-21  0.014925  0.099066 -0.036376 -0.031938 -0.011415 -0.081857   \n",
       "2000-01-28 -0.072000 -0.155026 -0.104968 -0.117072 -0.051546 -0.081891   \n",
       "2000-02-04  0.025355  0.027925 -0.044354  0.021404  0.152174 -0.027356   \n",
       "\n",
       "                 LMT       JNJ       PFE       DIS        VZ         T  \\\n",
       "date                                                                     \n",
       "2000-01-07 -0.020110  0.034853  0.063502  0.064274 -0.038464 -0.089726   \n",
       "2000-01-14 -0.043843 -0.029119  0.072464  0.078060 -0.042510 -0.048266   \n",
       "2000-01-21  0.024390 -0.040666 -0.052432 -0.024136  0.066596  0.023810   \n",
       "2000-01-28 -0.100952 -0.059858  0.003708  0.122137 -0.040460 -0.087209   \n",
       "2000-02-04  0.013242 -0.028047  0.015914  0.037551  0.016137  0.070064   \n",
       "\n",
       "                  ED       NEM  \n",
       "date                            \n",
       "2000-01-07  0.045217 -0.124898  \n",
       "2000-01-14 -0.065724 -0.029384  \n",
       "2000-01-21 -0.003859  0.018260  \n",
       "2000-01-28 -0.029797 -0.053327  \n",
       "2000-02-04 -0.009521  0.196411  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_original_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def AV(\n",
    "    start: str,\n",
    "    end: str,\n",
    "    split_ratio: list,\n",
    "    data_frequency: str = \"weekly\",\n",
    "    num_observations_per_window: int = 104,\n",
    "    num_assets=None,\n",
    "    use_cache: bool = False,\n",
    "    save_results: bool = False,\n",
    "    AV_key: str = None,\n",
    "):\n",
    "    if use_cache:\n",
    "        X: pd.DataFrame = pd.read_pickle(\"./cache/factor_\" + data_frequency + \".pkl\")\n",
    "        Y: pd.DataFrame = pd.read_pickle(\"./cache/asset_\" + data_frequency + \".pkl\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"We cannot download data from AlphaVantage without an API key.\"\n",
    "        )\n",
    "\n",
    "    # Partition dataset into training and testing sets. Lag the data by one observation, since we are predicting future returns, so we don't need the last observation that doesn't have a future return.\n",
    "    # we don't need the first Y observation that doesn't have a corresponding X observation.\n",
    "    return TrainTest(X[:-1], num_observations_per_window, split_ratio), TrainTest(\n",
    "        Y[1:], num_observations_per_window, split_ratio\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1134, 8)\n",
      "(680, 8)\n",
      "(680, 20)\n"
     ]
    }
   ],
   "source": [
    "X_data, Y_data = AV(\n",
    "    start,\n",
    "    end,\n",
    "    split_ratio_list,\n",
    "    data_frequency=data_frequency,\n",
    "    num_observations_per_window=number_of_observe_per_window,\n",
    "    num_assets=number_of_asset,\n",
    "    use_cache=True,\n",
    "    save_results=False,\n",
    "    AV_key=AV_key,\n",
    ")\n",
    "print(X_data.shape())\n",
    "print(X_data.train().shape)\n",
    "print(Y_data.train().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  8\n",
      "Number of assets:  20\n"
     ]
    }
   ],
   "source": [
    "# Number of features and assets\n",
    "n_X: int = X_data.train().shape[1]\n",
    "n_Y: int = Y_data.train().shape[1]\n",
    "print(\"Number of features: \", n_X)\n",
    "print(\"Number of assets: \", n_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low p-values (< 0.05) suggest that the factor significantly affects the stock returns.\n",
    "High p-values (> 0.05) suggest that the factor's effect on the stock returns is not statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Mkt-RF   SMB   HML   RMW   CMA  Mom     ST_Rev  LT_Rev\n",
      "AAPL    0.34  0.37  0.53  0.92  0.70    0.41    0.41    0.53\n",
      "MSFT    0.64  0.85  0.63  0.80  0.20    0.21    0.97    0.22\n",
      "AMZN    0.31  0.02  0.64  0.28  0.63    0.18    0.45    0.34\n",
      "C       0.25  0.69  0.02  0.04  0.21    0.07    0.02    0.24\n",
      "JPM     0.33  0.64  0.00  0.50  0.48    0.18    0.18    0.01\n",
      "BAC     0.16  0.91  0.01  0.16  0.56    0.15    0.06    0.19\n",
      "XOM     0.03  0.77  0.34  0.15  0.10    0.11    0.51    0.04\n",
      "HAL     0.92  0.48  0.47  0.14  0.14    0.42    0.92    0.05\n",
      "MCD     0.48  0.05  0.57  0.02  0.54    0.27    0.81    0.03\n",
      "WMT     0.00  0.01  0.25  0.00  0.40    0.62    0.04    0.03\n",
      "COST    0.00  0.22  0.85  0.01  0.38    0.66    0.39    0.05\n",
      "CAT     0.92  0.27  0.59  0.23  0.07    0.40    0.67    0.51\n",
      "LMT     0.27  0.74  0.04  0.00  0.61    0.32    0.38    0.60\n",
      "JNJ     0.00  0.91  0.39  0.09  0.84    0.82    0.19    0.06\n",
      "PFE     0.06  0.38  0.95  0.91  0.56    0.75    0.50    0.12\n",
      "DIS     0.35  0.67  0.82  0.01  0.39    0.61    0.19    0.04\n",
      "VZ      0.72  0.30  0.01  0.04  0.15    0.09    0.15    0.00\n",
      "T       0.62  0.80  0.00  0.95  0.01    0.14    0.86    0.00\n",
      "ED      0.30  0.94  0.96  0.00  0.23    0.89    0.73    0.16\n",
      "NEM     0.12  0.07  0.05  0.01  0.20    0.05    0.76    0.50\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def statanalysis(X: pd.DataFrame, Y: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Initialize an empty DataFrame to store p-values\n",
    "    # Rows correspond to assets (Y.columns) and columns correspond to features (X.columns)\n",
    "    stats = pd.DataFrame(\n",
    "        columns=X.columns, index=Y.columns\n",
    "    )  # Create an empty DataFrame to store the p-values\n",
    "    for ticker in Y.columns:\n",
    "        for feature in X.columns:\n",
    "            stats.loc[ticker, feature] = (\n",
    "                sm.OLS(Y[ticker].values, sm.add_constant(X[feature]).values)\n",
    "                .fit()\n",
    "                .pvalues[1]  # Get the p-value of the feature\n",
    "            )\n",
    "\n",
    "    return stats.astype(float).round(2)\n",
    "\n",
    "\n",
    "statistical_analysis: pd.DataFrame = statanalysis(X_data.train(), Y_data.train())\n",
    "print(statistical_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SlidingWindow(Dataset):\n",
    "    \"\"\"Sliding window dataset constructor for time series data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        XData: pd.DataFrame,\n",
    "        YData: pd.DataFrame,\n",
    "        num_observations: int,\n",
    "        performance_window: int,\n",
    "    ) -> None:\n",
    "        # Convert the feature DataFrame to a PyTorch tensor with double precision\n",
    "        self.X: torch.Tensor = torch.tensor(XData.values, dtype=torch.float64)\n",
    "        # Convert the asset return DataFrame to a PyTorch tensor with double precision\n",
    "        self.Y: torch.Tensor = torch.tensor(YData.values, dtype=torch.float64)\n",
    "        # Store the number of observations (scenarios) in the sliding window\n",
    "        self.num_observations: int = num_observations\n",
    "        # Store the number of scenarios in the performance window\n",
    "        self.perf_period: int = performance_window\n",
    "\n",
    "    def __getitem__(\n",
    "        self, index: int\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # Extract the feature window starting at 'index' and spanning 'n_obs + 1' time steps\n",
    "        x: torch.Tensor = self.X[index : index + self.num_observations + 1]\n",
    "        # Extract the realizations window starting at 'index' and spanning 'n_obs' time steps\n",
    "        y: torch.Tensor = self.Y[index : index + self.num_observations]\n",
    "        # Extract the performance window starting after the observations window and spanning 'perf_period + 1' time steps\n",
    "        y_future_performance: torch.Tensor = self.Y[\n",
    "            index\n",
    "            + self.num_observations : index\n",
    "            + self.num_observations\n",
    "            + self.perf_period\n",
    "            + 1\n",
    "        ]\n",
    "        # Return the extracted windows as a tuple\n",
    "        return x, y, y_future_performance\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # Calculate the effective length by subtracting the window sizes from the total data length\n",
    "        total_length: int = len(self.X) - self.num_observations - self.perf_period\n",
    "        # Return the calculated length\n",
    "        return total_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackTest:\n",
    "    \"\"\"Backtest object to store out-of-sample results.\"\"\"\n",
    "\n",
    "    def __init__(self, len_test: int, n_y: int, dates: pd.DatetimeIndex) -> None:\n",
    "        # Initialize the weights array with zeros; dimensions are (len_test, n_y)\n",
    "        self.weights: np.ndarray = np.zeros((len_test, n_y))\n",
    "        # Initialize the returns array with zeros; length is len_test\n",
    "        self.rets: np.ndarray = np.zeros(len_test)\n",
    "        # Store the dates corresponding to the out-of-sample period\n",
    "        self.dates: pd.DatetimeIndex = pd.DatetimeIndex(dates[-len_test:])\n",
    "\n",
    "    def stats(self) -> None:\n",
    "        # Calculate the cumulative product of returns plus one to get the total return index\n",
    "        tri: np.ndarray = np.cumprod(self.rets + 1)\n",
    "        # Calculate the geometric mean return over the out-of-sample period\n",
    "        self.mean: float = (tri[-1]) ** (1 / len(tri)) - 1\n",
    "        # Calculate the volatility (standard deviation) of the returns\n",
    "        self.vol: float = np.std(self.rets)\n",
    "        # Calculate the pseudo-Sharpe ratio, handling division by zero\n",
    "        self.sharpe: float = self.mean / self.vol if self.vol != 0 else np.nan\n",
    "        # Create a DataFrame with dates, realized returns, and total return index\n",
    "        self.returns = pd.DataFrame({\"Date\": self.dates, \"rets\": self.rets, \"tri\": tri})\n",
    "        # Set the 'Date' column as the index of the DataFrame\n",
    "        self.returns = self.returns.set_index(\"Date\")\n",
    "\n",
    "    def plot_cumulative_returns(\n",
    "        self,\n",
    "        figsize: tuple = (12, 6),\n",
    "        resample_freq: str | None = None,\n",
    "        title: str | None = None,\n",
    "    ) -> None:\n",
    "        if self.returns is None:\n",
    "            raise ValueError(\n",
    "                \"Returns DataFrame not initialized. Run 'compute_stats' after populating 'rets' and 'weights'.\"\n",
    "            )\n",
    "\n",
    "        data_to_plot = self.returns[\"tri\"]\n",
    "        label = \"Cumulative Return\"\n",
    "\n",
    "        if resample_freq:\n",
    "            # Resample the cumulative return\n",
    "            # For cumulative returns, resampling can be tricky. We'll resample the returns first,\n",
    "            # then compute cumulative returns on the resampled data.\n",
    "            resampled_rets = (\n",
    "                self.returns[\"rets\"]\n",
    "                .resample(resample_freq)\n",
    "                .apply(lambda x: (x + 1).prod() - 1)\n",
    "            )\n",
    "            data_to_plot = (resampled_rets + 1).cumprod()\n",
    "            label = f\"Cumulative Return ({resample_freq})\"\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.plot(data_to_plot, label=label, color=\"blue\")\n",
    "        plt.title(title if title else \"Cumulative Return Over Time\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Cumulative Return\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()  # Adjust layout to prevent clipping of labels\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "\n",
    "\n",
    "# Define the Sharpe loss function\n",
    "def sharpe_loss(z_star: torch.Tensor, y_perf: torch.Tensor) -> torch.Tensor:\n",
    "    loss = -torch.mean(y_perf @ z_star) / torch.std(y_perf @ z_star)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Define the portfolio variance risk function\n",
    "def p_var(z: cp.Variable, c: cp.Variable, x: cp.Expression) -> cp.Expression:\n",
    "    return cp.square(x @ z - c)\n",
    "\n",
    "\n",
    "# Define the Hellinger distance-based DRO optimization layer\n",
    "def hellinger(num_assets: int, num_observations: int, prisk) -> CvxpyLayer:\n",
    "    # Define decision variables\n",
    "    z = cp.Variable((num_assets, 1), nonneg=True)  # Portfolio weights\n",
    "    c_aux = cp.Variable()  # Centering parameter\n",
    "    lambda_aux = cp.Variable(nonneg=True)\n",
    "    xi_aux = cp.Variable()\n",
    "    beta_aux = cp.Variable(num_observations, nonneg=True)\n",
    "    tau_aux = cp.Variable(num_observations, nonneg=True)\n",
    "    mu_aux = cp.Variable()\n",
    "\n",
    "    # Define parameters\n",
    "    ep = cp.Parameter((num_observations, num_assets))  # Residuals matrix\n",
    "    y_hat = cp.Parameter(num_assets)  # Predicted returns\n",
    "    gamma = cp.Parameter(nonneg=True)  # Risk-return trade-off parameter\n",
    "    delta = cp.Parameter(nonneg=True)  # Ambiguity size parameter\n",
    "\n",
    "    # Define constraints\n",
    "    constraints = [\n",
    "        cp.sum(z) == 1,  # Total budget constraint\n",
    "        mu_aux == y_hat @ z,  # Expected return constraint\n",
    "    ]\n",
    "    for i in range(num_observations):\n",
    "        # Constraints based on the risk function\n",
    "        constraints += [xi_aux + lambda_aux >= prisk(z, c_aux, ep[i, :]) + tau_aux[i]]\n",
    "        constraints += [\n",
    "            beta_aux[i] >= cp.quad_over_lin(lambda_aux, tau_aux[i])\n",
    "        ]  # Constraint on the ambiguity set\n",
    "\n",
    "    # Define the objective function\n",
    "    objective = cp.Minimize(\n",
    "        xi_aux\n",
    "        + (delta - 1) * lambda_aux\n",
    "        + (1 / num_observations) * cp.sum(beta_aux)\n",
    "        - gamma * mu_aux\n",
    "    )\n",
    "\n",
    "    # Define the problem\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    # Create a CVXPY layer\n",
    "    cvxpylayer = CvxpyLayer(\n",
    "        problem, parameters=[ep, y_hat, gamma, delta], variables=[z]\n",
    "    )\n",
    "\n",
    "    return cvxpylayer\n",
    "\n",
    "\n",
    "# Define mappings for performance loss functions, risk functions, and optimization layers\n",
    "perf_loss_functions = {\n",
    "    \"sharpe_loss\": sharpe_loss,\n",
    "    # Add other performance loss functions here if needed\n",
    "}\n",
    "\n",
    "risk_functions = {\n",
    "    \"p_var\": p_var,\n",
    "    # Add other risk functions here if needed\n",
    "}\n",
    "\n",
    "opt_layer_functions = {\n",
    "    \"hellinger\": hellinger,\n",
    "    # Add other optimization layer functions here if needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def compute_annualized_sharpe_ratio(\n",
    "    returns: pd.Series | np.ndarray,\n",
    "    risk_free_rate: float = 0.02,\n",
    "    periods_per_year: int = 52,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute the annualized Sharpe Ratio.\n",
    "\n",
    "    Args:\n",
    "        returns (pd.Series | np.ndarray): Periodic returns of the portfolio.\n",
    "        risk_free_rate (float, optional): Annual risk-free rate (e.g., 0.02 for 2%). Defaults to 0.02.\n",
    "        periods_per_year (int, optional): Number of return periods in a year (e.g., 52 for weekly). Defaults to 52.\n",
    "\n",
    "    Returns:\n",
    "        float: Annualized Sharpe Ratio.\n",
    "    \"\"\"\n",
    "    if isinstance(returns, pd.Series):\n",
    "        returns = returns.dropna().values\n",
    "    else:\n",
    "        returns = np.array(returns)\n",
    "        returns = returns[~np.isnan(returns)]\n",
    "\n",
    "    if len(returns) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # Calculate excess returns by subtracting the per-period risk-free rate\n",
    "    excess_returns = returns - (risk_free_rate / periods_per_year)\n",
    "\n",
    "    # Calculate mean and standard deviation of excess returns\n",
    "    mean_excess_return = np.mean(excess_returns) * periods_per_year  # Annualize mean\n",
    "    std_excess_return = np.std(excess_returns, ddof=1) * np.sqrt(\n",
    "        periods_per_year\n",
    "    )  # Annualize std\n",
    "\n",
    "    # Compute Sharpe Ratio with numerical stability\n",
    "    sharpe_ratio = mean_excess_return / (\n",
    "        std_excess_return + 1e-18\n",
    "    )  # Add epsilon to prevent division by zero\n",
    "\n",
    "    return sharpe_ratio\n",
    "\n",
    "\n",
    "def analyze_returns(\n",
    "    returns: pd.DataFrame, risk_free_rate: float = 0.02, frequency: int = 52\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze the returns DataFrame to verify date index, count total weeks,\n",
    "    identify years covered, count weeks per year, and compute Sharpe Ratio per year.\n",
    "\n",
    "    Args:\n",
    "        returns (pd.DataFrame): DataFrame containing portfolio returns with a DatetimeIndex.\n",
    "        risk_free_rate (float, optional): Annual risk-free rate (default is 2%). Defaults to 0.02.\n",
    "        frequency (int, optional): Number of periods per year (default is 52 for weekly data). Defaults to 52.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary containing analysis results.\n",
    "    \"\"\"\n",
    "    analysis_results: Dict[str, Any] = {}\n",
    "\n",
    "    # 1. Verify that the DataFrame is indexed by dates\n",
    "    if not isinstance(returns.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"The DataFrame index must be a pandas DatetimeIndex.\")\n",
    "    analysis_results[\"DateIndexValid\"] = True\n",
    "\n",
    "    # 2. Count the total number of weeks\n",
    "    total_weeks: int = len(returns)\n",
    "    analysis_results[\"TotalWeeks\"] = total_weeks\n",
    "\n",
    "    # 3. Identify all the years covered in the dataset\n",
    "    years: list[int] = returns.index.year.unique().tolist()\n",
    "    years.sort()  # Sort the years in ascending order\n",
    "    analysis_results[\"YearsCovered\"] = years\n",
    "\n",
    "    # 4. Count the number of weeks for each individual year\n",
    "    weeks_per_year: Dict[int, int] = {}\n",
    "    grouped = returns.groupby(returns.index.year)\n",
    "\n",
    "    for year, group in grouped:\n",
    "        weeks_count = len(group)\n",
    "        weeks_per_year[year] = weeks_count\n",
    "\n",
    "    analysis_results[\"WeeksPerYear\"] = weeks_per_year\n",
    "\n",
    "    # 5. Compute Sharpe Ratio for each year\n",
    "    sharpe_ratios_per_year: Dict[int, float] = {}\n",
    "    for year, group in grouped:\n",
    "        sharpe = compute_annualized_sharpe_ratio(\n",
    "            returns=group[\"rets\"],\n",
    "            risk_free_rate=risk_free_rate,\n",
    "            periods_per_year=analysis_results[\"WeeksPerYear\"][year],\n",
    "        )\n",
    "        sharpe_ratios_per_year[year] = sharpe\n",
    "\n",
    "    analysis_results[\"SharpeRatiosPerYear\"] = sharpe_ratios_per_year\n",
    "\n",
    "    return analysis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E2E_net_Eps_Control(nn.Module):\n",
    "    \"\"\"End-to-end Distributionally Robust Optimization (DRO) learning neural net module.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_input_features: int,\n",
    "        num_assets: int,\n",
    "        num_observations: int,\n",
    "        optimization_layer: str = \"hellinger\",\n",
    "        prisk: str = \"p_var\",\n",
    "        performance_objective: str = \"sharpe_loss\",\n",
    "        pred_model: str = \"3layer\",\n",
    "        prediction_loss_factor: float | None = 0.5,\n",
    "        performance_period: int = 13,\n",
    "        train_pred: bool = True,\n",
    "        train_gamma: bool = True,\n",
    "        train_delta: bool = True,\n",
    "        set_seed: int | None = None,\n",
    "        cache_path: str = \"./cache/\",\n",
    "        self_overall_std_dev_factor: float = 1.0,\n",
    "        model_name: str = \"E2E_net_Eps_Control\",\n",
    "    ) -> None:\n",
    "        super(E2E_net_Eps_Control, self).__init__()\n",
    "\n",
    "        # Set random seed for reproducibility\n",
    "        if set_seed is not None:\n",
    "            torch.manual_seed(set_seed)\n",
    "            self.seed: int = set_seed\n",
    "\n",
    "        self.num_features: int = num_input_features  # Number of input features\n",
    "        self.num_assets: int = num_assets  # Number of assets\n",
    "        self.num_observations: int = (\n",
    "            num_observations  # Number of observations/scenarios\n",
    "        )\n",
    "\n",
    "        # Prediction loss function\n",
    "        if prediction_loss_factor is not None:\n",
    "            self.pred_loss_factor: float = prediction_loss_factor\n",
    "            self.pred_loss = nn.MSELoss()  # Mean squared error loss\n",
    "        else:\n",
    "            self.pred_loss = None\n",
    "\n",
    "        # Performance loss function\n",
    "        if performance_objective in perf_loss_functions:\n",
    "            self.perf_loss = perf_loss_functions[performance_objective]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown perf_loss function: {performance_objective}\")\n",
    "\n",
    "        self.perf_period: int = performance_period\n",
    "\n",
    "        # Initialize gamma parameter\n",
    "        self.gamma: nn.Parameter = nn.Parameter(\n",
    "            torch.FloatTensor(1).uniform_(0.02, 0.1)\n",
    "        )\n",
    "        self.gamma.requires_grad = train_gamma\n",
    "        self.gamma_init: float = self.gamma.item()\n",
    "\n",
    "        ub: float = (1 - 1 / (num_observations**0.5)) / 2\n",
    "        lb: float = (1 - 1 / (num_observations**0.5)) / 10\n",
    "        self.delta: nn.Parameter = nn.Parameter(torch.FloatTensor(1).uniform_(lb, ub))\n",
    "        self.delta.requires_grad = train_delta\n",
    "        self.delta_init: float = self.delta.item()\n",
    "        self.model_type = \"dro\"\n",
    "\n",
    "        self.pred_model: str = pred_model\n",
    "\n",
    "        if pred_model == \"2layer\":\n",
    "            hidden_size = int(0.5 * (num_input_features + num_assets))\n",
    "            self.pred_layer = nn.Sequential(\n",
    "                nn.Linear(num_input_features, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, num_assets),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_assets, num_assets),\n",
    "            )\n",
    "        elif pred_model == \"3layer\":\n",
    "            hidden_size1 = int(0.5 * (num_input_features + num_assets))\n",
    "            hidden_size2 = int(0.6 * (num_input_features + num_assets))\n",
    "            self.pred_layer = nn.Sequential(\n",
    "                nn.Linear(num_input_features, hidden_size1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size1, hidden_size2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size2, num_assets),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_assets, num_assets),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pred_model type: {pred_model}\")\n",
    "\n",
    "        # Define the optimization layer\n",
    "        if optimization_layer in opt_layer_functions:\n",
    "            if prisk in risk_functions:\n",
    "                self.opt_layer = opt_layer_functions[optimization_layer](\n",
    "                    num_assets, num_observations, risk_functions[prisk]\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prisk function: {prisk}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown opt_layer function: {optimization_layer}\")\n",
    "\n",
    "        self.cache_path: str = cache_path\n",
    "\n",
    "        self.overall_std_dev_factor: float = self_overall_std_dev_factor\n",
    "        self.model_name: str = model_name\n",
    "\n",
    "    def forward(\n",
    "        self, X: torch.Tensor, Y: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Generate predictions for each time step in X\n",
    "        Y_hat: torch.Tensor = self.pred_layer(X)  # Shape: (n_obs + 1, n_y)\n",
    "        # Calculate residuals\n",
    "        ep: torch.Tensor = Y - Y_hat[:-1]  # Shape: (n_obs, n_y)\n",
    "        # Calculate overall standard deviation (scalar)\n",
    "        self.overall_eps_std_dev: torch.Tensor = (\n",
    "            torch.std(ep, unbiased=True).to(\"cpu\").detach().numpy()\n",
    "        )\n",
    "\n",
    "        # Extract the last prediction\n",
    "        y_hat: torch.Tensor = Y_hat[-1]  # Shape: (n_y,)\n",
    "\n",
    "        # Solver arguments\n",
    "        solver_args: Dict[str, Any] = {\n",
    "            \"solve_method\": \"ECOS\",\n",
    "            \"max_iters\": 120,\n",
    "            \"abstol\": 1e-7,\n",
    "        }\n",
    "\n",
    "        # Optimize z_star\n",
    "        z_star: torch.Tensor\n",
    "        (z_star,) = self.opt_layer(\n",
    "            ep, y_hat, self.gamma, self.delta, solver_args=solver_args\n",
    "        )\n",
    "\n",
    "        return z_star, y_hat\n",
    "\n",
    "    def net_train(\n",
    "        self,\n",
    "        train_set: DataLoader,\n",
    "        val_set: DataLoader | None = None,\n",
    "        epochs: int | None = None,\n",
    "        lr: float | None = None,\n",
    "    ) -> float | None:\n",
    "        # Assign number of epochs and learning rate\n",
    "        if epochs is None:\n",
    "            epochs = self.epochs\n",
    "        if lr is None:\n",
    "            lr = self.lr\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        n_train: int = len(train_set)\n",
    "        self.train()\n",
    "        for epoch in range(epochs):\n",
    "            train_loss: float = 0.0\n",
    "            optimizer.zero_grad()\n",
    "            for _, (x, y, y_perf) in enumerate(train_set):\n",
    "                # Move tensors to the same device as the model\n",
    "                x = x.to(next(self.parameters()).device)\n",
    "                y = y.to(next(self.parameters()).device)\n",
    "                y_perf = y_perf.to(next(self.parameters()).device)\n",
    "\n",
    "                # Forward pass\n",
    "                z_star, y_hat = self(x.squeeze(), y.squeeze())\n",
    "\n",
    "                # Compute loss\n",
    "                if self.pred_loss is None:\n",
    "                    loss = (1 / n_train) * self.perf_loss(z_star, y_perf.squeeze())\n",
    "                else:\n",
    "                    loss = (1 / n_train) * (\n",
    "                        self.perf_loss(z_star, y_perf.squeeze())\n",
    "                        + (self.pred_loss_factor / self.num_assets)\n",
    "                        * self.pred_loss(y_hat, y_perf.squeeze()[0])\n",
    "                        + (\n",
    "                            self.overall_std_dev_factor\n",
    "                            # / self.num_assets\n",
    "                            # / self.num_observations\n",
    "                        )\n",
    "                        * self.overall_eps_std_dev\n",
    "                    )\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Accumulate loss\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "            # Ensure gamma and delta remain positive\n",
    "            for name, param in self.named_parameters():\n",
    "                if name == \"gamma\":\n",
    "                    param.data.clamp_(min=0.0001)\n",
    "\n",
    "        # Validation\n",
    "        if val_set is not None:\n",
    "            n_val: int = len(val_set)\n",
    "            val_loss: float = 0.0\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                for t, (x, y, y_perf) in enumerate(val_set):\n",
    "                    # Forward pass\n",
    "                    z_val, y_val = self(x.squeeze(), y.squeeze())\n",
    "\n",
    "                    # Compute loss\n",
    "                    if self.pred_loss is None:\n",
    "                        loss = (1 / n_val) * self.perf_loss(z_val, y_perf.squeeze())\n",
    "                    else:\n",
    "                        loss = (1 / n_val) * (\n",
    "                            self.perf_loss(z_val, y_perf.squeeze())\n",
    "                            + (self.pred_loss_factor / self.num_assets)\n",
    "                            * self.pred_loss(y_val, y_perf.squeeze()[0])\n",
    "                            + (\n",
    "                                self.overall_std_dev_factor\n",
    "                                # / self.num_assets\n",
    "                                # / self.num_observations\n",
    "                            )\n",
    "                            * self.overall_eps_std_dev\n",
    "                        )\n",
    "\n",
    "                    # Accumulate validation loss\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            return val_loss\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    # net_roll_test: Test the e2e neural net\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def net_roll_test(\n",
    "        self,\n",
    "        X: TrainTest,\n",
    "        Y: TrainTest,\n",
    "        n_roll: int,\n",
    "        lr: float,\n",
    "        epochs: int,\n",
    "        load_state: list[bool] = [False, False, False, False],\n",
    "        save_state: list[bool] = [False, False, False, False],\n",
    "    ) -> None:\n",
    "        # Initialize backtest object\n",
    "        portfolio = BackTest(\n",
    "            len(Y.test()) - Y.number_of_observation_per_window,\n",
    "            self.num_assets,\n",
    "            Y.test().index[Y.number_of_observation_per_window :],\n",
    "        )\n",
    "\n",
    "        # Initialize lists to store trained parameters\n",
    "        self.gamma_trained = []\n",
    "        self.delta_trained = []\n",
    "\n",
    "        # Store initial split\n",
    "        init_split = Y.split_ratio\n",
    "\n",
    "        # Calculate window size\n",
    "        win_size = init_split[1] / n_roll\n",
    "\n",
    "        split = [0, 0]\n",
    "        t = 0\n",
    "        for i in range(n_roll):\n",
    "\n",
    "            print(f\"Out-of-sample window: {i+1} / {n_roll}\")\n",
    "\n",
    "            split[0] = init_split[0] + win_size * i\n",
    "            if i < n_roll - 1:\n",
    "                split[1] = win_size\n",
    "            else:\n",
    "                split[1] = 1 - split[0]\n",
    "\n",
    "            X.split_update(split)\n",
    "            Y.split_update(split)\n",
    "            train_set = DataLoader(\n",
    "                SlidingWindow(\n",
    "                    X.train(), Y.train(), self.num_observations, self.perf_period\n",
    "                )\n",
    "            )\n",
    "            test_set = DataLoader(\n",
    "                SlidingWindow(X.test(), Y.test(), self.num_observations, 0)\n",
    "            )\n",
    "            if load_state[i]:\n",
    "                # Reset model parameters to initial state\n",
    "                self.load_state_dict(\n",
    "                    torch.load(\n",
    "                        self.cache_path + self.model_name + \"_model_\" + str(i) + \".pt\",\n",
    "                        weights_only=True,\n",
    "                    )\n",
    "                )\n",
    "            # Train the model\n",
    "            self.train()\n",
    "            self.net_train(train_set, lr=lr, epochs=epochs)\n",
    "            # Save the trained model\n",
    "            if save_state[i]:\n",
    "                torch.save(\n",
    "                    self.state_dict(),\n",
    "                    self.cache_path + self.model_name + \"_model_\" + str(i) + \".pt\",\n",
    "                )\n",
    "            self.gamma_trained.append(self.gamma.item())\n",
    "            self.delta_trained.append(self.delta.item())\n",
    "            # Test the model\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                for __, (x, y, y_perf) in enumerate(test_set):\n",
    "                    # Move tensors to the same device as the model\n",
    "                    x = x.to(next(self.parameters()).device)\n",
    "                    y = y.to(next(self.parameters()).device)\n",
    "                    y_perf = y_perf.to(next(self.parameters()).device)\n",
    "\n",
    "                    z_star, _ = self(x.squeeze(), y.squeeze())\n",
    "                    if not np.isclose(torch.sum(z_star).cpu().numpy(), 1.0, atol=1e-2):\n",
    "                        print(z_star)\n",
    "\n",
    "                    portfolio.weights[t] = z_star.squeeze().cpu().numpy()\n",
    "                    portfolio.rets[t] = (\n",
    "                        y_perf.squeeze().cpu().numpy() @ portfolio.weights[t]\n",
    "                    ).item()\n",
    "                    t += 1\n",
    "\n",
    "        # Reset dataset splits\n",
    "        X.split_update(init_split)\n",
    "        Y.split_update(init_split)\n",
    "\n",
    "        # Calculate portfolio statistics\n",
    "        portfolio.stats()\n",
    "        self.portfolio = portfolio\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    # load_cv_results: Load cross-validation results\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def load_cv_results(self, cv_results):\n",
    "        self.cv_results = cv_results\n",
    "\n",
    "        # Select and store the optimal hyperparameters\n",
    "        idx = cv_results.val_loss.idxmin()\n",
    "        self.lr = cv_results.lr[idx]\n",
    "        self.epochs = cv_results.epochs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "Out-of-sample window: 1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhang\\anaconda3\\envs\\py12\\Lib\\site-packages\\diffcp\\cone_program.py:371: UserWarning: Solved/Inaccurate.\n",
      "  warnings.warn(\"Solved/Inaccurate.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60, Loss: -0.3694\n",
      "Epoch 2/60, Loss: -0.3107\n",
      "Epoch 3/60, Loss: -0.3493\n",
      "Epoch 4/60, Loss: -0.3493\n",
      "Epoch 5/60, Loss: -0.3337\n",
      "Epoch 6/60, Loss: -0.3385\n",
      "Epoch 7/60, Loss: -0.3469\n",
      "Epoch 8/60, Loss: -0.3533\n",
      "Epoch 9/60, Loss: -0.3562\n",
      "Epoch 10/60, Loss: -0.3501\n",
      "Epoch 11/60, Loss: -0.3472\n",
      "Epoch 12/60, Loss: -0.3519\n",
      "Epoch 13/60, Loss: -0.3589\n",
      "Epoch 14/60, Loss: -0.3628\n",
      "Epoch 15/60, Loss: -0.3628\n",
      "Epoch 16/60, Loss: -0.3616\n",
      "Epoch 17/60, Loss: -0.3608\n",
      "Epoch 18/60, Loss: -0.3602\n",
      "Epoch 19/60, Loss: -0.3621\n",
      "Epoch 20/60, Loss: -0.3649\n",
      "Epoch 21/60, Loss: -0.3655\n",
      "Epoch 22/60, Loss: -0.3646\n",
      "Epoch 23/60, Loss: -0.3656\n",
      "Epoch 24/60, Loss: -0.3674\n",
      "Epoch 25/60, Loss: -0.3688\n",
      "Epoch 26/60, Loss: -0.3693\n",
      "Epoch 27/60, Loss: -0.3674\n",
      "Epoch 28/60, Loss: -0.3693\n",
      "Epoch 29/60, Loss: -0.3701\n",
      "Epoch 30/60, Loss: -0.3701\n",
      "Epoch 31/60, Loss: -0.3700\n",
      "Epoch 32/60, Loss: -0.3706\n",
      "Epoch 33/60, Loss: -0.3710\n",
      "Epoch 34/60, Loss: -0.3714\n",
      "Epoch 35/60, Loss: -0.3717\n",
      "Epoch 36/60, Loss: -0.3719\n",
      "Epoch 37/60, Loss: -0.3725\n",
      "Epoch 38/60, Loss: -0.3729\n",
      "Epoch 39/60, Loss: -0.3734\n",
      "Epoch 40/60, Loss: -0.3737\n",
      "Epoch 41/60, Loss: -0.3735\n",
      "Epoch 42/60, Loss: -0.3738\n",
      "Epoch 43/60, Loss: -0.3739\n",
      "Epoch 44/60, Loss: -0.3745\n",
      "Epoch 45/60, Loss: -0.3744\n",
      "Epoch 46/60, Loss: -0.3744\n",
      "Epoch 47/60, Loss: -0.3751\n",
      "Epoch 48/60, Loss: -0.3749\n",
      "Epoch 49/60, Loss: -0.3750\n",
      "Epoch 50/60, Loss: -0.3755\n",
      "Epoch 51/60, Loss: -0.3755\n",
      "Epoch 52/60, Loss: -0.3759\n",
      "Epoch 53/60, Loss: -0.3762\n",
      "Epoch 54/60, Loss: -0.3763\n",
      "Epoch 55/60, Loss: -0.3764\n",
      "Epoch 56/60, Loss: -0.3763\n",
      "Epoch 57/60, Loss: -0.3765\n",
      "Epoch 58/60, Loss: -0.3768\n",
      "Epoch 59/60, Loss: -0.3756\n",
      "Epoch 60/60, Loss: -0.3767\n",
      "Out-of-sample window: 2 / 4\n",
      "Epoch 1/60, Loss: -0.3685\n",
      "Epoch 2/60, Loss: -0.3283\n",
      "Epoch 3/60, Loss: -0.3296\n",
      "Epoch 4/60, Loss: -0.3363\n",
      "Epoch 5/60, Loss: -0.3444\n",
      "Epoch 6/60, Loss: -0.3472\n",
      "Epoch 7/60, Loss: -0.3503\n",
      "Epoch 8/60, Loss: -0.3516\n",
      "Epoch 9/60, Loss: -0.3492\n",
      "Epoch 10/60, Loss: -0.3549\n",
      "Epoch 11/60, Loss: -0.3504\n",
      "Epoch 12/60, Loss: -0.3538\n",
      "Epoch 13/60, Loss: -0.3575\n",
      "Epoch 14/60, Loss: -0.3540\n",
      "Epoch 15/60, Loss: -0.3580\n",
      "Epoch 16/60, Loss: -0.3584\n",
      "Epoch 17/60, Loss: -0.3562\n",
      "Epoch 18/60, Loss: -0.3601\n",
      "Epoch 19/60, Loss: -0.3616\n",
      "Epoch 20/60, Loss: -0.3615\n",
      "Epoch 21/60, Loss: -0.3642\n",
      "Epoch 22/60, Loss: -0.3629\n",
      "Epoch 23/60, Loss: -0.3641\n",
      "Epoch 24/60, Loss: -0.3654\n",
      "Epoch 25/60, Loss: -0.3638\n",
      "Epoch 26/60, Loss: -0.3672\n",
      "Epoch 27/60, Loss: -0.3667\n",
      "Epoch 28/60, Loss: -0.3663\n",
      "Epoch 29/60, Loss: -0.3677\n",
      "Epoch 30/60, Loss: -0.3686\n",
      "Epoch 31/60, Loss: -0.3701\n",
      "Epoch 32/60, Loss: -0.3699\n",
      "Epoch 33/60, Loss: -0.3710\n",
      "Epoch 34/60, Loss: -0.3710\n",
      "Epoch 35/60, Loss: -0.3719\n",
      "Epoch 36/60, Loss: -0.3721\n",
      "Epoch 37/60, Loss: -0.3720\n",
      "Epoch 38/60, Loss: -0.3729\n",
      "Epoch 39/60, Loss: -0.3733\n",
      "Epoch 40/60, Loss: -0.3733\n",
      "Epoch 41/60, Loss: -0.3732\n",
      "Epoch 42/60, Loss: -0.3753\n",
      "Epoch 43/60, Loss: -0.3753\n",
      "Epoch 44/60, Loss: -0.3743\n",
      "Epoch 45/60, Loss: -0.3754\n",
      "Epoch 46/60, Loss: -0.3756\n",
      "Epoch 47/60, Loss: -0.3744\n",
      "Epoch 48/60, Loss: -0.3759\n",
      "Epoch 49/60, Loss: -0.3759\n",
      "Epoch 50/60, Loss: -0.3759\n",
      "Epoch 51/60, Loss: -0.3765\n",
      "Epoch 52/60, Loss: -0.3757\n",
      "Epoch 53/60, Loss: -0.3748\n",
      "Epoch 54/60, Loss: -0.3743\n",
      "Epoch 55/60, Loss: -0.3734\n",
      "Epoch 56/60, Loss: -0.3744\n",
      "Epoch 57/60, Loss: -0.3751\n",
      "Epoch 58/60, Loss: -0.3760\n",
      "Epoch 59/60, Loss: -0.3763\n",
      "Epoch 60/60, Loss: -0.3762\n",
      "Out-of-sample window: 3 / 4\n",
      "Epoch 1/60, Loss: -0.3674\n",
      "Epoch 2/60, Loss: -0.3076\n",
      "Epoch 3/60, Loss: -0.3290\n",
      "Epoch 4/60, Loss: -0.3188\n",
      "Epoch 5/60, Loss: -0.3454\n",
      "Epoch 6/60, Loss: -0.3574\n",
      "Epoch 7/60, Loss: -0.3507\n",
      "Epoch 8/60, Loss: -0.3568\n",
      "Epoch 9/60, Loss: -0.3516\n",
      "Epoch 10/60, Loss: -0.3558\n",
      "Epoch 11/60, Loss: -0.3600\n",
      "Epoch 12/60, Loss: -0.3595\n",
      "Epoch 13/60, Loss: -0.3589\n",
      "Epoch 14/60, Loss: -0.3574\n",
      "Epoch 15/60, Loss: -0.3548\n",
      "Epoch 16/60, Loss: -0.3589\n",
      "Epoch 17/60, Loss: -0.3615\n",
      "Epoch 18/60, Loss: -0.3637\n",
      "Epoch 19/60, Loss: -0.3655\n",
      "Epoch 20/60, Loss: -0.3661\n",
      "Epoch 21/60, Loss: -0.3662\n",
      "Epoch 22/60, Loss: -0.3657\n",
      "Epoch 23/60, Loss: -0.3663\n",
      "Epoch 24/60, Loss: -0.3646\n",
      "Epoch 25/60, Loss: -0.3558\n",
      "Epoch 26/60, Loss: -0.3584\n",
      "Epoch 27/60, Loss: -0.3642\n",
      "Epoch 28/60, Loss: -0.3607\n",
      "Epoch 29/60, Loss: -0.3606\n",
      "Epoch 30/60, Loss: -0.3590\n",
      "Epoch 31/60, Loss: -0.3637\n",
      "Epoch 32/60, Loss: -0.3672\n",
      "Epoch 33/60, Loss: -0.3639\n",
      "Epoch 34/60, Loss: -0.3659\n",
      "Epoch 35/60, Loss: -0.3643\n",
      "Epoch 36/60, Loss: -0.3652\n",
      "Epoch 37/60, Loss: -0.3662\n",
      "Epoch 38/60, Loss: -0.3650\n",
      "Epoch 39/60, Loss: -0.3624\n",
      "Epoch 40/60, Loss: -0.3676\n",
      "Epoch 41/60, Loss: -0.3667\n",
      "Epoch 42/60, Loss: -0.3665\n",
      "Epoch 43/60, Loss: -0.3666\n",
      "Epoch 44/60, Loss: -0.3673\n",
      "Epoch 45/60, Loss: -0.3648\n",
      "Epoch 46/60, Loss: -0.3674\n",
      "Epoch 47/60, Loss: -0.3659\n",
      "Epoch 48/60, Loss: -0.3668\n",
      "Epoch 49/60, Loss: -0.3673\n",
      "Epoch 50/60, Loss: -0.3674\n",
      "Epoch 51/60, Loss: -0.3683\n",
      "Epoch 52/60, Loss: -0.3702\n",
      "Epoch 53/60, Loss: -0.3699\n",
      "Epoch 54/60, Loss: -0.3700\n",
      "Epoch 55/60, Loss: -0.3691\n",
      "Epoch 56/60, Loss: -0.3712\n",
      "Epoch 57/60, Loss: -0.3714\n",
      "Epoch 58/60, Loss: -0.3711\n",
      "Epoch 59/60, Loss: -0.3715\n",
      "Epoch 60/60, Loss: -0.3713\n",
      "Out-of-sample window: 4 / 4\n",
      "Epoch 1/60, Loss: -0.3717\n",
      "Epoch 2/60, Loss: -0.3291\n",
      "Epoch 3/60, Loss: -0.3354\n",
      "Epoch 4/60, Loss: -0.3515\n",
      "Epoch 5/60, Loss: -0.3536\n",
      "Epoch 6/60, Loss: -0.3459\n",
      "Epoch 7/60, Loss: -0.3567\n",
      "Epoch 8/60, Loss: -0.3595\n",
      "Epoch 9/60, Loss: -0.3588\n",
      "Epoch 10/60, Loss: -0.3602\n",
      "Epoch 11/60, Loss: -0.3612\n",
      "Epoch 12/60, Loss: -0.3611\n",
      "Epoch 13/60, Loss: -0.3624\n",
      "Epoch 14/60, Loss: -0.3613\n",
      "Epoch 15/60, Loss: -0.3615\n",
      "Epoch 16/60, Loss: -0.3644\n",
      "Epoch 17/60, Loss: -0.3634\n",
      "Epoch 18/60, Loss: -0.3636\n",
      "Epoch 19/60, Loss: -0.3644\n",
      "Epoch 20/60, Loss: -0.3667\n",
      "Epoch 21/60, Loss: -0.3675\n",
      "Epoch 22/60, Loss: -0.3680\n",
      "Epoch 23/60, Loss: -0.3677\n",
      "Epoch 24/60, Loss: -0.3680\n",
      "Epoch 25/60, Loss: -0.3673\n",
      "Epoch 26/60, Loss: -0.3662\n",
      "Epoch 27/60, Loss: -0.3678\n",
      "Epoch 28/60, Loss: -0.3701\n",
      "Epoch 29/60, Loss: -0.3696\n",
      "Epoch 30/60, Loss: -0.3702\n",
      "Epoch 31/60, Loss: -0.3706\n",
      "Epoch 32/60, Loss: -0.3704\n",
      "Epoch 33/60, Loss: -0.3716\n",
      "Epoch 34/60, Loss: -0.3717\n",
      "Epoch 35/60, Loss: -0.3720\n",
      "Epoch 36/60, Loss: -0.3725\n",
      "Epoch 37/60, Loss: -0.3727\n",
      "Epoch 38/60, Loss: -0.3729\n",
      "Epoch 39/60, Loss: -0.3734\n",
      "Epoch 40/60, Loss: -0.3737\n",
      "Epoch 41/60, Loss: -0.3742\n",
      "Epoch 42/60, Loss: -0.3733\n",
      "Epoch 43/60, Loss: -0.3747\n",
      "Epoch 44/60, Loss: -0.3738\n",
      "Epoch 45/60, Loss: -0.3755\n",
      "Epoch 46/60, Loss: -0.3747\n",
      "Epoch 47/60, Loss: -0.3749\n",
      "Epoch 48/60, Loss: -0.3749\n",
      "Epoch 49/60, Loss: -0.3748\n",
      "Epoch 50/60, Loss: -0.3738\n",
      "Epoch 51/60, Loss: -0.3746\n",
      "Epoch 52/60, Loss: -0.3708\n",
      "Epoch 53/60, Loss: -0.3635\n",
      "Epoch 54/60, Loss: -0.3629\n",
      "Epoch 55/60, Loss: -0.3670\n",
      "Epoch 56/60, Loss: -0.3701\n",
      "Epoch 57/60, Loss: -0.3732\n",
      "Epoch 58/60, Loss: -0.3692\n",
      "Epoch 59/60, Loss: -0.3700\n",
      "Epoch 60/60, Loss: -0.3713\n"
     ]
    }
   ],
   "source": [
    "n_roll: int = 4  # Number of rolling windows\n",
    "dr_net_eps_0 = E2E_net_Eps_Control(\n",
    "    num_input_features=n_X,\n",
    "    num_assets=n_Y,\n",
    "    num_observations=number_of_observe_per_window,\n",
    "    prisk=\"p_var\",\n",
    "    train_pred=True,\n",
    "    train_gamma=True,\n",
    "    train_delta=True,\n",
    "    set_seed=19260817,\n",
    "    optimization_layer=\"hellinger\",\n",
    "    performance_objective=\"sharpe_loss\",\n",
    "    cache_path=\"./cache/\",\n",
    "    performance_period=13,\n",
    "    prediction_loss_factor=0.5,\n",
    "    self_overall_std_dev_factor=0,\n",
    "    model_name=\"E2E_net_Eps_Control_0\",\n",
    ").double()\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "dr_net_eps_0.to(device)\n",
    "dr_net_eps_0.net_roll_test(\n",
    "    X_data,\n",
    "    Y_data,\n",
    "    n_roll=n_roll,\n",
    "    lr=0.005,\n",
    "    epochs=60,\n",
    "    load_state=[True] * (n_roll),\n",
    "    save_state=[True] * (n_roll),\n",
    ")\n",
    "portfolio_0 = dr_net_eps_0.portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe Ratio:  0.17236529318423396\n",
      "Mean Return:  0.003694613518108625\n",
      "Volatility:  0.021434788000851242\n",
      "Annualized Sharpe Ratio:  1.2429438053722783\n"
     ]
    }
   ],
   "source": [
    "print(\"Sharpe Ratio: \", portfolio_0.sharpe)\n",
    "print(\"Mean Return: \", portfolio_0.mean)\n",
    "print(\"Volatility: \", portfolio_0.vol)\n",
    "print(\"Annualized Sharpe Ratio: \", portfolio_0.sharpe * np.sqrt(52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "portfolio_0.plot_cumulative_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_24524\\4089635158.py:90: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  factor_5 = pdr.get_data_famafrench(\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_24524\\4089635158.py:99: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  mom_df = pdr.get_data_famafrench(\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_24524\\4089635158.py:106: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  st_df = pdr.get_data_famafrench(\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_24524\\4089635158.py:113: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  lt_df = pdr.get_data_famafrench(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6868, 8) (6868, 20)\n",
      "(6180, 8) (6180, 20) (791, 8) (791, 20)\n",
      "            Mkt-RF     SMB     HML     RMW     CMA  Mom     ST_Rev  LT_Rev\n",
      "Date                                                                      \n",
      "1997-05-16 -0.0113  0.0108  0.0037 -0.0044  0.0047 -0.0036  0.0113  0.0069\n",
      "1997-05-19  0.0027 -0.0004 -0.0028 -0.0001  0.0006  0.0024  0.0016  0.0000\n",
      "1997-05-20  0.0091 -0.0055 -0.0021  0.0030 -0.0079 -0.0001 -0.0050 -0.0078\n",
      "1997-05-21 -0.0013  0.0073 -0.0073 -0.0013 -0.0006 -0.0056  0.0022  0.0010\n",
      "1997-05-22 -0.0027  0.0064  0.0041 -0.0008  0.0008 -0.0011  0.0029  0.0005             Mkt-RF     SMB     HML     RMW     CMA  Mom     ST_Rev  LT_Rev\n",
      "Date                                                                      \n",
      "2024-08-23  0.0129  0.0190  0.0085 -0.0048  0.0068  0.0013  0.0078 -0.0011\n",
      "2024-08-26 -0.0034  0.0033  0.0017  0.0013 -0.0006 -0.0045  0.0020  0.0060\n",
      "2024-08-27  0.0005 -0.0090  0.0002  0.0027  0.0023  0.0053 -0.0080 -0.0016\n",
      "2024-08-28 -0.0067 -0.0022  0.0114  0.0055 -0.0016  0.0030  0.0029  0.0068\n",
      "2024-08-29  0.0008  0.0067  0.0028 -0.0015 -0.0122 -0.0078  0.0067  0.0021\n",
      "Ticker           PFE      COST         C       CAT       JPM       NEM  \\\n",
      "Date                                                                     \n",
      "1997-05-19 -0.003722  0.038911  0.009050 -0.005076 -0.010681  0.016835   \n",
      "1997-05-20  0.004981  0.029963  0.033632 -0.010204  0.041835  0.003311   \n",
      "1997-05-21 -0.006196 -0.040000 -0.030369 -0.002577 -0.034974 -0.003300   \n",
      "1997-05-22 -0.008728 -0.020834 -0.006711  0.007752  0.016108  0.000000   \n",
      "1997-05-23  0.011320 -0.013540  0.018018  0.000000  0.005284  0.013245   \n",
      "\n",
      "Ticker          MSFT      AAPL       BAC        VZ       HAL       XOM  \\\n",
      "Date                                                                     \n",
      "1997-05-19 -0.002707 -0.014492 -0.016563  0.000000 -0.009967  0.017204   \n",
      "1997-05-20  0.034745  0.014705  0.012632  0.000000  0.006711  0.000000   \n",
      "1997-05-21  0.010493 -0.021738 -0.027027 -0.001815  0.008333  0.006343   \n",
      "1997-05-22  0.002077 -0.014814  0.000000 -0.014546  0.001653 -0.016807   \n",
      "1997-05-23  0.018653  0.015036  0.008547  0.016606  0.000000  0.023504   \n",
      "\n",
      "Ticker             T       MCD       DIS        ED       LMT       WMT  \\\n",
      "Date                                                                     \n",
      "1997-05-19  0.002179  0.002404  0.029367  0.012931  0.016644  0.020920   \n",
      "1997-05-20  0.006521 -0.016786  0.003003  0.000000  0.013642  0.000000   \n",
      "1997-05-21 -0.017278 -0.007317 -0.016467 -0.008510  0.008075 -0.016393   \n",
      "1997-05-22 -0.017582  0.000000  0.000000 -0.017168  0.005340 -0.008333   \n",
      "1997-05-23  0.017897  0.004914  0.013699  0.013100 -0.010624  0.012605   \n",
      "\n",
      "Ticker          AMZN       JNJ  \n",
      "Date                            \n",
      "1997-05-19 -0.012040  0.016597  \n",
      "1997-05-20 -0.042685 -0.006123  \n",
      "1997-05-21 -0.127392 -0.024640  \n",
      "1997-05-22 -0.021891  0.002105  \n",
      "1997-05-23  0.074622  0.006303   Ticker           PFE      COST         C       CAT       JPM       NEM  \\\n",
      "Date                                                                     \n",
      "2024-08-26  0.000692  0.015127 -0.005632  0.007893  0.003939  0.004227   \n",
      "2024-08-27 -0.003458  0.018364 -0.001780 -0.000114  0.004608  0.008419   \n",
      "2024-08-28 -0.002429 -0.022940 -0.001621 -0.008316  0.005041 -0.016509   \n",
      "2024-08-29 -0.001044 -0.001599  0.004872  0.009879  0.004158  0.026047   \n",
      "2024-08-30  0.010098  0.006485  0.012282  0.012683  0.011656  0.003949   \n",
      "\n",
      "Ticker          MSFT      AAPL       BAC        VZ       HAL       XOM  \\\n",
      "Date                                                                     \n",
      "2024-08-26 -0.007918  0.001499  0.003772  0.006795  0.003463  0.021406   \n",
      "2024-08-27  0.000846  0.003742 -0.006263 -0.000964 -0.007844 -0.009511   \n",
      "2024-08-28 -0.007829 -0.006753  0.007058  0.000965 -0.019608 -0.009857   \n",
      "2024-08-29  0.006137  0.014570  0.005507 -0.005785  0.012258  0.013817   \n",
      "2024-08-30  0.009731 -0.003438  0.014439  0.012849 -0.009242 -0.001608   \n",
      "\n",
      "Ticker             T       MCD       DIS        ED       LMT       WMT  \\\n",
      "Date                                                                     \n",
      "2024-08-26  0.001521 -0.002901  0.013472  0.002982  0.005567  0.004359   \n",
      "2024-08-27 -0.005567  0.003326 -0.009588 -0.012983  0.004139  0.001315   \n",
      "2024-08-28  0.008651 -0.008942 -0.015512  0.008033  0.006370 -0.000657   \n",
      "2024-08-29 -0.003027  0.002822  0.003576  0.004184  0.005089  0.004469   \n",
      "2024-08-30  0.007085  0.002779  0.006347  0.007440  0.002205  0.010599   \n",
      "\n",
      "Ticker          AMZN       JNJ  \n",
      "Date                            \n",
      "2024-08-26 -0.008699  0.002924  \n",
      "2024-08-27 -0.013561 -0.002571  \n",
      "2024-08-28 -0.013401  0.005953  \n",
      "2024-08-29  0.007728  0.001891  \n",
      "2024-08-30  0.037067  0.009925  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from pandas_datareader import data as pdr\n",
    "\n",
    "# Assuming TrainTest is defined elsewhere in your codebase\n",
    "# from your_module import TrainTest\n",
    "\n",
    "\n",
    "def AV_yFinance(\n",
    "    start: str,\n",
    "    end: str,\n",
    "    split: List[float],\n",
    "    freq: str = \"weekly\",\n",
    "    n_obs: int = 104,\n",
    "    n_y: Optional[int] = None,\n",
    "    use_cache: bool = False,\n",
    "    save_results: bool = False,\n",
    ") -> Tuple[TrainTest, TrainTest]:\n",
    "\n",
    "    if use_cache:\n",
    "        X = pd.read_pickle(f\"./cache/factor_{freq}.pkl\")\n",
    "        Y = pd.read_pickle(f\"./cache/asset_{freq}.pkl\")\n",
    "    else:\n",
    "        # Define the list of tickers\n",
    "        tick_list = [\n",
    "            \"AAPL\",\n",
    "            \"MSFT\",\n",
    "            \"AMZN\",\n",
    "            \"C\",\n",
    "            \"JPM\",\n",
    "            \"BAC\",\n",
    "            \"XOM\",\n",
    "            \"HAL\",\n",
    "            \"MCD\",\n",
    "            \"WMT\",\n",
    "            \"COST\",\n",
    "            \"CAT\",\n",
    "            \"LMT\",\n",
    "            \"JNJ\",\n",
    "            \"PFE\",\n",
    "            \"DIS\",\n",
    "            \"VZ\",\n",
    "            \"T\",\n",
    "            \"ED\",\n",
    "            \"NEM\",\n",
    "        ]\n",
    "\n",
    "        if n_y is not None:\n",
    "            tick_list = tick_list[:n_y]\n",
    "\n",
    "        # Download asset data using yfinance\n",
    "        data = yf.download(\n",
    "            tick_list,\n",
    "            start=start,\n",
    "            end=end,\n",
    "            progress=False,\n",
    "            group_by=\"ticker\",\n",
    "            auto_adjust=True,  # Adjusted close prices\n",
    "            threads=True,  # Enable multi-threading for faster downloads\n",
    "        )\n",
    "\n",
    "        if data.empty:\n",
    "            raise ValueError(\n",
    "                \"No data downloaded. Please check the ticker symbols and date range.\"\n",
    "            )\n",
    "\n",
    "        # Extract Adjusted Close prices\n",
    "        if len(tick_list) == 1:\n",
    "            # For single ticker, data['Close'] is a Series, convert to DataFrame\n",
    "            adj_close = data[\"Close\"].to_frame()\n",
    "            adj_close.columns = tick_list\n",
    "        else:\n",
    "            # For multiple tickers, use xs to extract 'Close' for all tickers\n",
    "            try:\n",
    "                adj_close = data.xs(\"Close\", level=1, axis=1)\n",
    "            except KeyError:\n",
    "                raise KeyError(\"Close prices not found in the downloaded data.\")\n",
    "\n",
    "        # Compute daily returns as percentage change\n",
    "        Y = adj_close.pct_change().dropna()\n",
    "\n",
    "        # Download factor data from Kenneth French's data library\n",
    "        dl_freq = \"_daily\"\n",
    "\n",
    "        try:\n",
    "            # 5-Factor Model\n",
    "            factor_5 = pdr.get_data_famafrench(\n",
    "                \"F-F_Research_Data_5_Factors_2x3\" + dl_freq,\n",
    "                start=start,\n",
    "                end=end,\n",
    "            )[0]\n",
    "            rf_df = factor_5[\"RF\"]\n",
    "            factor_5 = factor_5.drop([\"RF\"], axis=1)\n",
    "\n",
    "            # Momentum Factor\n",
    "            mom_df = pdr.get_data_famafrench(\n",
    "                \"F-F_Momentum_Factor\" + dl_freq,\n",
    "                start=start,\n",
    "                end=end,\n",
    "            )[0]\n",
    "\n",
    "            # Short-Term Reversal Factor\n",
    "            st_df = pdr.get_data_famafrench(\n",
    "                \"F-F_ST_Reversal_Factor\" + dl_freq,\n",
    "                start=start,\n",
    "                end=end,\n",
    "            )[0]\n",
    "\n",
    "            # Long-Term Reversal Factor\n",
    "            lt_df = pdr.get_data_famafrench(\n",
    "                \"F-F_LT_Reversal_Factor\" + dl_freq,\n",
    "                start=start,\n",
    "                end=end,\n",
    "            )[0]\n",
    "\n",
    "            # Concatenate all factors and convert to decimal\n",
    "            X = pd.concat([factor_5, mom_df, st_df, lt_df], axis=1) / 100\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to download factor data: {e}\")\n",
    "\n",
    "        # Align factor data (X) with asset returns (Y) based on dates\n",
    "\n",
    "        # Remove timezone from Y.index if present\n",
    "        if Y.index.tz is not None:\n",
    "            Y.index = Y.index.tz_convert(None)\n",
    "\n",
    "        # Ensure X.index is also timezone-naive\n",
    "        if X.index.tz is not None:\n",
    "            X.index = X.index.tz_convert(None)\n",
    "\n",
    "        # Now, perform the alignment\n",
    "        try:\n",
    "            X = X.loc[Y.index]\n",
    "        except KeyError as e:\n",
    "            missing_dates = Y.index.difference(X.index)\n",
    "            if not missing_dates.empty:\n",
    "                print(f\"Missing dates in factor data: {missing_dates}\")\n",
    "                # Optionally, you can drop missing dates or handle them differently\n",
    "                Y = Y.loc[Y.index.intersection(X.index)]\n",
    "                X = X.loc[X.index.intersection(Y.index)]\n",
    "            else:\n",
    "                raise e  # Re-raise if no missing dates found\n",
    "\n",
    "        # Resample data if frequency is not daily\n",
    "        freq_lower = freq.lower()\n",
    "        if freq_lower in [\"weekly\", \"wk\", \"1wk\"]:\n",
    "            Y = Y.resample(\"W-FRI\").apply(lambda x: (x + 1).prod() - 1)\n",
    "            X = X.resample(\"W-FRI\").apply(lambda x: (x + 1).prod() - 1)\n",
    "        elif freq_lower in [\"monthly\", \"1mo\"]:\n",
    "            Y = Y.resample(\"M\").apply(lambda x: (x + 1).prod() - 1)\n",
    "            X = X.resample(\"M\").apply(lambda x: (x + 1).prod() - 1)\n",
    "        # Add more resampling frequencies if needed\n",
    "\n",
    "        # Handle missing values by forward and backward filling using ffill() and bfill()\n",
    "        Y = Y.ffill().bfill()\n",
    "        X = X.ffill().bfill()\n",
    "\n",
    "        # Convert the index to 'YYYY-MM-DD' format\n",
    "        X.index = X.index.strftime(\"%Y-%m-%d\")\n",
    "        X.index = pd.DatetimeIndex(X.index)\n",
    "        Y.index = Y.index.strftime(\"%Y-%m-%d\")\n",
    "        X.index = pd.DatetimeIndex(Y.index)\n",
    "\n",
    "        # Optionally save the results to cache\n",
    "        if save_results:\n",
    "            os.makedirs(\"./cache\", exist_ok=True)\n",
    "            X.to_pickle(f\"./cache/factor_{freq}.pkl\")\n",
    "            Y.to_pickle(f\"./cache/asset_{freq}.pkl\")\n",
    "    print(X.shape, Y.shape)\n",
    "    # Partition dataset into training and testing sets. Lag the data by one observation\n",
    "    # Using the provided TrainTest class\n",
    "    X_train_test = TrainTest(X[:-1], n_obs, split)\n",
    "    Y_train_test = TrainTest(Y[1:], n_obs, split)\n",
    "\n",
    "    return X_train_test, Y_train_test\n",
    "\n",
    "\n",
    "start_paddling = \"1997-04-01\"\n",
    "end_paddling = \"2024-09-01\"  # Data frequency and start/end dates\n",
    "daily_frequency = \"daily\"\n",
    "xf_train_test, yf_train_test = AV_yFinance(\n",
    "    start=start_paddling,\n",
    "    end=end_paddling,\n",
    "    split=[0.9, 0.1],\n",
    "    freq=daily_frequency,\n",
    "    n_obs=104,\n",
    "    n_y=20,\n",
    "    use_cache=False,\n",
    "    save_results=True,\n",
    ")\n",
    "print(\n",
    "    xf_train_test.train().shape,\n",
    "    yf_train_test.train().shape,\n",
    "    xf_train_test.test().shape,\n",
    "    yf_train_test.test().shape,\n",
    ")\n",
    "print(xf_train_test.train().head(), xf_train_test.test().tail())\n",
    "print(yf_train_test.train().head(), yf_train_test.test().tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
