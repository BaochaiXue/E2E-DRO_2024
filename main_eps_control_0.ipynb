{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")  # close all previous plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "cache_path: str = \"./cache/exp/\"\n",
    "data_frequency = \"weekly\"\n",
    "start = \"2000-01-01\"\n",
    "end = \"2021-09-30\"  # Data frequency and start/end dates\n",
    "split_ratio_list = [0.6, 0.4]  # Train, validation and test split percentage\n",
    "number_of_observe_per_window: int = 104\n",
    "number_of_asset: int = 20  # Number of assets n_y = 20\n",
    "AV_key: str = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTest:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        number_of_observation_per_window: int,\n",
    "        split_ratio_list: list[float],\n",
    "    ) -> None:\n",
    "        self.data: pd.DataFrame = data\n",
    "        self.number_of_observation_per_window: int = number_of_observation_per_window\n",
    "        self.split_ratio: list[float] = split_ratio_list\n",
    "\n",
    "        num_total_observations: int = self.data.shape[\n",
    "            0\n",
    "        ]  # Calculate the total number of observations in the DataFrame\n",
    "        num_observations_cumulative_split: list[float] = (\n",
    "            num_total_observations * np.cumsum(split_ratio_list)\n",
    "        )  # np.cumsum([0.7, 0.2, 0.1]) = [0.7, 0.9, 1.0]\n",
    "        self.cumulative_number_window_observation: list[int] = [\n",
    "            round(num_observation_cumulative_split)\n",
    "            for num_observation_cumulative_split in num_observations_cumulative_split\n",
    "        ]\n",
    "\n",
    "    def split_update(self, split_ratio_list: list[float]) -> None:\n",
    "        self.split_ratio: list[float] = split_ratio_list\n",
    "        num_observations_total: int = self.data.shape[0]\n",
    "        num_observations_cumulative_split: list[float] = (\n",
    "            num_observations_total * np.cumsum(split_ratio_list)\n",
    "        )  # np.cumsum([0.7, 0.2, 0.1]) = [0.7, 0.9, 1.0]\n",
    "        self.cumulative_number_window_observation = [\n",
    "            round(i) for i in num_observations_cumulative_split\n",
    "        ]\n",
    "\n",
    "    def train(self) -> pd.DataFrame:\n",
    "        return self.data[\n",
    "            : self.cumulative_number_window_observation[0]\n",
    "        ]  # Return the training subset of observations\n",
    "\n",
    "    def test(self):\n",
    "        if (\n",
    "            self.cumulative_number_window_observation[0]\n",
    "            - self.number_of_observation_per_window\n",
    "            < 0\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"The number of observations per window exceeds the number of observations of train data in the dataset.\"\n",
    "            )\n",
    "        return self.data[\n",
    "            self.cumulative_number_window_observation[0]\n",
    "            - self.number_of_observation_per_window : self.cumulative_number_window_observation[\n",
    "                1\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "    def shape(self):\n",
    "        return self.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_original_data: pd.DataFrame = pd.read_pickle(\n",
    "    \"./cache/factor_\" + data_frequency + \".pkl\"\n",
    ")\n",
    "Y_original_data: pd.DataFrame = pd.read_pickle(\n",
    "    \"./cache/asset_\" + data_frequency + \".pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mkt-RF</th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>Mom</th>\n",
       "      <th>ST_Rev</th>\n",
       "      <th>LT_Rev</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>-0.024889</td>\n",
       "      <td>-0.003948</td>\n",
       "      <td>0.005921</td>\n",
       "      <td>-0.008655</td>\n",
       "      <td>0.021933</td>\n",
       "      <td>-0.033839</td>\n",
       "      <td>0.034927</td>\n",
       "      <td>0.002774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-14</th>\n",
       "      <td>0.020696</td>\n",
       "      <td>0.013858</td>\n",
       "      <td>-0.015028</td>\n",
       "      <td>-0.028196</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.015969</td>\n",
       "      <td>-0.001553</td>\n",
       "      <td>0.008910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-21</th>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.060555</td>\n",
       "      <td>-0.025968</td>\n",
       "      <td>-0.048690</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>0.053417</td>\n",
       "      <td>-0.043407</td>\n",
       "      <td>0.020229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-28</th>\n",
       "      <td>-0.057084</td>\n",
       "      <td>0.009003</td>\n",
       "      <td>0.016956</td>\n",
       "      <td>0.013910</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>-0.030094</td>\n",
       "      <td>0.031843</td>\n",
       "      <td>-0.012653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-04</th>\n",
       "      <td>0.044559</td>\n",
       "      <td>-0.001065</td>\n",
       "      <td>-0.026655</td>\n",
       "      <td>-0.019944</td>\n",
       "      <td>-0.014198</td>\n",
       "      <td>0.037680</td>\n",
       "      <td>-0.001666</td>\n",
       "      <td>0.015425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Mkt-RF       SMB       HML       RMW       CMA    Mom     \\\n",
       "Date                                                                     \n",
       "2000-01-07 -0.024889 -0.003948  0.005921 -0.008655  0.021933 -0.033839   \n",
       "2000-01-14  0.020696  0.013858 -0.015028 -0.028196  0.000918  0.015969   \n",
       "2000-01-21  0.000378  0.060555 -0.025968 -0.048690  0.001365  0.053417   \n",
       "2000-01-28 -0.057084  0.009003  0.016956  0.013910  0.016216 -0.030094   \n",
       "2000-02-04  0.044559 -0.001065 -0.026655 -0.019944 -0.014198  0.037680   \n",
       "\n",
       "              ST_Rev    LT_Rev  \n",
       "Date                            \n",
       "2000-01-07  0.034927  0.002774  \n",
       "2000-01-14 -0.001553  0.008910  \n",
       "2000-01-21 -0.043407  0.020229  \n",
       "2000-01-28  0.031843 -0.012653  \n",
       "2000-02-04 -0.001666  0.015425  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_original_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>C</th>\n",
       "      <th>JPM</th>\n",
       "      <th>BAC</th>\n",
       "      <th>XOM</th>\n",
       "      <th>HAL</th>\n",
       "      <th>MCD</th>\n",
       "      <th>WMT</th>\n",
       "      <th>COST</th>\n",
       "      <th>CAT</th>\n",
       "      <th>LMT</th>\n",
       "      <th>JNJ</th>\n",
       "      <th>PFE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>VZ</th>\n",
       "      <th>T</th>\n",
       "      <th>ED</th>\n",
       "      <th>NEM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>-0.032195</td>\n",
       "      <td>-0.045482</td>\n",
       "      <td>-0.086300</td>\n",
       "      <td>-0.030347</td>\n",
       "      <td>-0.058169</td>\n",
       "      <td>-0.029886</td>\n",
       "      <td>0.054369</td>\n",
       "      <td>0.010932</td>\n",
       "      <td>-0.010667</td>\n",
       "      <td>-0.009113</td>\n",
       "      <td>0.019836</td>\n",
       "      <td>0.132809</td>\n",
       "      <td>-0.020110</td>\n",
       "      <td>0.034853</td>\n",
       "      <td>0.063502</td>\n",
       "      <td>0.064274</td>\n",
       "      <td>-0.038464</td>\n",
       "      <td>-0.089726</td>\n",
       "      <td>0.045217</td>\n",
       "      <td>-0.124898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-14</th>\n",
       "      <td>0.009447</td>\n",
       "      <td>0.007268</td>\n",
       "      <td>-0.076337</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.015533</td>\n",
       "      <td>0.037174</td>\n",
       "      <td>-0.014010</td>\n",
       "      <td>-0.052347</td>\n",
       "      <td>0.068455</td>\n",
       "      <td>-0.058394</td>\n",
       "      <td>0.054374</td>\n",
       "      <td>-0.025699</td>\n",
       "      <td>-0.043843</td>\n",
       "      <td>-0.029119</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.078060</td>\n",
       "      <td>-0.042510</td>\n",
       "      <td>-0.048266</td>\n",
       "      <td>-0.065724</td>\n",
       "      <td>-0.029384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-21</th>\n",
       "      <td>0.108224</td>\n",
       "      <td>-0.075724</td>\n",
       "      <td>-0.034086</td>\n",
       "      <td>-0.026897</td>\n",
       "      <td>-0.012723</td>\n",
       "      <td>-0.095248</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.099066</td>\n",
       "      <td>-0.036376</td>\n",
       "      <td>-0.031938</td>\n",
       "      <td>-0.011415</td>\n",
       "      <td>-0.081857</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>-0.040666</td>\n",
       "      <td>-0.052432</td>\n",
       "      <td>-0.024136</td>\n",
       "      <td>0.066596</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>-0.003859</td>\n",
       "      <td>0.018260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-28</th>\n",
       "      <td>-0.087054</td>\n",
       "      <td>-0.053012</td>\n",
       "      <td>-0.005962</td>\n",
       "      <td>-0.005493</td>\n",
       "      <td>0.051412</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>-0.072000</td>\n",
       "      <td>-0.155026</td>\n",
       "      <td>-0.104968</td>\n",
       "      <td>-0.117072</td>\n",
       "      <td>-0.051546</td>\n",
       "      <td>-0.081891</td>\n",
       "      <td>-0.100952</td>\n",
       "      <td>-0.059858</td>\n",
       "      <td>0.003708</td>\n",
       "      <td>0.122137</td>\n",
       "      <td>-0.040460</td>\n",
       "      <td>-0.087209</td>\n",
       "      <td>-0.029797</td>\n",
       "      <td>-0.053327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-04</th>\n",
       "      <td>0.062783</td>\n",
       "      <td>0.084580</td>\n",
       "      <td>0.273464</td>\n",
       "      <td>-0.021814</td>\n",
       "      <td>0.065980</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>0.025355</td>\n",
       "      <td>0.027925</td>\n",
       "      <td>-0.044354</td>\n",
       "      <td>0.021404</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>-0.027356</td>\n",
       "      <td>0.013242</td>\n",
       "      <td>-0.028047</td>\n",
       "      <td>0.015914</td>\n",
       "      <td>0.037551</td>\n",
       "      <td>0.016137</td>\n",
       "      <td>0.070064</td>\n",
       "      <td>-0.009521</td>\n",
       "      <td>0.196411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                AAPL      MSFT      AMZN         C       JPM       BAC  \\\n",
       "date                                                                     \n",
       "2000-01-07 -0.032195 -0.045482 -0.086300 -0.030347 -0.058169 -0.029886   \n",
       "2000-01-14  0.009447  0.007268 -0.076337  0.074074  0.015533  0.037174   \n",
       "2000-01-21  0.108224 -0.075724 -0.034086 -0.026897 -0.012723 -0.095248   \n",
       "2000-01-28 -0.087054 -0.053012 -0.005962 -0.005493  0.051412  0.001313   \n",
       "2000-02-04  0.062783  0.084580  0.273464 -0.021814  0.065980  0.002842   \n",
       "\n",
       "                 XOM       HAL       MCD       WMT      COST       CAT  \\\n",
       "date                                                                     \n",
       "2000-01-07  0.054369  0.010932 -0.010667 -0.009113  0.019836  0.132809   \n",
       "2000-01-14 -0.014010 -0.052347  0.068455 -0.058394  0.054374 -0.025699   \n",
       "2000-01-21  0.014925  0.099066 -0.036376 -0.031938 -0.011415 -0.081857   \n",
       "2000-01-28 -0.072000 -0.155026 -0.104968 -0.117072 -0.051546 -0.081891   \n",
       "2000-02-04  0.025355  0.027925 -0.044354  0.021404  0.152174 -0.027356   \n",
       "\n",
       "                 LMT       JNJ       PFE       DIS        VZ         T  \\\n",
       "date                                                                     \n",
       "2000-01-07 -0.020110  0.034853  0.063502  0.064274 -0.038464 -0.089726   \n",
       "2000-01-14 -0.043843 -0.029119  0.072464  0.078060 -0.042510 -0.048266   \n",
       "2000-01-21  0.024390 -0.040666 -0.052432 -0.024136  0.066596  0.023810   \n",
       "2000-01-28 -0.100952 -0.059858  0.003708  0.122137 -0.040460 -0.087209   \n",
       "2000-02-04  0.013242 -0.028047  0.015914  0.037551  0.016137  0.070064   \n",
       "\n",
       "                  ED       NEM  \n",
       "date                            \n",
       "2000-01-07  0.045217 -0.124898  \n",
       "2000-01-14 -0.065724 -0.029384  \n",
       "2000-01-21 -0.003859  0.018260  \n",
       "2000-01-28 -0.029797 -0.053327  \n",
       "2000-02-04 -0.009521  0.196411  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_original_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def AV(\n",
    "    start: str,\n",
    "    end: str,\n",
    "    split_ratio: list,\n",
    "    data_frequency: str = \"weekly\",\n",
    "    num_observations_per_window: int = 104,\n",
    "    num_assets=None,\n",
    "    use_cache: bool = False,\n",
    "    save_results: bool = False,\n",
    "    AV_key: str = None,\n",
    "):\n",
    "    if use_cache:\n",
    "        X: pd.DataFrame = pd.read_pickle(\"./cache/factor_\" + data_frequency + \".pkl\")\n",
    "        Y: pd.DataFrame = pd.read_pickle(\"./cache/asset_\" + data_frequency + \".pkl\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"We cannot download data from AlphaVantage without an API key.\"\n",
    "        )\n",
    "\n",
    "    # Partition dataset into training and testing sets. Lag the data by one observation, since we are predicting future returns, so we don't need the last observation that doesn't have a future return.\n",
    "    # we don't need the first Y observation that doesn't have a corresponding X observation.\n",
    "    return TrainTest(X[:-1], num_observations_per_window, split_ratio), TrainTest(\n",
    "        Y[1:], num_observations_per_window, split_ratio\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1134, 8)\n",
      "(680, 8)\n",
      "(680, 20)\n"
     ]
    }
   ],
   "source": [
    "X_data, Y_data = AV(\n",
    "    start,\n",
    "    end,\n",
    "    split_ratio_list,\n",
    "    data_frequency=data_frequency,\n",
    "    num_observations_per_window=number_of_observe_per_window,\n",
    "    num_assets=number_of_asset,\n",
    "    use_cache=True,\n",
    "    save_results=False,\n",
    "    AV_key=AV_key,\n",
    ")\n",
    "print(X_data.shape())\n",
    "print(X_data.train().shape)\n",
    "print(Y_data.train().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  8\n",
      "Number of assets:  20\n"
     ]
    }
   ],
   "source": [
    "# Number of features and assets\n",
    "n_X: int = X_data.train().shape[1]\n",
    "n_Y: int = Y_data.train().shape[1]\n",
    "print(\"Number of features: \", n_X)\n",
    "print(\"Number of assets: \", n_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low p-values (< 0.05) suggest that the factor significantly affects the stock returns.\n",
    "High p-values (> 0.05) suggest that the factor's effect on the stock returns is not statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Mkt-RF   SMB   HML   RMW   CMA  Mom     ST_Rev  LT_Rev\n",
      "AAPL    0.34  0.37  0.53  0.92  0.70    0.41    0.41    0.53\n",
      "MSFT    0.64  0.85  0.63  0.80  0.20    0.21    0.97    0.22\n",
      "AMZN    0.31  0.02  0.64  0.28  0.63    0.18    0.45    0.34\n",
      "C       0.25  0.69  0.02  0.04  0.21    0.07    0.02    0.24\n",
      "JPM     0.33  0.64  0.00  0.50  0.48    0.18    0.18    0.01\n",
      "BAC     0.16  0.91  0.01  0.16  0.56    0.15    0.06    0.19\n",
      "XOM     0.03  0.77  0.34  0.15  0.10    0.11    0.51    0.04\n",
      "HAL     0.92  0.48  0.47  0.14  0.14    0.42    0.92    0.05\n",
      "MCD     0.48  0.05  0.57  0.02  0.54    0.27    0.81    0.03\n",
      "WMT     0.00  0.01  0.25  0.00  0.40    0.62    0.04    0.03\n",
      "COST    0.00  0.22  0.85  0.01  0.38    0.66    0.39    0.05\n",
      "CAT     0.92  0.27  0.59  0.23  0.07    0.40    0.67    0.51\n",
      "LMT     0.27  0.74  0.04  0.00  0.61    0.32    0.38    0.60\n",
      "JNJ     0.00  0.91  0.39  0.09  0.84    0.82    0.19    0.06\n",
      "PFE     0.06  0.38  0.95  0.91  0.56    0.75    0.50    0.12\n",
      "DIS     0.35  0.67  0.82  0.01  0.39    0.61    0.19    0.04\n",
      "VZ      0.72  0.30  0.01  0.04  0.15    0.09    0.15    0.00\n",
      "T       0.62  0.80  0.00  0.95  0.01    0.14    0.86    0.00\n",
      "ED      0.30  0.94  0.96  0.00  0.23    0.89    0.73    0.16\n",
      "NEM     0.12  0.07  0.05  0.01  0.20    0.05    0.76    0.50\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def statanalysis(X: pd.DataFrame, Y: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Initialize an empty DataFrame to store p-values\n",
    "    # Rows correspond to assets (Y.columns) and columns correspond to features (X.columns)\n",
    "    stats = pd.DataFrame(\n",
    "        columns=X.columns, index=Y.columns\n",
    "    )  # Create an empty DataFrame to store the p-values\n",
    "    for ticker in Y.columns:\n",
    "        for feature in X.columns:\n",
    "            stats.loc[ticker, feature] = (\n",
    "                sm.OLS(Y[ticker].values, sm.add_constant(X[feature]).values)\n",
    "                .fit()\n",
    "                .pvalues[1]  # Get the p-value of the feature\n",
    "            )\n",
    "\n",
    "    return stats.astype(float).round(2)\n",
    "\n",
    "\n",
    "statistical_analysis: pd.DataFrame = statanalysis(X_data.train(), Y_data.train())\n",
    "print(statistical_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SlidingWindow(Dataset):\n",
    "    \"\"\"Sliding window dataset constructor for time series data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        XData: pd.DataFrame,\n",
    "        YData: pd.DataFrame,\n",
    "        num_observations: int,\n",
    "        performance_window: int,\n",
    "    ) -> None:\n",
    "        # Convert the feature DataFrame to a PyTorch tensor with double precision\n",
    "        self.X: torch.Tensor = torch.tensor(XData.values, dtype=torch.float64)\n",
    "        # Convert the asset return DataFrame to a PyTorch tensor with double precision\n",
    "        self.Y: torch.Tensor = torch.tensor(YData.values, dtype=torch.float64)\n",
    "        # Store the number of observations (scenarios) in the sliding window\n",
    "        self.num_observations: int = num_observations\n",
    "        # Store the number of scenarios in the performance window\n",
    "        self.perf_period: int = performance_window\n",
    "\n",
    "    def __getitem__(\n",
    "        self, index: int\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # Extract the feature window starting at 'index' and spanning 'n_obs + 1' time steps\n",
    "        x: torch.Tensor = self.X[index : index + self.num_observations + 1]\n",
    "        # Extract the realizations window starting at 'index' and spanning 'n_obs' time steps\n",
    "        y: torch.Tensor = self.Y[index : index + self.num_observations]\n",
    "        # Extract the performance window starting after the observations window and spanning 'perf_period + 1' time steps\n",
    "        y_future_performance: torch.Tensor = self.Y[\n",
    "            index\n",
    "            + self.num_observations : index\n",
    "            + self.num_observations\n",
    "            + self.perf_period\n",
    "            + 1\n",
    "        ]\n",
    "        # Return the extracted windows as a tuple\n",
    "        return x, y, y_future_performance\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # Calculate the effective length by subtracting the window sizes from the total data length\n",
    "        total_length: int = len(self.X) - self.num_observations - self.perf_period\n",
    "        # Return the calculated length\n",
    "        return total_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackTest:\n",
    "    \"\"\"Backtest object to store out-of-sample results.\"\"\"\n",
    "\n",
    "    def __init__(self, len_test: int, n_y: int, dates: pd.DatetimeIndex) -> None:\n",
    "        # Initialize the weights array with zeros; dimensions are (len_test, n_y)\n",
    "        self.weights: np.ndarray = np.zeros((len_test, n_y))\n",
    "        # Initialize the returns array with zeros; length is len_test\n",
    "        self.rets: np.ndarray = np.zeros(len_test)\n",
    "        # Store the dates corresponding to the out-of-sample period\n",
    "        self.dates: pd.DatetimeIndex = pd.DatetimeIndex(dates[-len_test:])\n",
    "\n",
    "    def stats(self) -> None:\n",
    "        # Calculate the cumulative product of returns plus one to get the total return index\n",
    "        tri: np.ndarray = np.cumprod(self.rets + 1)\n",
    "        # Calculate the geometric mean return over the out-of-sample period\n",
    "        self.mean: float = (tri[-1]) ** (1 / len(tri)) - 1\n",
    "        # Calculate the volatility (standard deviation) of the returns\n",
    "        self.vol: float = np.std(self.rets)\n",
    "        # Calculate the pseudo-Sharpe ratio, handling division by zero\n",
    "        self.sharpe: float = self.mean / self.vol if self.vol != 0 else np.nan\n",
    "        # Create a DataFrame with dates, realized returns, and total return index\n",
    "        self.returns = pd.DataFrame({\"Date\": self.dates, \"rets\": self.rets, \"tri\": tri})\n",
    "        # Set the 'Date' column as the index of the DataFrame\n",
    "        self.returns = self.returns.set_index(\"Date\")\n",
    "\n",
    "    def plot_cumulative_returns(\n",
    "        self,\n",
    "        figsize: tuple = (12, 6),\n",
    "        resample_freq: str | None = None,\n",
    "        title: str | None = None,\n",
    "    ) -> None:\n",
    "        if self.returns is None:\n",
    "            raise ValueError(\n",
    "                \"Returns DataFrame not initialized. Run 'compute_stats' after populating 'rets' and 'weights'.\"\n",
    "            )\n",
    "\n",
    "        data_to_plot = self.returns[\"tri\"]\n",
    "        label = \"Cumulative Return\"\n",
    "\n",
    "        if resample_freq:\n",
    "            # Resample the cumulative return\n",
    "            # For cumulative returns, resampling can be tricky. We'll resample the returns first,\n",
    "            # then compute cumulative returns on the resampled data.\n",
    "            resampled_rets = (\n",
    "                self.returns[\"rets\"]\n",
    "                .resample(resample_freq)\n",
    "                .apply(lambda x: (x + 1).prod() - 1)\n",
    "            )\n",
    "            data_to_plot = (resampled_rets + 1).cumprod()\n",
    "            label = f\"Cumulative Return ({resample_freq})\"\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.plot(data_to_plot, label=label, color=\"blue\")\n",
    "        plt.title(title if title else \"Cumulative Return Over Time\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Cumulative Return\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()  # Adjust layout to prevent clipping of labels\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "\n",
    "\n",
    "# Define the Sharpe loss function\n",
    "def sharpe_loss(z_star: torch.Tensor, y_perf: torch.Tensor) -> torch.Tensor:\n",
    "    loss = -torch.mean(y_perf @ z_star) / torch.std(y_perf @ z_star)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Define the portfolio variance risk function\n",
    "def p_var(z: cp.Variable, c: cp.Variable, x: cp.Expression) -> cp.Expression:\n",
    "    return cp.square(x @ z - c)\n",
    "\n",
    "\n",
    "# Define the Hellinger distance-based DRO optimization layer\n",
    "def hellinger(num_assets: int, num_observations: int, prisk) -> CvxpyLayer:\n",
    "    # Define decision variables\n",
    "    z = cp.Variable((num_assets, 1), nonneg=True)  # Portfolio weights\n",
    "    c_aux = cp.Variable()  # Centering parameter\n",
    "    lambda_aux = cp.Variable(nonneg=True)\n",
    "    xi_aux = cp.Variable()\n",
    "    beta_aux = cp.Variable(num_observations, nonneg=True)\n",
    "    tau_aux = cp.Variable(num_observations, nonneg=True)\n",
    "    mu_aux = cp.Variable()\n",
    "\n",
    "    # Define parameters\n",
    "    ep = cp.Parameter((num_observations, num_assets))  # Residuals matrix\n",
    "    y_hat = cp.Parameter(num_assets)  # Predicted returns\n",
    "    gamma = cp.Parameter(nonneg=True)  # Risk-return trade-off parameter\n",
    "    delta = cp.Parameter(nonneg=True)  # Ambiguity size parameter\n",
    "\n",
    "    # Define constraints\n",
    "    constraints = [\n",
    "        cp.sum(z) == 1,  # Total budget constraint\n",
    "        mu_aux == y_hat @ z,  # Expected return constraint\n",
    "    ]\n",
    "    for i in range(num_observations):\n",
    "        # Constraints based on the risk function\n",
    "        constraints += [xi_aux + lambda_aux >= prisk(z, c_aux, ep[i, :]) + tau_aux[i]]\n",
    "        constraints += [\n",
    "            beta_aux[i] >= cp.quad_over_lin(lambda_aux, tau_aux[i])\n",
    "        ]  # Constraint on the ambiguity set\n",
    "\n",
    "    # Define the objective function\n",
    "    objective = cp.Minimize(\n",
    "        xi_aux\n",
    "        + (delta - 1) * lambda_aux\n",
    "        + (1 / num_observations) * cp.sum(beta_aux)\n",
    "        - gamma * mu_aux\n",
    "    )\n",
    "\n",
    "    # Define the problem\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    # Create a CVXPY layer\n",
    "    cvxpylayer = CvxpyLayer(\n",
    "        problem, parameters=[ep, y_hat, gamma, delta], variables=[z]\n",
    "    )\n",
    "\n",
    "    return cvxpylayer\n",
    "\n",
    "\n",
    "# Define mappings for performance loss functions, risk functions, and optimization layers\n",
    "perf_loss_functions = {\n",
    "    \"sharpe_loss\": sharpe_loss,\n",
    "    # Add other performance loss functions here if needed\n",
    "}\n",
    "\n",
    "risk_functions = {\n",
    "    \"p_var\": p_var,\n",
    "    # Add other risk functions here if needed\n",
    "}\n",
    "\n",
    "opt_layer_functions = {\n",
    "    \"hellinger\": hellinger,\n",
    "    # Add other optimization layer functions here if needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def compute_annualized_sharpe_ratio(\n",
    "    returns: pd.Series | np.ndarray,\n",
    "    risk_free_rate: float = 0.02,\n",
    "    periods_per_year: int = 52,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute the annualized Sharpe Ratio.\n",
    "\n",
    "    Args:\n",
    "        returns (pd.Series | np.ndarray): Periodic returns of the portfolio.\n",
    "        risk_free_rate (float, optional): Annual risk-free rate (e.g., 0.02 for 2%). Defaults to 0.02.\n",
    "        periods_per_year (int, optional): Number of return periods in a year (e.g., 52 for weekly). Defaults to 52.\n",
    "\n",
    "    Returns:\n",
    "        float: Annualized Sharpe Ratio.\n",
    "    \"\"\"\n",
    "    if isinstance(returns, pd.Series):\n",
    "        returns = returns.dropna().values\n",
    "    else:\n",
    "        returns = np.array(returns)\n",
    "        returns = returns[~np.isnan(returns)]\n",
    "\n",
    "    if len(returns) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # Calculate excess returns by subtracting the per-period risk-free rate\n",
    "    excess_returns = returns - (risk_free_rate / periods_per_year)\n",
    "\n",
    "    # Calculate mean and standard deviation of excess returns\n",
    "    mean_excess_return = np.mean(excess_returns) * periods_per_year  # Annualize mean\n",
    "    std_excess_return = np.std(excess_returns, ddof=1) * np.sqrt(\n",
    "        periods_per_year\n",
    "    )  # Annualize std\n",
    "\n",
    "    # Compute Sharpe Ratio with numerical stability\n",
    "    sharpe_ratio = mean_excess_return / (\n",
    "        std_excess_return + 1e-18\n",
    "    )  # Add epsilon to prevent division by zero\n",
    "\n",
    "    return sharpe_ratio\n",
    "\n",
    "\n",
    "def analyze_returns(\n",
    "    returns: pd.DataFrame, risk_free_rate: float = 0.02, frequency: int = 52\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze the returns DataFrame to verify date index, count total weeks,\n",
    "    identify years covered, count weeks per year, and compute Sharpe Ratio per year.\n",
    "\n",
    "    Args:\n",
    "        returns (pd.DataFrame): DataFrame containing portfolio returns with a DatetimeIndex.\n",
    "        risk_free_rate (float, optional): Annual risk-free rate (default is 2%). Defaults to 0.02.\n",
    "        frequency (int, optional): Number of periods per year (default is 52 for weekly data). Defaults to 52.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary containing analysis results.\n",
    "    \"\"\"\n",
    "    analysis_results: Dict[str, Any] = {}\n",
    "\n",
    "    # 1. Verify that the DataFrame is indexed by dates\n",
    "    if not isinstance(returns.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"The DataFrame index must be a pandas DatetimeIndex.\")\n",
    "    analysis_results[\"DateIndexValid\"] = True\n",
    "\n",
    "    # 2. Count the total number of weeks\n",
    "    total_weeks: int = len(returns)\n",
    "    analysis_results[\"TotalWeeks\"] = total_weeks\n",
    "\n",
    "    # 3. Identify all the years covered in the dataset\n",
    "    years: list[int] = returns.index.year.unique().tolist()\n",
    "    years.sort()  # Sort the years in ascending order\n",
    "    analysis_results[\"YearsCovered\"] = years\n",
    "\n",
    "    # 4. Count the number of weeks for each individual year\n",
    "    weeks_per_year: Dict[int, int] = {}\n",
    "    grouped = returns.groupby(returns.index.year)\n",
    "\n",
    "    for year, group in grouped:\n",
    "        weeks_count = len(group)\n",
    "        weeks_per_year[year] = weeks_count\n",
    "\n",
    "    analysis_results[\"WeeksPerYear\"] = weeks_per_year\n",
    "\n",
    "    # 5. Compute Sharpe Ratio for each year\n",
    "    sharpe_ratios_per_year: Dict[int, float] = {}\n",
    "    for year, group in grouped:\n",
    "        sharpe = compute_annualized_sharpe_ratio(\n",
    "            returns=group[\"rets\"],\n",
    "            risk_free_rate=risk_free_rate,\n",
    "            periods_per_year=analysis_results[\"WeeksPerYear\"][year],\n",
    "        )\n",
    "        sharpe_ratios_per_year[year] = sharpe\n",
    "\n",
    "    analysis_results[\"SharpeRatiosPerYear\"] = sharpe_ratios_per_year\n",
    "\n",
    "    return analysis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E2E_net_Eps_Control(nn.Module):\n",
    "    \"\"\"End-to-end Distributionally Robust Optimization (DRO) learning neural net module.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_input_features: int,\n",
    "        num_assets: int,\n",
    "        num_observations: int,\n",
    "        optimization_layer: str = \"hellinger\",\n",
    "        prisk: str = \"p_var\",\n",
    "        performance_objective: str = \"sharpe_loss\",\n",
    "        pred_model: str = \"3layer\",\n",
    "        prediction_loss_factor: float | None = 0.5,\n",
    "        performance_period: int = 13,\n",
    "        train_pred: bool = True,\n",
    "        train_gamma: bool = True,\n",
    "        train_delta: bool = True,\n",
    "        set_seed: int | None = None,\n",
    "        cache_path: str = \"./cache/\",\n",
    "        self_overall_std_dev_factor: float = 1.0,\n",
    "        model_name: str = \"E2E_net_Eps_Control\",\n",
    "    ) -> None:\n",
    "        super(E2E_net_Eps_Control, self).__init__()\n",
    "\n",
    "        # Set random seed for reproducibility\n",
    "        if set_seed is not None:\n",
    "            torch.manual_seed(set_seed)\n",
    "            self.seed: int = set_seed\n",
    "\n",
    "        self.num_features: int = num_input_features  # Number of input features\n",
    "        self.num_assets: int = num_assets  # Number of assets\n",
    "        self.num_observations: int = (\n",
    "            num_observations  # Number of observations/scenarios\n",
    "        )\n",
    "\n",
    "        # Prediction loss function\n",
    "        if prediction_loss_factor is not None:\n",
    "            self.pred_loss_factor: float = prediction_loss_factor\n",
    "            self.pred_loss = nn.MSELoss()  # Mean squared error loss\n",
    "        else:\n",
    "            self.pred_loss = None\n",
    "\n",
    "        # Performance loss function\n",
    "        if performance_objective in perf_loss_functions:\n",
    "            self.perf_loss = perf_loss_functions[performance_objective]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown perf_loss function: {performance_objective}\")\n",
    "\n",
    "        self.perf_period: int = performance_period\n",
    "\n",
    "        # Initialize gamma parameter\n",
    "        self.gamma: nn.Parameter = nn.Parameter(\n",
    "            torch.FloatTensor(1).uniform_(0.02, 0.1)\n",
    "        )\n",
    "        self.gamma.requires_grad = train_gamma\n",
    "        self.gamma_init: float = self.gamma.item()\n",
    "\n",
    "        ub: float = (1 - 1 / (num_observations**0.5)) / 2\n",
    "        lb: float = (1 - 1 / (num_observations**0.5)) / 10\n",
    "        self.delta: nn.Parameter = nn.Parameter(torch.FloatTensor(1).uniform_(lb, ub))\n",
    "        self.delta.requires_grad = train_delta\n",
    "        self.delta_init: float = self.delta.item()\n",
    "        self.model_type = \"dro\"\n",
    "\n",
    "        self.pred_model: str = pred_model\n",
    "\n",
    "        if pred_model == \"2layer\":\n",
    "            hidden_size = int(0.5 * (num_input_features + num_assets))\n",
    "            self.pred_layer = nn.Sequential(\n",
    "                nn.Linear(num_input_features, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, num_assets),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_assets, num_assets),\n",
    "            )\n",
    "        elif pred_model == \"3layer\":\n",
    "            hidden_size1 = int(0.5 * (num_input_features + num_assets))\n",
    "            hidden_size2 = int(0.6 * (num_input_features + num_assets))\n",
    "            self.pred_layer = nn.Sequential(\n",
    "                nn.Linear(num_input_features, hidden_size1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size1, hidden_size2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size2, num_assets),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_assets, num_assets),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pred_model type: {pred_model}\")\n",
    "\n",
    "        # Define the optimization layer\n",
    "        if optimization_layer in opt_layer_functions:\n",
    "            if prisk in risk_functions:\n",
    "                self.opt_layer = opt_layer_functions[optimization_layer](\n",
    "                    num_assets, num_observations, risk_functions[prisk]\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prisk function: {prisk}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown opt_layer function: {optimization_layer}\")\n",
    "\n",
    "        self.cache_path: str = cache_path\n",
    "\n",
    "        self.overall_std_dev_factor: float = self_overall_std_dev_factor\n",
    "        self.model_name: str = model_name\n",
    "\n",
    "    def forward(\n",
    "        self, X: torch.Tensor, Y: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Generate predictions for each time step in X\n",
    "        Y_hat: torch.Tensor = self.pred_layer(X)  # Shape: (n_obs + 1, n_y)\n",
    "        # Calculate residuals\n",
    "        ep: torch.Tensor = Y - Y_hat[:-1]  # Shape: (n_obs, n_y)\n",
    "        # Calculate overall standard deviation (scalar)\n",
    "        self.overall_eps_std_dev: torch.Tensor = (\n",
    "            torch.std(ep, unbiased=True).to(\"cpu\").detach().numpy()\n",
    "        )\n",
    "\n",
    "        # Extract the last prediction\n",
    "        y_hat: torch.Tensor = Y_hat[-1]  # Shape: (n_y,)\n",
    "\n",
    "        # Solver arguments\n",
    "        solver_args: Dict[str, Any] = {\n",
    "            \"solve_method\": \"ECOS\",\n",
    "            \"max_iters\": 120,\n",
    "            \"abstol\": 1e-7,\n",
    "        }\n",
    "\n",
    "        # Optimize z_star\n",
    "        z_star: torch.Tensor\n",
    "        (z_star,) = self.opt_layer(\n",
    "            ep, y_hat, self.gamma, self.delta, solver_args=solver_args\n",
    "        )\n",
    "\n",
    "        return z_star, y_hat\n",
    "\n",
    "    def net_train(\n",
    "        self,\n",
    "        train_set: DataLoader,\n",
    "        val_set: DataLoader | None = None,\n",
    "        epochs: int | None = None,\n",
    "        lr: float | None = None,\n",
    "    ) -> float | None:\n",
    "        # Assign number of epochs and learning rate\n",
    "        if epochs is None:\n",
    "            epochs = self.epochs\n",
    "        if lr is None:\n",
    "            lr = self.lr\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        n_train: int = len(train_set)\n",
    "        self.train()\n",
    "        for epoch in range(epochs):\n",
    "            train_loss: float = 0.0\n",
    "            optimizer.zero_grad()\n",
    "            for _, (x, y, y_perf) in enumerate(train_set):\n",
    "                # Move tensors to the same device as the model\n",
    "                x = x.to(next(self.parameters()).device)\n",
    "                y = y.to(next(self.parameters()).device)\n",
    "                y_perf = y_perf.to(next(self.parameters()).device)\n",
    "\n",
    "                # Forward pass\n",
    "                z_star, y_hat = self(x.squeeze(), y.squeeze())\n",
    "\n",
    "                # Compute loss\n",
    "                if self.pred_loss is None:\n",
    "                    loss = (1 / n_train) * self.perf_loss(z_star, y_perf.squeeze())\n",
    "                else:\n",
    "                    loss = (1 / n_train) * (\n",
    "                        self.perf_loss(z_star, y_perf.squeeze())\n",
    "                        + (self.pred_loss_factor / self.num_assets)\n",
    "                        * self.pred_loss(y_hat, y_perf.squeeze()[0])\n",
    "                        + (\n",
    "                            self.overall_std_dev_factor\n",
    "                            # / self.num_assets\n",
    "                            # / self.num_observations\n",
    "                        )\n",
    "                        * self.overall_eps_std_dev\n",
    "                    )\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Accumulate loss\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "            # Ensure gamma and delta remain positive\n",
    "            for name, param in self.named_parameters():\n",
    "                if name == \"gamma\":\n",
    "                    param.data.clamp_(min=0.0001)\n",
    "\n",
    "        # Validation\n",
    "        if val_set is not None:\n",
    "            n_val: int = len(val_set)\n",
    "            val_loss: float = 0.0\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                for t, (x, y, y_perf) in enumerate(val_set):\n",
    "                    # Forward pass\n",
    "                    z_val, y_val = self(x.squeeze(), y.squeeze())\n",
    "\n",
    "                    # Compute loss\n",
    "                    if self.pred_loss is None:\n",
    "                        loss = (1 / n_val) * self.perf_loss(z_val, y_perf.squeeze())\n",
    "                    else:\n",
    "                        loss = (1 / n_val) * (\n",
    "                            self.perf_loss(z_val, y_perf.squeeze())\n",
    "                            + (self.pred_loss_factor / self.num_assets)\n",
    "                            * self.pred_loss(y_val, y_perf.squeeze()[0])\n",
    "                            + (\n",
    "                                self.overall_std_dev_factor\n",
    "                                # / self.num_assets\n",
    "                                # / self.num_observations\n",
    "                            )\n",
    "                            * self.overall_eps_std_dev\n",
    "                        )\n",
    "\n",
    "                    # Accumulate validation loss\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            return val_loss\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    # net_roll_test: Test the e2e neural net\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def net_roll_test(\n",
    "        self,\n",
    "        X: TrainTest,\n",
    "        Y: TrainTest,\n",
    "        n_roll: int,\n",
    "        lr: float,\n",
    "        epochs: int,\n",
    "        load_state: list[bool] = [False, False, False, False],\n",
    "        save_state: list[bool] = [False, False, False, False],\n",
    "    ) -> None:\n",
    "        # Initialize backtest object\n",
    "        portfolio = BackTest(\n",
    "            len(Y.test()) - Y.number_of_observation_per_window,\n",
    "            self.num_assets,\n",
    "            Y.test().index[Y.number_of_observation_per_window :],\n",
    "        )\n",
    "\n",
    "        # Initialize lists to store trained parameters\n",
    "        self.gamma_trained = []\n",
    "        self.delta_trained = []\n",
    "\n",
    "        # Store initial split\n",
    "        init_split = Y.split_ratio\n",
    "\n",
    "        # Calculate window size\n",
    "        win_size = init_split[1] / n_roll\n",
    "\n",
    "        split = [0, 0]\n",
    "        t = 0\n",
    "        for i in range(n_roll):\n",
    "\n",
    "            print(f\"Out-of-sample window: {i+1} / {n_roll}\")\n",
    "\n",
    "            split[0] = init_split[0] + win_size * i\n",
    "            if i < n_roll - 1:\n",
    "                split[1] = win_size\n",
    "            else:\n",
    "                split[1] = 1 - split[0]\n",
    "\n",
    "            X.split_update(split)\n",
    "            Y.split_update(split)\n",
    "            train_set = DataLoader(\n",
    "                SlidingWindow(\n",
    "                    X.train(), Y.train(), self.num_observations, self.perf_period\n",
    "                )\n",
    "            )\n",
    "            test_set = DataLoader(\n",
    "                SlidingWindow(X.test(), Y.test(), self.num_observations, 0)\n",
    "            )\n",
    "            if load_state[i]:\n",
    "                # Reset model parameters to initial state\n",
    "                self.load_state_dict(\n",
    "                    torch.load(\n",
    "                        self.cache_path + self.model_name + \"_model_\" + str(i) + \".pt\",\n",
    "                        weights_only=True,\n",
    "                    )\n",
    "                )\n",
    "            # Train the model\n",
    "            self.train()\n",
    "            self.net_train(train_set, lr=lr, epochs=epochs)\n",
    "            # Save the trained model\n",
    "            if save_state[i]:\n",
    "                torch.save(\n",
    "                    self.state_dict(),\n",
    "                    self.cache_path + self.model_name + \"_model_\" + str(i) + \".pt\",\n",
    "                )\n",
    "            self.gamma_trained.append(self.gamma.item())\n",
    "            self.delta_trained.append(self.delta.item())\n",
    "            # Test the model\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                for __, (x, y, y_perf) in enumerate(test_set):\n",
    "                    # Move tensors to the same device as the model\n",
    "                    x = x.to(next(self.parameters()).device)\n",
    "                    y = y.to(next(self.parameters()).device)\n",
    "                    y_perf = y_perf.to(next(self.parameters()).device)\n",
    "\n",
    "                    z_star, _ = self(x.squeeze(), y.squeeze())\n",
    "                    if not np.isclose(torch.sum(z_star).cpu().numpy(), 1.0, atol=1e-2):\n",
    "                        print(z_star)\n",
    "\n",
    "                    portfolio.weights[t] = z_star.squeeze().cpu().numpy()\n",
    "                    portfolio.rets[t] = (\n",
    "                        y_perf.squeeze().cpu().numpy() @ portfolio.weights[t]\n",
    "                    ).item()\n",
    "                    t += 1\n",
    "\n",
    "        # Reset dataset splits\n",
    "        X.split_update(init_split)\n",
    "        Y.split_update(init_split)\n",
    "\n",
    "        # Calculate portfolio statistics\n",
    "        portfolio.stats()\n",
    "        self.portfolio = portfolio\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    # load_cv_results: Load cross-validation results\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def load_cv_results(self, cv_results):\n",
    "        self.cv_results = cv_results\n",
    "\n",
    "        # Select and store the optimal hyperparameters\n",
    "        idx = cv_results.val_loss.idxmin()\n",
    "        self.lr = cv_results.lr[idx]\n",
    "        self.epochs = cv_results.epochs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "Out-of-sample window: 1 / 4\n",
      "Epoch 1/60, Loss: -0.3460\n",
      "Epoch 2/60, Loss: -0.3029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhang\\anaconda3\\envs\\py12\\Lib\\site-packages\\diffcp\\cone_program.py:371: UserWarning: Solved/Inaccurate.\n",
      "  warnings.warn(\"Solved/Inaccurate.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/60, Loss: -0.3310\n",
      "Epoch 4/60, Loss: -0.3237\n",
      "Epoch 5/60, Loss: -0.3249\n",
      "Epoch 6/60, Loss: -0.3330\n",
      "Epoch 7/60, Loss: -0.3356\n",
      "Epoch 8/60, Loss: -0.3303\n",
      "Epoch 9/60, Loss: -0.3343\n",
      "Epoch 10/60, Loss: -0.3392\n",
      "Epoch 11/60, Loss: -0.3389\n",
      "Epoch 12/60, Loss: -0.3386\n",
      "Epoch 13/60, Loss: -0.3385\n",
      "Epoch 14/60, Loss: -0.3384\n",
      "Epoch 15/60, Loss: -0.3394\n",
      "Epoch 16/60, Loss: -0.3410\n",
      "Epoch 17/60, Loss: -0.3416\n",
      "Epoch 18/60, Loss: -0.3418\n",
      "Epoch 19/60, Loss: -0.3426\n",
      "Epoch 20/60, Loss: -0.3442\n",
      "Epoch 21/60, Loss: -0.3455\n",
      "Epoch 22/60, Loss: -0.3465\n",
      "Epoch 23/60, Loss: -0.3467\n",
      "Epoch 24/60, Loss: -0.3471\n",
      "Epoch 25/60, Loss: -0.3478\n",
      "Epoch 26/60, Loss: -0.3478\n",
      "Epoch 27/60, Loss: -0.3480\n",
      "Epoch 28/60, Loss: -0.3488\n",
      "Epoch 29/60, Loss: -0.3493\n",
      "Epoch 30/60, Loss: -0.3498\n",
      "Epoch 31/60, Loss: -0.3503\n",
      "Epoch 32/60, Loss: -0.3508\n",
      "Epoch 33/60, Loss: -0.3510\n",
      "Epoch 34/60, Loss: -0.3513\n",
      "Epoch 35/60, Loss: -0.3515\n",
      "Epoch 36/60, Loss: -0.3518\n",
      "Epoch 37/60, Loss: -0.3523\n",
      "Epoch 38/60, Loss: -0.3528\n",
      "Epoch 39/60, Loss: -0.3529\n",
      "Epoch 40/60, Loss: -0.3533\n",
      "Epoch 41/60, Loss: -0.3533\n",
      "Epoch 42/60, Loss: -0.3538\n",
      "Epoch 43/60, Loss: -0.3541\n",
      "Epoch 44/60, Loss: -0.3544\n",
      "Epoch 45/60, Loss: -0.3549\n",
      "Epoch 46/60, Loss: -0.3553\n",
      "Epoch 47/60, Loss: -0.3557\n",
      "Epoch 48/60, Loss: -0.3560\n",
      "Epoch 49/60, Loss: -0.3563\n",
      "Epoch 50/60, Loss: -0.3567\n",
      "Epoch 51/60, Loss: -0.3570\n",
      "Epoch 52/60, Loss: -0.3576\n",
      "Epoch 53/60, Loss: -0.3579\n",
      "Epoch 54/60, Loss: -0.3582\n",
      "Epoch 55/60, Loss: -0.3585\n",
      "Epoch 56/60, Loss: -0.3588\n",
      "Epoch 57/60, Loss: -0.3592\n",
      "Epoch 58/60, Loss: -0.3597\n",
      "Epoch 59/60, Loss: -0.3600\n",
      "Epoch 60/60, Loss: -0.3603\n",
      "Out-of-sample window: 2 / 4\n",
      "Epoch 1/60, Loss: -0.3557\n",
      "Epoch 2/60, Loss: -0.2482\n",
      "Epoch 3/60, Loss: -0.3279\n",
      "Epoch 4/60, Loss: -0.3133\n",
      "Epoch 5/60, Loss: -0.3063\n",
      "Epoch 6/60, Loss: -0.3155\n",
      "Epoch 7/60, Loss: -0.3149\n",
      "Epoch 8/60, Loss: -0.3189\n",
      "Epoch 9/60, Loss: -0.3236\n",
      "Epoch 10/60, Loss: -0.3316\n",
      "Epoch 11/60, Loss: -0.3345\n",
      "Epoch 12/60, Loss: -0.3291\n",
      "Epoch 13/60, Loss: -0.3303\n",
      "Epoch 14/60, Loss: -0.3372\n",
      "Epoch 15/60, Loss: -0.3425\n",
      "Epoch 16/60, Loss: -0.3426\n",
      "Epoch 17/60, Loss: -0.3412\n",
      "Epoch 18/60, Loss: -0.3416\n",
      "Epoch 19/60, Loss: -0.3441\n",
      "Epoch 20/60, Loss: -0.3461\n",
      "Epoch 21/60, Loss: -0.3462\n",
      "Epoch 22/60, Loss: -0.3464\n",
      "Epoch 23/60, Loss: -0.3490\n",
      "Epoch 24/60, Loss: -0.3512\n",
      "Epoch 25/60, Loss: -0.3500\n",
      "Epoch 26/60, Loss: -0.3498\n",
      "Epoch 27/60, Loss: -0.3521\n",
      "Epoch 28/60, Loss: -0.3528\n",
      "Epoch 29/60, Loss: -0.3519\n",
      "Epoch 30/60, Loss: -0.3536\n",
      "Epoch 31/60, Loss: -0.3547\n",
      "Epoch 32/60, Loss: -0.3540\n",
      "Epoch 33/60, Loss: -0.3552\n",
      "Epoch 34/60, Loss: -0.3566\n",
      "Epoch 35/60, Loss: -0.3563\n",
      "Epoch 36/60, Loss: -0.3568\n",
      "Epoch 37/60, Loss: -0.3573\n",
      "Epoch 38/60, Loss: -0.3581\n",
      "Epoch 39/60, Loss: -0.3589\n",
      "Epoch 40/60, Loss: -0.3593\n",
      "Epoch 41/60, Loss: -0.3596\n",
      "Epoch 42/60, Loss: -0.3602\n",
      "Epoch 43/60, Loss: -0.3602\n",
      "Epoch 44/60, Loss: -0.3610\n",
      "Epoch 45/60, Loss: -0.3606\n",
      "Epoch 46/60, Loss: -0.3614\n",
      "Epoch 47/60, Loss: -0.3619\n",
      "Epoch 48/60, Loss: -0.3624\n",
      "Epoch 49/60, Loss: -0.3624\n",
      "Epoch 50/60, Loss: -0.3627\n",
      "Epoch 51/60, Loss: -0.3631\n",
      "Epoch 52/60, Loss: -0.3630\n",
      "Epoch 53/60, Loss: -0.3636\n",
      "Epoch 54/60, Loss: -0.3636\n",
      "Epoch 55/60, Loss: -0.3634\n",
      "Epoch 56/60, Loss: -0.3636\n",
      "Epoch 57/60, Loss: -0.3637\n",
      "Epoch 58/60, Loss: -0.3642\n",
      "Epoch 59/60, Loss: -0.3647\n",
      "Epoch 60/60, Loss: -0.3645\n",
      "Out-of-sample window: 3 / 4\n",
      "Epoch 1/60, Loss: -0.3636\n",
      "Epoch 2/60, Loss: -0.3068\n",
      "Epoch 3/60, Loss: -0.3312\n",
      "Epoch 4/60, Loss: -0.3348\n",
      "Epoch 5/60, Loss: -0.3303\n",
      "Epoch 6/60, Loss: -0.3380\n",
      "Epoch 7/60, Loss: -0.3458\n",
      "Epoch 8/60, Loss: -0.3483\n",
      "Epoch 9/60, Loss: -0.3412\n",
      "Epoch 10/60, Loss: -0.3424\n",
      "Epoch 11/60, Loss: -0.3485\n",
      "Epoch 12/60, Loss: -0.3501\n",
      "Epoch 13/60, Loss: -0.3505\n",
      "Epoch 14/60, Loss: -0.3529\n",
      "Epoch 15/60, Loss: -0.3527\n",
      "Epoch 16/60, Loss: -0.3522\n",
      "Epoch 17/60, Loss: -0.3542\n",
      "Epoch 18/60, Loss: -0.3557\n",
      "Epoch 19/60, Loss: -0.3557\n",
      "Epoch 20/60, Loss: -0.3574\n",
      "Epoch 21/60, Loss: -0.3583\n",
      "Epoch 22/60, Loss: -0.3584\n",
      "Epoch 23/60, Loss: -0.3591\n",
      "Epoch 24/60, Loss: -0.3582\n",
      "Epoch 25/60, Loss: -0.3596\n",
      "Epoch 26/60, Loss: -0.3604\n",
      "Epoch 27/60, Loss: -0.3608\n",
      "Epoch 28/60, Loss: -0.3622\n",
      "Epoch 29/60, Loss: -0.3618\n",
      "Epoch 30/60, Loss: -0.3618\n",
      "Epoch 31/60, Loss: -0.3626\n",
      "Epoch 32/60, Loss: -0.3632\n",
      "Epoch 33/60, Loss: -0.3638\n",
      "Epoch 34/60, Loss: -0.3644\n",
      "Epoch 35/60, Loss: -0.3646\n",
      "Epoch 36/60, Loss: -0.3642\n",
      "Epoch 37/60, Loss: -0.3647\n",
      "Epoch 38/60, Loss: -0.3650\n",
      "Epoch 39/60, Loss: -0.3651\n",
      "Epoch 40/60, Loss: -0.3650\n",
      "Epoch 41/60, Loss: -0.3648\n",
      "Epoch 42/60, Loss: -0.3656\n",
      "Epoch 43/60, Loss: -0.3661\n",
      "Epoch 44/60, Loss: -0.3667\n",
      "Epoch 45/60, Loss: -0.3668\n",
      "Epoch 46/60, Loss: -0.3673\n",
      "Epoch 47/60, Loss: -0.3673\n",
      "Epoch 48/60, Loss: -0.3674\n",
      "Epoch 49/60, Loss: -0.3678\n",
      "Epoch 50/60, Loss: -0.3683\n",
      "Epoch 51/60, Loss: -0.3679\n",
      "Epoch 52/60, Loss: -0.3683\n",
      "Epoch 53/60, Loss: -0.3686\n",
      "Epoch 54/60, Loss: -0.3687\n",
      "Epoch 55/60, Loss: -0.3687\n",
      "Epoch 56/60, Loss: -0.3688\n",
      "Epoch 57/60, Loss: -0.3693\n",
      "Epoch 58/60, Loss: -0.3695\n",
      "Epoch 59/60, Loss: -0.3698\n",
      "Epoch 60/60, Loss: -0.3700\n",
      "Out-of-sample window: 4 / 4\n",
      "Epoch 1/60, Loss: -0.3734\n",
      "Epoch 2/60, Loss: -0.3025\n",
      "Epoch 3/60, Loss: -0.3144\n",
      "Epoch 4/60, Loss: -0.3220\n",
      "Epoch 5/60, Loss: -0.3234\n",
      "Epoch 6/60, Loss: -0.3373\n",
      "Epoch 7/60, Loss: -0.3349\n",
      "Epoch 8/60, Loss: -0.3437\n",
      "Epoch 9/60, Loss: -0.3389\n",
      "Epoch 10/60, Loss: -0.3466\n",
      "Epoch 11/60, Loss: -0.3501\n",
      "Epoch 12/60, Loss: -0.3486\n",
      "Epoch 13/60, Loss: -0.3534\n",
      "Epoch 14/60, Loss: -0.3529\n",
      "Epoch 15/60, Loss: -0.3520\n",
      "Epoch 16/60, Loss: -0.3554\n",
      "Epoch 17/60, Loss: -0.3557\n",
      "Epoch 18/60, Loss: -0.3592\n",
      "Epoch 19/60, Loss: -0.3610\n",
      "Epoch 20/60, Loss: -0.3614\n",
      "Epoch 21/60, Loss: -0.3617\n",
      "Epoch 22/60, Loss: -0.3625\n",
      "Epoch 23/60, Loss: -0.3635\n",
      "Epoch 24/60, Loss: -0.3640\n",
      "Epoch 25/60, Loss: -0.3645\n",
      "Epoch 26/60, Loss: -0.3649\n",
      "Epoch 27/60, Loss: -0.3648\n",
      "Epoch 28/60, Loss: -0.3659\n",
      "Epoch 29/60, Loss: -0.3667\n",
      "Epoch 30/60, Loss: -0.3676\n",
      "Epoch 31/60, Loss: -0.3666\n",
      "Epoch 32/60, Loss: -0.3685\n",
      "Epoch 33/60, Loss: -0.3677\n",
      "Epoch 34/60, Loss: -0.3686\n",
      "Epoch 35/60, Loss: -0.3691\n",
      "Epoch 36/60, Loss: -0.3695\n",
      "Epoch 37/60, Loss: -0.3703\n",
      "Epoch 38/60, Loss: -0.3708\n",
      "Epoch 39/60, Loss: -0.3711\n",
      "Epoch 40/60, Loss: -0.3714\n",
      "Epoch 41/60, Loss: -0.3716\n",
      "Epoch 42/60, Loss: -0.3709\n",
      "Epoch 43/60, Loss: -0.3721\n",
      "Epoch 44/60, Loss: -0.3717\n",
      "Epoch 45/60, Loss: -0.3716\n",
      "Epoch 46/60, Loss: -0.3713\n",
      "Epoch 47/60, Loss: -0.3706\n",
      "Epoch 48/60, Loss: -0.3697\n",
      "Epoch 49/60, Loss: -0.3703\n",
      "Epoch 50/60, Loss: -0.3721\n",
      "Epoch 51/60, Loss: -0.3712\n",
      "Epoch 52/60, Loss: -0.3719\n",
      "Epoch 53/60, Loss: -0.3721\n",
      "Epoch 54/60, Loss: -0.3700\n",
      "Epoch 55/60, Loss: -0.3697\n",
      "Epoch 56/60, Loss: -0.3704\n",
      "Epoch 57/60, Loss: -0.3703\n",
      "Epoch 58/60, Loss: -0.3704\n",
      "Epoch 59/60, Loss: -0.3735\n",
      "Epoch 60/60, Loss: -0.3701\n"
     ]
    }
   ],
   "source": [
    "n_roll: int = 4  # Number of rolling windows\n",
    "dr_net_eps_0 = E2E_net_Eps_Control(\n",
    "    num_input_features=n_X,\n",
    "    num_assets=n_Y,\n",
    "    num_observations=number_of_observe_per_window,\n",
    "    prisk=\"p_var\",\n",
    "    train_pred=True,\n",
    "    train_gamma=True,\n",
    "    train_delta=True,\n",
    "    set_seed=19260817,\n",
    "    optimization_layer=\"hellinger\",\n",
    "    performance_objective=\"sharpe_loss\",\n",
    "    cache_path=\"./cache/\",\n",
    "    performance_period=13,\n",
    "    prediction_loss_factor=0.5,\n",
    "    self_overall_std_dev_factor=0,\n",
    "    model_name=\"E2E_net_Eps_Control_0\",\n",
    ").double()\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "dr_net_eps_0.to(device)\n",
    "dr_net_eps_0.net_roll_test(\n",
    "    X_data,\n",
    "    Y_data,\n",
    "    n_roll=n_roll,\n",
    "    lr=0.005,\n",
    "    epochs=60,\n",
    "    load_state=[True] * (n_roll),\n",
    "    save_state=[True] * (n_roll),\n",
    ")\n",
    "portfolio_0 = dr_net_eps_0.portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe Ratio:  0.160146061355976\n",
      "Mean Return:  0.0032563818638320363\n",
      "Volatility:  0.020333824236823925\n",
      "Annualized Sharpe Ratio:  1.1548296715651472\n"
     ]
    }
   ],
   "source": [
    "print(\"Sharpe Ratio: \", portfolio_0.sharpe)\n",
    "print(\"Mean Return: \", portfolio_0.mean)\n",
    "print(\"Volatility: \", portfolio_0.vol)\n",
    "print(\"Annualized Sharpe Ratio: \", portfolio_0.sharpe * np.sqrt(52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "portfolio_0.plot_cumulative_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_50356\\4089635158.py:90: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  factor_5 = pdr.get_data_famafrench(\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_50356\\4089635158.py:99: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  mom_df = pdr.get_data_famafrench(\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_50356\\4089635158.py:106: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  st_df = pdr.get_data_famafrench(\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_50356\\4089635158.py:113: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  lt_df = pdr.get_data_famafrench(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6868, 8) (6868, 20)\n",
      "(6180, 8) (6180, 20) (791, 8) (791, 20)\n",
      "            Mkt-RF     SMB     HML     RMW     CMA  Mom     ST_Rev  LT_Rev\n",
      "Date                                                                      \n",
      "1997-05-16 -0.0113  0.0108  0.0037 -0.0044  0.0047 -0.0036  0.0113  0.0069\n",
      "1997-05-19  0.0027 -0.0004 -0.0028 -0.0001  0.0006  0.0024  0.0016  0.0000\n",
      "1997-05-20  0.0091 -0.0055 -0.0021  0.0030 -0.0079 -0.0001 -0.0050 -0.0078\n",
      "1997-05-21 -0.0013  0.0073 -0.0073 -0.0013 -0.0006 -0.0056  0.0022  0.0010\n",
      "1997-05-22 -0.0027  0.0064  0.0041 -0.0008  0.0008 -0.0011  0.0029  0.0005             Mkt-RF     SMB     HML     RMW     CMA  Mom     ST_Rev  LT_Rev\n",
      "Date                                                                      \n",
      "2024-08-23  0.0129  0.0190  0.0085 -0.0048  0.0068  0.0013  0.0078 -0.0012\n",
      "2024-08-26 -0.0034  0.0033  0.0016  0.0013 -0.0006 -0.0045  0.0020  0.0059\n",
      "2024-08-27  0.0006 -0.0090  0.0002  0.0027  0.0023  0.0053 -0.0080 -0.0016\n",
      "2024-08-28 -0.0067 -0.0022  0.0114  0.0055 -0.0016  0.0030  0.0029  0.0068\n",
      "2024-08-29  0.0008  0.0067  0.0028 -0.0015 -0.0122 -0.0079  0.0067  0.0021\n",
      "Ticker          MSFT      COST      AMZN       BAC         C       HAL  \\\n",
      "Date                                                                     \n",
      "1997-05-19 -0.002707  0.038911 -0.012040 -0.016563  0.009050 -0.009966   \n",
      "1997-05-20  0.034745  0.029962 -0.042685  0.012631  0.033632  0.006711   \n",
      "1997-05-21  0.010494 -0.040000 -0.127392 -0.027027 -0.030369  0.008333   \n",
      "1997-05-22  0.002077 -0.020833 -0.021891  0.000000 -0.006712  0.001652   \n",
      "1997-05-23  0.018653 -0.013540  0.074622  0.008547  0.018018  0.000000   \n",
      "\n",
      "Ticker            ED       LMT         T       WMT       MCD       NEM  \\\n",
      "Date                                                                     \n",
      "1997-05-19  0.012931  0.016644  0.002180  0.020920  0.002404  0.016835   \n",
      "1997-05-20  0.000000  0.013643  0.006521  0.000000 -0.016786  0.003311   \n",
      "1997-05-21 -0.008510  0.008076 -0.017278 -0.016393 -0.007317 -0.003300   \n",
      "1997-05-22 -0.017168  0.005340 -0.017583 -0.008333  0.000000  0.000000   \n",
      "1997-05-23  0.013101 -0.010623  0.017897  0.012605  0.004914  0.013245   \n",
      "\n",
      "Ticker            VZ       JPM       CAT      AAPL       DIS       JNJ  \\\n",
      "Date                                                                     \n",
      "1997-05-19  0.000000 -0.010680 -0.005076 -0.014492  0.029366  0.016597   \n",
      "1997-05-20  0.000000  0.041835 -0.010203  0.014705  0.003003 -0.006122   \n",
      "1997-05-21 -0.001815 -0.034974 -0.002578 -0.021738 -0.016467 -0.024640   \n",
      "1997-05-22 -0.014545  0.016107  0.007752 -0.014813  0.000000  0.002105   \n",
      "1997-05-23  0.016605  0.005284  0.000000  0.015036  0.013699  0.006303   \n",
      "\n",
      "Ticker           XOM       PFE  \n",
      "Date                            \n",
      "1997-05-19  0.017204 -0.003722  \n",
      "1997-05-20  0.000000  0.004981  \n",
      "1997-05-21  0.006343 -0.006196  \n",
      "1997-05-22 -0.016807 -0.008728  \n",
      "1997-05-23  0.023505  0.011321   Ticker          MSFT      COST      AMZN       BAC         C       HAL  \\\n",
      "Date                                                                     \n",
      "2024-08-26 -0.007918  0.015127 -0.008699  0.003772 -0.005632  0.003463   \n",
      "2024-08-27  0.000846  0.018364 -0.013561 -0.006263 -0.001780 -0.007844   \n",
      "2024-08-28 -0.007829 -0.022940 -0.013401  0.007058 -0.001621 -0.019608   \n",
      "2024-08-29  0.006137 -0.001599  0.007728  0.005507  0.004872  0.012258   \n",
      "2024-08-30  0.009731  0.006485  0.037067  0.014439  0.012282 -0.009242   \n",
      "\n",
      "Ticker            ED       LMT         T       WMT       MCD       NEM  \\\n",
      "Date                                                                     \n",
      "2024-08-26  0.002982  0.005567  0.001521  0.004359 -0.002901  0.004227   \n",
      "2024-08-27 -0.012983  0.004139 -0.005567  0.001315  0.003326  0.008419   \n",
      "2024-08-28  0.008033  0.006370  0.008651 -0.000657 -0.008942 -0.016509   \n",
      "2024-08-29  0.004184  0.005089 -0.003027  0.004469  0.002822  0.026047   \n",
      "2024-08-30  0.007440  0.002205  0.007085  0.010599  0.002779  0.003949   \n",
      "\n",
      "Ticker            VZ       JPM       CAT      AAPL       DIS       JNJ  \\\n",
      "Date                                                                     \n",
      "2024-08-26  0.006795  0.003939  0.007893  0.001499  0.013472  0.002924   \n",
      "2024-08-27 -0.000964  0.004608 -0.000114  0.003742 -0.009588 -0.002571   \n",
      "2024-08-28  0.000965  0.005041 -0.008316 -0.006753 -0.015512  0.005953   \n",
      "2024-08-29 -0.005785  0.004158  0.009879  0.014570  0.003576  0.001891   \n",
      "2024-08-30  0.012849  0.011656  0.012683 -0.003438  0.006347  0.009925   \n",
      "\n",
      "Ticker           XOM       PFE  \n",
      "Date                            \n",
      "2024-08-26  0.021406  0.000692  \n",
      "2024-08-27 -0.009511 -0.003458  \n",
      "2024-08-28 -0.009857 -0.002429  \n",
      "2024-08-29  0.013817 -0.001044  \n",
      "2024-08-30 -0.001608  0.010098  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from pandas_datareader import data as pdr\n",
    "\n",
    "# Assuming TrainTest is defined elsewhere in your codebase\n",
    "# from your_module import TrainTest\n",
    "\n",
    "\n",
    "def AV_yFinance(\n",
    "    start: str,\n",
    "    end: str,\n",
    "    split: List[float],\n",
    "    freq: str = \"weekly\",\n",
    "    n_obs: int = 104,\n",
    "    n_y: Optional[int] = None,\n",
    "    use_cache: bool = False,\n",
    "    save_results: bool = False,\n",
    ") -> Tuple[TrainTest, TrainTest]:\n",
    "\n",
    "    if use_cache:\n",
    "        X = pd.read_pickle(f\"./cache/factor_{freq}.pkl\")\n",
    "        Y = pd.read_pickle(f\"./cache/asset_{freq}.pkl\")\n",
    "    else:\n",
    "        # Define the list of tickers\n",
    "        tick_list = [\n",
    "            \"AAPL\",\n",
    "            \"MSFT\",\n",
    "            \"AMZN\",\n",
    "            \"C\",\n",
    "            \"JPM\",\n",
    "            \"BAC\",\n",
    "            \"XOM\",\n",
    "            \"HAL\",\n",
    "            \"MCD\",\n",
    "            \"WMT\",\n",
    "            \"COST\",\n",
    "            \"CAT\",\n",
    "            \"LMT\",\n",
    "            \"JNJ\",\n",
    "            \"PFE\",\n",
    "            \"DIS\",\n",
    "            \"VZ\",\n",
    "            \"T\",\n",
    "            \"ED\",\n",
    "            \"NEM\",\n",
    "        ]\n",
    "\n",
    "        if n_y is not None:\n",
    "            tick_list = tick_list[:n_y]\n",
    "\n",
    "        # Download asset data using yfinance\n",
    "        data = yf.download(\n",
    "            tick_list,\n",
    "            start=start,\n",
    "            end=end,\n",
    "            progress=False,\n",
    "            group_by=\"ticker\",\n",
    "            auto_adjust=True,  # Adjusted close prices\n",
    "            threads=True,  # Enable multi-threading for faster downloads\n",
    "        )\n",
    "\n",
    "        if data.empty:\n",
    "            raise ValueError(\n",
    "                \"No data downloaded. Please check the ticker symbols and date range.\"\n",
    "            )\n",
    "\n",
    "        # Extract Adjusted Close prices\n",
    "        if len(tick_list) == 1:\n",
    "            # For single ticker, data['Close'] is a Series, convert to DataFrame\n",
    "            adj_close = data[\"Close\"].to_frame()\n",
    "            adj_close.columns = tick_list\n",
    "        else:\n",
    "            # For multiple tickers, use xs to extract 'Close' for all tickers\n",
    "            try:\n",
    "                adj_close = data.xs(\"Close\", level=1, axis=1)\n",
    "            except KeyError:\n",
    "                raise KeyError(\"Close prices not found in the downloaded data.\")\n",
    "\n",
    "        # Compute daily returns as percentage change\n",
    "        Y = adj_close.pct_change().dropna()\n",
    "\n",
    "        # Download factor data from Kenneth French's data library\n",
    "        dl_freq = \"_daily\"\n",
    "\n",
    "        try:\n",
    "            # 5-Factor Model\n",
    "            factor_5 = pdr.get_data_famafrench(\n",
    "                \"F-F_Research_Data_5_Factors_2x3\" + dl_freq,\n",
    "                start=start,\n",
    "                end=end,\n",
    "            )[0]\n",
    "            rf_df = factor_5[\"RF\"]\n",
    "            factor_5 = factor_5.drop([\"RF\"], axis=1)\n",
    "\n",
    "            # Momentum Factor\n",
    "            mom_df = pdr.get_data_famafrench(\n",
    "                \"F-F_Momentum_Factor\" + dl_freq,\n",
    "                start=start,\n",
    "                end=end,\n",
    "            )[0]\n",
    "\n",
    "            # Short-Term Reversal Factor\n",
    "            st_df = pdr.get_data_famafrench(\n",
    "                \"F-F_ST_Reversal_Factor\" + dl_freq,\n",
    "                start=start,\n",
    "                end=end,\n",
    "            )[0]\n",
    "\n",
    "            # Long-Term Reversal Factor\n",
    "            lt_df = pdr.get_data_famafrench(\n",
    "                \"F-F_LT_Reversal_Factor\" + dl_freq,\n",
    "                start=start,\n",
    "                end=end,\n",
    "            )[0]\n",
    "\n",
    "            # Concatenate all factors and convert to decimal\n",
    "            X = pd.concat([factor_5, mom_df, st_df, lt_df], axis=1) / 100\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to download factor data: {e}\")\n",
    "\n",
    "        # Align factor data (X) with asset returns (Y) based on dates\n",
    "\n",
    "        # Remove timezone from Y.index if present\n",
    "        if Y.index.tz is not None:\n",
    "            Y.index = Y.index.tz_convert(None)\n",
    "\n",
    "        # Ensure X.index is also timezone-naive\n",
    "        if X.index.tz is not None:\n",
    "            X.index = X.index.tz_convert(None)\n",
    "\n",
    "        # Now, perform the alignment\n",
    "        try:\n",
    "            X = X.loc[Y.index]\n",
    "        except KeyError as e:\n",
    "            missing_dates = Y.index.difference(X.index)\n",
    "            if not missing_dates.empty:\n",
    "                print(f\"Missing dates in factor data: {missing_dates}\")\n",
    "                # Optionally, you can drop missing dates or handle them differently\n",
    "                Y = Y.loc[Y.index.intersection(X.index)]\n",
    "                X = X.loc[X.index.intersection(Y.index)]\n",
    "            else:\n",
    "                raise e  # Re-raise if no missing dates found\n",
    "\n",
    "        # Resample data if frequency is not daily\n",
    "        freq_lower = freq.lower()\n",
    "        if freq_lower in [\"weekly\", \"wk\", \"1wk\"]:\n",
    "            Y = Y.resample(\"W-FRI\").apply(lambda x: (x + 1).prod() - 1)\n",
    "            X = X.resample(\"W-FRI\").apply(lambda x: (x + 1).prod() - 1)\n",
    "        elif freq_lower in [\"monthly\", \"1mo\"]:\n",
    "            Y = Y.resample(\"M\").apply(lambda x: (x + 1).prod() - 1)\n",
    "            X = X.resample(\"M\").apply(lambda x: (x + 1).prod() - 1)\n",
    "        # Add more resampling frequencies if needed\n",
    "\n",
    "        # Handle missing values by forward and backward filling using ffill() and bfill()\n",
    "        Y = Y.ffill().bfill()\n",
    "        X = X.ffill().bfill()\n",
    "\n",
    "        # Convert the index to 'YYYY-MM-DD' format\n",
    "        X.index = X.index.strftime(\"%Y-%m-%d\")\n",
    "        X.index = pd.DatetimeIndex(X.index)\n",
    "        Y.index = Y.index.strftime(\"%Y-%m-%d\")\n",
    "        X.index = pd.DatetimeIndex(Y.index)\n",
    "\n",
    "        # Optionally save the results to cache\n",
    "        if save_results:\n",
    "            os.makedirs(\"./cache\", exist_ok=True)\n",
    "            X.to_pickle(f\"./cache/factor_{freq}.pkl\")\n",
    "            Y.to_pickle(f\"./cache/asset_{freq}.pkl\")\n",
    "    print(X.shape, Y.shape)\n",
    "    # Partition dataset into training and testing sets. Lag the data by one observation\n",
    "    # Using the provided TrainTest class\n",
    "    X_train_test = TrainTest(X[:-1], n_obs, split)\n",
    "    Y_train_test = TrainTest(Y[1:], n_obs, split)\n",
    "\n",
    "    return X_train_test, Y_train_test\n",
    "\n",
    "\n",
    "start_paddling = \"1997-04-01\"\n",
    "end_paddling = \"2024-09-01\"  # Data frequency and start/end dates\n",
    "daily_frequency = \"daily\"\n",
    "xf_train_test, yf_train_test = AV_yFinance(\n",
    "    start=start_paddling,\n",
    "    end=end_paddling,\n",
    "    split=[0.9, 0.1],\n",
    "    freq=daily_frequency,\n",
    "    n_obs=104,\n",
    "    n_y=20,\n",
    "    use_cache=False,\n",
    "    save_results=True,\n",
    ")\n",
    "print(\n",
    "    xf_train_test.train().shape,\n",
    "    yf_train_test.train().shape,\n",
    "    xf_train_test.test().shape,\n",
    "    yf_train_test.test().shape,\n",
    ")\n",
    "print(xf_train_test.train().head(), xf_train_test.test().tail())\n",
    "print(yf_train_test.train().head(), yf_train_test.test().tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
