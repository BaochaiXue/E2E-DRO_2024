{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing p_var...\n",
      "Optimized portfolio weights (Variance): [0.33333333 0.33333333 0.33333333]\n",
      "Variance objective value: 0.0\n",
      "\n",
      "Testing p_mad...\n",
      "Optimized portfolio weights (MAD): [ 0.13355262  0.33328474 -0.66566742]\n",
      "MAD objective value: 3.469446951953614e-18\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "# Risk functions module\n",
    "#\n",
    "# This module defines the financial risk measures to be used in the optimization layer of the E2E\n",
    "# problem.\n",
    "#\n",
    "\n",
    "\n",
    "def p_var(z: cp.Expression, c: float, x: np.ndarray) -> cp.Expression:\n",
    "    \"\"\"\n",
    "    Compute the squared error for the given input.\n",
    "\n",
    "    :param z: A cvxpy expression (decision variable)\n",
    "    :param c: A constant threshold or target value\n",
    "    :param x: A numpy array (weights or features)\n",
    "    :return: The squared error expression\n",
    "    \"\"\"\n",
    "    return cp.square(x @ z - c)\n",
    "\n",
    "\n",
    "def p_mad(z: cp.Expression, c: float, x: np.ndarray) -> cp.Expression:\n",
    "    \"\"\"\n",
    "    Compute the mean absolute deviation for the given input.\n",
    "\n",
    "    :param z: A cvxpy expression (decision variable)\n",
    "    :param c: A constant threshold or target value\n",
    "    :param x: A numpy array (weights or features)\n",
    "    :return: The absolute deviation expression\n",
    "    \"\"\"\n",
    "    return cp.abs(x @ z - c)\n",
    "\n",
    "\n",
    "# Define test data\n",
    "z = cp.Variable(3)  # Decision variable (portfolio weights)\n",
    "c = 0.02  # Centering parameter (expected return)\n",
    "x = np.array(\n",
    "    [\n",
    "        [0.05, 0.02, -0.01],  # Realized returns for multiple scenarios\n",
    "        [0.03, -0.01, 0.04],\n",
    "        [-0.02, 0.01, 0.01],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Test variance function (p_var)\n",
    "print(\"\\nTesting p_var...\")\n",
    "var_expr = p_var(z, c, x[0])  # Apply p_var to the first row of x\n",
    "objective_var = cp.Minimize(var_expr)  # Minimize the variance\n",
    "constraints = [\n",
    "    cp.sum(z) == 1,\n",
    "    z >= 0,\n",
    "]  # Portfolio constraints: sum of weights = 1, weights >= 0\n",
    "problem_var = cp.Problem(objective_var, constraints)\n",
    "var_opt_value = problem_var.solve()\n",
    "\n",
    "# Output results for variance minimization\n",
    "print(\"Optimized portfolio weights (Variance):\", z.value)\n",
    "print(\"Variance objective value:\", var_opt_value)\n",
    "\n",
    "# Reinitialize decision variable for MAD problem\n",
    "z = cp.Variable(3)\n",
    "\n",
    "# Test MAD function (p_mad)\n",
    "print(\"\\nTesting p_mad...\")\n",
    "mad_expr = p_mad(z, c, x[0])  # Apply p_mad to the first row of x\n",
    "objective_mad = cp.Minimize(mad_expr)  # Minimize the MAD\n",
    "problem_mad = cp.Problem(objective_mad, constraints)\n",
    "mad_opt_value = problem_mad.solve()\n",
    "\n",
    "# Output results for MAD minimization\n",
    "print(\"Optimized portfolio weights (MAD):\", z.value)\n",
    "print(\"MAD objective value:\", mad_opt_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing single_period_loss...\n",
      "Single period loss: -0.010999999940395355\n",
      "\n",
      "Testing single_period_over_var_loss...\n",
      "Single period loss over volatility: -10.989008903503418\n",
      "\n",
      "Testing sharpe_loss...\n",
      "Sharpe ratio loss: -11.988011360168457\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "# Performance loss functions with type hints and improved comments\n",
    "def single_period_loss(z_star: Tensor, y_perf: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the single-period loss based on the out-of-sample portfolio return.\n",
    "\n",
    "    This function computes the out-of-sample portfolio return for a given portfolio over the next\n",
    "    time step. It computes the loss as the negative return since optimization typically focuses\n",
    "    on minimizing the loss, and maximizing returns translates into minimizing negative returns.\n",
    "\n",
    "    :param z_star: Tensor of shape (n_y, 1) representing the optimal portfolio weights.\n",
    "    :param y_perf: Tensor of shape (perf_period, n_y) representing the realized returns.\n",
    "    :return: A scalar tensor representing the realized return at the first time step (negative).\n",
    "    \"\"\"\n",
    "    # Calculate the portfolio return for the first time step and negate it (since we want to minimize loss)\n",
    "    return -y_perf[0] @ z_star\n",
    "\n",
    "\n",
    "def single_period_over_var_loss(z_star: Tensor, y_perf: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the loss as the portfolio return divided by the portfolio's volatility.\n",
    "\n",
    "    This function computes the portfolio return at the first time step and divides it by the\n",
    "    realized volatility (standard deviation) of the portfolio returns over the performance period.\n",
    "    This provides a return-over-risk measure, which is often used in portfolio analysis.\n",
    "\n",
    "    :param z_star: Tensor of shape (n_y, 1) representing the optimal portfolio weights.\n",
    "    :param y_perf: Tensor of shape (perf_period, n_y) representing the realized returns.\n",
    "    :return: A scalar tensor representing the return over realized volatility (negative).\n",
    "    \"\"\"\n",
    "    # Calculate the portfolio returns over the entire performance period\n",
    "    portfolio_returns = y_perf @ z_star\n",
    "    # Calculate the standard deviation (volatility) of the portfolio returns, adding epsilon for numerical stability\n",
    "    volatility = torch.std(portfolio_returns, unbiased=True) + 1e-6\n",
    "    # Calculate the return at the first time step and divide by the volatility, then negate for loss\n",
    "    return -portfolio_returns[0] / volatility\n",
    "\n",
    "\n",
    "def sharpe_loss(z_star: Tensor, y_perf: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the loss based on the Sharpe ratio over a performance period.\n",
    "\n",
    "    This function computes a simplified Sharpe ratio, which is the ratio of the mean portfolio\n",
    "    return to its standard deviation (volatility) over the performance period. The loss is defined\n",
    "    as the negative Sharpe ratio to allow for minimization.\n",
    "\n",
    "    :param z_star: Tensor of shape (n_y, 1) representing the optimal portfolio weights.\n",
    "    :param y_perf: Tensor of shape (perf_period, n_y) representing the realized returns.\n",
    "    :return: A scalar tensor representing the negative Sharpe ratio.\n",
    "    \"\"\"\n",
    "    # Calculate the portfolio returns over the entire performance period\n",
    "    portfolio_returns = y_perf @ z_star\n",
    "    # Calculate the mean return of the portfolio\n",
    "    mean_return = torch.mean(portfolio_returns)\n",
    "    # Calculate the standard deviation (volatility) of the portfolio returns, adding epsilon for numerical stability\n",
    "    volatility = torch.std(portfolio_returns, unbiased=True) + 1e-6\n",
    "    # Calculate the Sharpe ratio and negate it for loss\n",
    "    return -mean_return / volatility\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example portfolio weights (3 assets)\n",
    "    z_star = torch.tensor([0.3, 0.5, 0.2])\n",
    "    # Realized returns for 3 assets over 3 periods\n",
    "    y_perf = torch.tensor(\n",
    "        [\n",
    "            [0.01, 0.02, -0.01],\n",
    "            [0.03, -0.01, 0.04],\n",
    "            [0.02, 0.01, 0.01],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Test the single-period loss function\n",
    "    print(\"Testing single_period_loss...\")\n",
    "    loss_sp = single_period_loss(z_star, y_perf)\n",
    "    print(f\"Single period loss: {loss_sp.item()}\")\n",
    "\n",
    "    # Test the single-period-over-volatility loss function\n",
    "    print(\"\\nTesting single_period_over_var_loss...\")\n",
    "    loss_sp_var = single_period_over_var_loss(z_star, y_perf)\n",
    "    print(f\"Single period loss over volatility: {loss_sp_var.item()}\")\n",
    "\n",
    "    # Test the Sharpe ratio loss function\n",
    "    print(\"\\nTesting sharpe_loss...\")\n",
    "    loss_sharpe = sharpe_loss(z_star, y_perf)\n",
    "    print(f\"Sharpe ratio loss: {loss_sharpe.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SlidingWindow...\n",
      "x (features): torch.Size([11, 3])\n",
      "y (realizations): torch.Size([10, 2])\n",
      "y_perf (performance window): torch.Size([5, 2])\n",
      "\n",
      "Testing Backtest...\n",
      "                rets       tri\n",
      "Date                          \n",
      "2020-03-11 -1.961243 -0.961243\n",
      "2020-03-12 -0.652503 -0.334029\n",
      "2020-03-13 -1.149731  0.050015\n",
      "2020-03-14  0.026874  0.051359\n",
      "2020-03-15 -0.958301  0.002142\n",
      "Mean return: nan\n",
      "Volatility: 0.8065\n",
      "Sharpe ratio: nan\n",
      "\n",
      "Testing Backtest stats calculation...\n",
      "            rets       tri\n",
      "Date                      \n",
      "2020-04-05  0.05  1.050000\n",
      "2020-04-06 -0.02  1.029000\n",
      "2020-04-07  0.03  1.059870\n",
      "2020-04-08  0.04  1.102265\n",
      "2020-04-09 -0.01  1.091242\n",
      "Mean return: 0.0176\n",
      "Volatility: 0.0279\n",
      "Sharpe ratio: 0.6324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23865\\AppData\\Local\\Temp\\ipykernel_4164\\2887798928.py:115: RuntimeWarning: invalid value encountered in scalar power\n",
      "  self.mean = (tri[-1]) ** (1 / len(tri)) - 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Tuple\n",
    "\n",
    "####################################################################################################\n",
    "# SlidingWindow Dataset to index data using a sliding window\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "class SlidingWindow(Dataset):\n",
    "    \"\"\"Dataset class for creating a sliding window from time series data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        Y: pd.DataFrame,\n",
    "        n_obs: int,\n",
    "        perf_period: int,\n",
    "        dtype: torch.dtype = torch.float32,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the SlidingWindow dataset.\n",
    "\n",
    "        :param X: DataFrame containing the complete feature dataset.\n",
    "        :param Y: DataFrame containing the complete asset return dataset.\n",
    "        :param n_obs: Number of observations in the sliding window.\n",
    "        :param perf_period: Number of future observations used for out-of-sample performance evaluation.\n",
    "        :param dtype: The desired data type for tensors (default is torch.float32).\n",
    "        :param device: Device on which to place the tensors (e.g., 'cpu' or 'cuda' for GPU).\n",
    "        \"\"\"\n",
    "        self.X = torch.tensor(\n",
    "            X.values, dtype=dtype, device=device\n",
    "        )  # Convert feature dataset to tensor\n",
    "        self.Y = torch.tensor(\n",
    "            Y.values, dtype=dtype, device=device\n",
    "        )  # Convert asset return dataset to tensor\n",
    "        self.n_obs = n_obs  # Number of observations in the sliding window\n",
    "        self.perf_period = (\n",
    "            perf_period  # Number of future observations for performance evaluation\n",
    "        )\n",
    "\n",
    "    def __getitem__(\n",
    "        self, index: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Retrieve a single window of data.\n",
    "\n",
    "        :param index: Index of the sliding window.\n",
    "        :return: Tuple (x, y, y_perf):\n",
    "            x: Features window of shape (n_obs + 1, n_x).\n",
    "            y: Realizations window of shape (n_obs, n_y).\n",
    "            y_perf: Future performance window of shape (perf_period, n_y).\n",
    "        \"\"\"\n",
    "        # Retrieve features for the sliding window (n_obs + 1 observations)\n",
    "        x = self.X[index : index + self.n_obs + 1]\n",
    "        # Retrieve asset returns for the sliding window (n_obs observations)\n",
    "        y = self.Y[index : index + self.n_obs]\n",
    "        # Retrieve future performance data (perf_period observations)\n",
    "        y_perf = self.Y[index + self.n_obs : index + self.n_obs + self.perf_period]\n",
    "        return (x, y, y_perf)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the number of windows that can be created from the dataset.\n",
    "\n",
    "        :return: Length of the dataset, considering the sliding windows.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            len(self.X) - self.n_obs - self.perf_period\n",
    "        )  # Total number of sliding windows available\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Backtest class to store out-of-sample results\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "class Backtest:\n",
    "    \"\"\"Class to store out-of-sample results for a backtest.\"\"\"\n",
    "\n",
    "    def __init__(self, len_test: int, n_y: int, dates: pd.DatetimeIndex) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Backtest object.\n",
    "\n",
    "        :param len_test: Number of scenarios in the out-of-sample evaluation period.\n",
    "        :param n_y: Number of assets in the portfolio.\n",
    "        :param dates: DatetimeIndex containing the corresponding dates.\n",
    "        \"\"\"\n",
    "        self.weights = np.zeros(\n",
    "            (len_test, n_y)\n",
    "        )  # Initialize portfolio weights over time\n",
    "        self.rets = np.zeros(\n",
    "            len_test\n",
    "        )  # Initialize realized portfolio returns over time\n",
    "        self.dates = dates[\n",
    "            -len_test:\n",
    "        ]  # Keep only the dates for the out-of-sample period\n",
    "\n",
    "    def stats(self) -> None:\n",
    "        \"\"\"\n",
    "        Compute and store the cumulative returns, mean return, volatility, and Sharpe ratio.\n",
    "\n",
    "        This method calculates key performance metrics of the portfolio, including:\n",
    "        - Cumulative returns (Total Return Index), which show the total growth of the portfolio over time.\n",
    "        - Annualized mean return, which is an estimate of the average return the portfolio would achieve per year.\n",
    "        - Volatility, which measures the risk by calculating the standard deviation of returns.\n",
    "        - Sharpe ratio, which indicates the risk-adjusted return of the portfolio.\n",
    "        \"\"\"\n",
    "        # Calculate cumulative returns (Total Return Index)\n",
    "        tri = np.cumprod(self.rets + 1)\n",
    "        # Calculate the annualized mean return using the final cumulative return and the number of periods\n",
    "        self.mean = (tri[-1]) ** (1 / len(tri)) - 1\n",
    "        # Calculate the volatility (standard deviation) of the portfolio returns\n",
    "        self.vol = np.std(self.rets)\n",
    "        # Calculate the Sharpe ratio (mean return divided by volatility)\n",
    "        self.sharpe = self.mean / self.vol\n",
    "        # Create a DataFrame containing realized returns and cumulative returns, indexed by dates\n",
    "        if len(self.dates) == len(self.rets):\n",
    "            self.rets = pd.DataFrame(\n",
    "                {\"Date\": self.dates, \"rets\": self.rets, \"tri\": tri}\n",
    "            ).set_index(\"Date\")\n",
    "        else:\n",
    "            raise ValueError(\"Length of dates and returns must be equal.\")\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# InSample class to store in-sample results\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "class InSample:\n",
    "    \"\"\"Class to store the in-sample results of neural network training.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the InSample object.\n",
    "        \"\"\"\n",
    "        self.loss = []  # List to hold training losses\n",
    "        self.gamma = []  # List to hold gamma values (hyperparameter)\n",
    "        self.delta = []  # List to hold delta values (hyperparameter)\n",
    "        self.val_loss = []  # List to hold validation losses (optional)\n",
    "\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Return a DataFrame containing the training statistics.\n",
    "\n",
    "        :return: DataFrame with columns representing different metrics during training.\n",
    "        \"\"\"\n",
    "        # Return a DataFrame based on available data, adjusting columns accordingly\n",
    "        if not self.delta and not self.val_loss:\n",
    "            return pd.DataFrame(\n",
    "                list(zip(self.loss, self.gamma)), columns=[\"loss\", \"gamma\"]\n",
    "            )\n",
    "        elif not self.delta:\n",
    "            return pd.DataFrame(\n",
    "                list(zip(self.loss, self.val_loss, self.gamma)),\n",
    "                columns=[\"loss\", \"val_loss\", \"gamma\"],\n",
    "            )\n",
    "        elif not self.val_loss:\n",
    "            return pd.DataFrame(\n",
    "                list(zip(self.loss, self.gamma, self.delta)),\n",
    "                columns=[\"loss\", \"gamma\", \"delta\"],\n",
    "            )\n",
    "        else:\n",
    "            return pd.DataFrame(\n",
    "                list(zip(self.loss, self.val_loss, self.gamma, self.delta)),\n",
    "                columns=[\"loss\", \"val_loss\", \"gamma\", \"delta\"],\n",
    "            )\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# CrossVal class to store cross-validation results\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "class CrossVal:\n",
    "    \"\"\"Class to store cross-validation results of neural network training.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the CrossVal object.\n",
    "        \"\"\"\n",
    "        self.lr = []  # List to hold learning rates\n",
    "        self.epochs = []  # List to hold the number of epochs in each run\n",
    "        self.val_loss = []  # List to hold validation losses\n",
    "\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Return a DataFrame containing the cross-validation statistics.\n",
    "\n",
    "        :return: DataFrame with learning rate, epochs, and validation loss.\n",
    "        \"\"\"\n",
    "        # Create and return a DataFrame with learning rates, epochs, and validation losses\n",
    "        return pd.DataFrame(\n",
    "            list(zip(self.lr, self.epochs, self.val_loss)),\n",
    "            columns=[\"lr\", \"epochs\", \"val_loss\"],\n",
    "        )\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Test code for the Backtest class\n",
    "####################################################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    X = pd.DataFrame(\n",
    "        np.random.randn(100, 3)\n",
    "    )  # Create feature dataset with 100 samples and 3 features\n",
    "    Y = pd.DataFrame(\n",
    "        np.random.randn(100, 2)\n",
    "    )  # Create asset return dataset with 100 samples and 2 assets\n",
    "    dates = pd.date_range(start=\"2020-01-01\", periods=100, freq=\"D\")\n",
    "\n",
    "    # Check if GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize SlidingWindow with given parameters\n",
    "    n_obs = 10\n",
    "    perf_period = 5\n",
    "    sliding_window = SlidingWindow(X, Y, n_obs, perf_period, device=device)\n",
    "\n",
    "    # Fetch a sample window\n",
    "    print(\"Testing SlidingWindow...\")\n",
    "    x, y, y_perf = sliding_window[0]\n",
    "    print(f\"x (features): {x.shape}\")\n",
    "    print(f\"y (realizations): {y.shape}\")\n",
    "    print(f\"y_perf (performance window): {y_perf.shape}\")\n",
    "\n",
    "    # Initialize Backtest with given parameters\n",
    "    len_test = 30\n",
    "    backtest_obj = Backtest(len_test=len_test, n_y=2, dates=dates)\n",
    "\n",
    "    # Simulate some portfolio returns\n",
    "    backtest_obj.rets = np.random.randn(len_test)\n",
    "\n",
    "    print(\"\\nTesting Backtest...\")\n",
    "    backtest_obj.stats()\n",
    "    print(backtest_obj.rets.head())\n",
    "    print(f\"Mean return: {backtest_obj.mean:.4f}\")\n",
    "    print(f\"Volatility: {backtest_obj.vol:.4f}\")\n",
    "    print(f\"Sharpe ratio: {backtest_obj.sharpe:.4f}\")\n",
    "\n",
    "    # Test Backtest stats calculation\n",
    "    print(\"\\nTesting Backtest stats calculation...\")\n",
    "    backtest_obj.rets = np.array([0.05, -0.02, 0.03, 0.04, -0.01])\n",
    "    backtest_obj.dates = dates[\n",
    "        -len(backtest_obj.rets) :\n",
    "    ]  # Adjust dates to match returns length\n",
    "    backtest_obj.stats()\n",
    "    print(backtest_obj.rets.head())\n",
    "    print(f\"Mean return: {backtest_obj.mean:.4f}\")\n",
    "    print(f\"Volatility: {backtest_obj.vol:.4f}\")\n",
    "    print(f\"Sharpe ratio: {backtest_obj.sharpe:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "   Feature_0  Feature_1  Feature_2  Feature_3  Feature_4\n",
      "0  -0.074288  -1.611273   1.551819  -0.570024  -0.968283\n",
      "1   0.460295   0.830840  -0.169389  -1.699986  -0.895349\n",
      "2   1.748863   1.145611  -0.486230   1.499275   0.197001\n",
      "3   0.214283  -0.362579  -0.619226   0.905574   0.437117\n",
      "4   1.184838  -0.616461   1.771004   0.241603   0.195820\n",
      "Number of training observations: 700\n",
      "\n",
      "Test Data:\n",
      "     Feature_0  Feature_1  Feature_2  Feature_3  Feature_4\n",
      "650  -0.128159   0.060916   1.026948  -0.036438  -1.136972\n",
      "651   0.520090  -0.620568   0.838925  -0.219185   1.081232\n",
      "652   1.498235   0.744269   1.025227  -0.283826  -0.550065\n",
      "653   0.064667   1.787847  -0.359358   1.498579   1.320986\n",
      "654  -0.934380  -0.904862  -0.629665  -0.246409   0.865627\n",
      "Number of test observations: 350\n",
      "\n",
      "Updated Training Data:\n",
      "   Feature_0  Feature_1  Feature_2  Feature_3  Feature_4\n",
      "0  -0.074288  -1.611273   1.551819  -0.570024  -0.968283\n",
      "1   0.460295   0.830840  -0.169389  -1.699986  -0.895349\n",
      "2   1.748863   1.145611  -0.486230   1.499275   0.197001\n",
      "3   0.214283  -0.362579  -0.619226   0.905574   0.437117\n",
      "4   1.184838  -0.616461   1.771004   0.241603   0.195820\n",
      "Number of updated training observations: 600\n",
      "\n",
      "Updated Test Data:\n",
      "     Feature_0  Feature_1  Feature_2  Feature_3  Feature_4\n",
      "550   0.666698   1.612499   0.535864  -0.356594  -0.176166\n",
      "551   1.238057  -0.303584  -0.147851   1.722317   1.848785\n",
      "552   0.049947   1.240598   1.235890   1.564383   0.740894\n",
      "553  -1.020681  -0.118791  -2.248850   0.761289   1.155703\n",
      "554  -1.024892   0.686353  -0.782312  -0.837240   0.136867\n",
      "Number of updated test observations: 450\n"
     ]
    }
   ],
   "source": [
    "# DataLoad module\n",
    "#\n",
    "####################################################################################################\n",
    "# Import libraries\n",
    "####################################################################################################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "import numpy as np\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# TrainTest class\n",
    "####################################################################################################\n",
    "class TrainTest:\n",
    "    def __init__(self, data: pd.DataFrame, n_obs: int, split: list[float]) -> None:\n",
    "        \"\"\"\n",
    "        Object to hold the training, validation, and testing datasets.\n",
    "\n",
    "        :param data: pandas DataFrame with time series data.\n",
    "        :param n_obs: Number of observations per batch.\n",
    "        :param split: List of ratios that control the partition of data into training, testing, and validation sets.\n",
    "        \"\"\"\n",
    "        self.data: pd.DataFrame = data  # Store the input data as a DataFrame\n",
    "        self.n_obs: int = n_obs  # Set the number of observations per batch\n",
    "        self.split: list[float] = (\n",
    "            split  # Set the split ratios for training, validation, and testing\n",
    "        )\n",
    "\n",
    "        n_obs_tot: int = self.data.shape[\n",
    "            0\n",
    "        ]  # Calculate the total number of observations in the dataset\n",
    "        numel: np.ndarray = n_obs_tot * np.cumsum(\n",
    "            split\n",
    "        )  # Calculate the cumulative number of elements based on split ratios\n",
    "        self.numel: list[int] = [\n",
    "            round(i) for i in numel\n",
    "        ]  # Round the cumulative elements to get the indices for splits\n",
    "\n",
    "    def split_update(self, split: list[float]) -> None:\n",
    "        \"\"\"\n",
    "        Update the list outlining the split ratio of training, validation, and testing datasets.\n",
    "\n",
    "        :param split: List of ratios that control the partition of data into training, testing, and validation sets.\n",
    "        \"\"\"\n",
    "        self.split: list[float] = (\n",
    "            split  # Update the split ratios with the new list provided\n",
    "        )\n",
    "        n_obs_tot: int = self.data.shape[\n",
    "            0\n",
    "        ]  # Calculate the total number of observations in the dataset\n",
    "        numel: np.ndarray = n_obs_tot * np.cumsum(\n",
    "            split\n",
    "        )  # Calculate the cumulative number of elements based on new split ratios\n",
    "        self.numel: list[int] = [\n",
    "            round(i) for i in numel\n",
    "        ]  # Round the cumulative elements to get the indices for splits\n",
    "\n",
    "    def train(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Return the training subset of observations.\n",
    "\n",
    "        :return: pandas DataFrame containing the training data subset.\n",
    "        \"\"\"\n",
    "        return self.data[\n",
    "            : self.numel[0]\n",
    "        ]  # Return the data from the start up to the end of the training set\n",
    "\n",
    "    def test(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Return the test subset of observations.\n",
    "\n",
    "        :return: pandas DataFrame containing the test data subset.\n",
    "        \"\"\"\n",
    "        return self.data[\n",
    "            self.numel[0] - self.n_obs : self.numel[1]\n",
    "        ]  # Return the data for the test set, including overlap\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data for testing\n",
    "    n_tot = 1000  # Total number of observations\n",
    "    n_features = 5  # Number of features\n",
    "    split_ratios = [0.7, 0.3]  # 70% training, 30% testing\n",
    "    n_obs = 50  # Number of observations per batch\n",
    "\n",
    "    # Create a synthetic DataFrame with random data\n",
    "    data = pd.DataFrame(\n",
    "        np.random.randn(n_tot, n_features),\n",
    "        columns=[f\"Feature_{i}\" for i in range(n_features)],\n",
    "    )\n",
    "\n",
    "    # Initialize TrainTest object\n",
    "    train_test_obj = TrainTest(data=data, n_obs=n_obs, split=split_ratios)\n",
    "\n",
    "    # Test the training data split\n",
    "    train_data = train_test_obj.train()\n",
    "    print(\"Training Data:\")\n",
    "    print(train_data.head())\n",
    "    print(f\"Number of training observations: {len(train_data)}\")\n",
    "\n",
    "    # Test the test data split\n",
    "    test_data = train_test_obj.test()\n",
    "    print(\"\\nTest Data:\")\n",
    "    print(test_data.head())\n",
    "    print(f\"Number of test observations: {len(test_data)}\")\n",
    "\n",
    "    # Update split ratios and test again\n",
    "    new_split_ratios = [0.6, 0.4]  # Update split ratios\n",
    "    train_test_obj.split_update(split=new_split_ratios)\n",
    "\n",
    "    # Test the updated training data split\n",
    "    updated_train_data = train_test_obj.train()\n",
    "    print(\"\\nUpdated Training Data:\")\n",
    "    print(updated_train_data.head())\n",
    "    print(f\"Number of updated training observations: {len(updated_train_data)}\")\n",
    "\n",
    "    # Test the updated test data split\n",
    "    updated_test_data = train_test_obj.test()\n",
    "    print(\"\\nUpdated Test Data:\")\n",
    "    print(updated_test_data.head())\n",
    "    print(f\"Number of updated test observations: {len(updated_test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      "          0         1         2         3         4\n",
      "0  0.037531 -0.007538  0.036639  0.000060 -0.001520\n",
      "1  0.000079 -0.003700 -0.049743 -0.034093 -0.022725\n",
      "2 -0.059466  0.000666 -0.004978 -0.009004  0.002649\n",
      "3  0.000444  0.006347 -0.015048 -0.025928  0.001903\n",
      "4 -0.008474 -0.023720 -0.007309 -0.025420  0.031723\n",
      "Number of training feature observations: 720\n",
      "\n",
      "Test Features:\n",
      "            0         1         2         3         4\n",
      "616  0.014443  0.017019 -0.003300 -0.056563 -0.017051\n",
      "617  0.018964 -0.016458 -0.003853 -0.011657 -0.022333\n",
      "618 -0.022941  0.020247 -0.012068  0.003381  0.019936\n",
      "619  0.006151  0.024943 -0.006490 -0.010648 -0.017412\n",
      "620 -0.031169 -0.034504  0.012386  0.022285 -0.043454\n",
      "Number of test feature observations: 584\n",
      "\n",
      "Training Outputs:\n",
      "          0         1         2         3         4         5         6  \\\n",
      "0  0.020391 -0.024384 -0.050628 -0.016314 -0.044117  0.017402  0.090213   \n",
      "1 -0.001323  0.039438  0.068975 -0.035036  0.059205 -0.009392 -0.091626   \n",
      "2 -0.009475 -0.028947  0.089823 -0.018012  0.025656 -0.011985 -0.050961   \n",
      "3 -0.051537 -0.012596  0.051167 -0.006315  0.005678  0.046476 -0.037532   \n",
      "4 -0.016574  0.052634  0.000314  0.013466  0.029502 -0.026224  0.000905   \n",
      "\n",
      "          7         8         9  \n",
      "0  0.195578  0.055969 -0.010406  \n",
      "1  0.022831 -0.056277  0.004937  \n",
      "2 -0.112770  0.026023  0.100238  \n",
      "3  0.042647 -0.042372 -0.044804  \n",
      "4 -0.004659 -0.046663  0.068020  \n",
      "Number of training output observations: 720\n",
      "\n",
      "Test Outputs:\n",
      "            0         1         2         3         4         5         6  \\\n",
      "616  0.014203  0.029007 -0.032045 -0.003457 -0.008616  0.027413 -0.099575   \n",
      "617 -0.002404 -0.055210 -0.005516  0.008291 -0.002596  0.053899  0.041505   \n",
      "618 -0.028518  0.072404  0.042859 -0.040510  0.051107 -0.006973 -0.113639   \n",
      "619  0.000575  0.018074  0.017655  0.030531  0.117955 -0.056270 -0.133210   \n",
      "620  0.007498 -0.056192 -0.011489  0.000290  0.015717  0.015404  0.052565   \n",
      "\n",
      "            7         8         9  \n",
      "616  0.041896 -0.052044  0.115593  \n",
      "617  0.059450  0.037099  0.051167  \n",
      "618  0.036960 -0.121276  0.155090  \n",
      "619  0.046830  0.081611  0.026692  \n",
      "620 -0.160624  0.032781 -0.008716  \n",
      "Number of test output observations: 584\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "# Generate linear synthetic data\n",
    "####################################################################################################\n",
    "def synthetic(\n",
    "    n_x: int = 5,\n",
    "    n_y: int = 10,\n",
    "    n_tot: int = 1200,\n",
    "    n_obs: int = 104,\n",
    "    split: list[float] = [0.6, 0.4],\n",
    "    set_seed: int = 100,\n",
    ") -> Tuple[TrainTest, TrainTest]:\n",
    "    \"\"\"\n",
    "    Generates synthetic (normally-distributed) asset and factor data.\n",
    "\n",
    "    :param n_x: Number of features.\n",
    "    :param n_y: Number of assets.\n",
    "    :param n_tot: Number of observations in the whole dataset.\n",
    "    :param n_obs: Number of observations per batch.\n",
    "    :param split: List of floats representing train-validation-test split percentages (must sum up to one).\n",
    "    :param set_seed: Integer seed for replicability of the numpy RNG.\n",
    "\n",
    "    :return: Tuple of TrainTest objects for features and asset data split into train, validation, and test subsets.\n",
    "    \"\"\"\n",
    "    np.random.seed(set_seed)  # Set the random seed for reproducibility\n",
    "\n",
    "    # 'True' prediction bias and weights\n",
    "    a: np.ndarray = (\n",
    "        np.sort(np.random.rand(n_y) / 250) + 0.0001\n",
    "    )  # Generate small bias terms for each asset\n",
    "    b: np.ndarray = (\n",
    "        np.random.randn(n_x, n_y) / 5\n",
    "    )  # Generate random weights for linear relationships between features and assets\n",
    "    c: np.ndarray = np.random.randn(\n",
    "        int((n_x + 1) / 2), n_y\n",
    "    )  # Generate additional random weights for auxiliary features\n",
    "\n",
    "    # Noise standard deviation\n",
    "    s: np.ndarray = (\n",
    "        np.sort(np.random.rand(n_y)) / 20 + 0.02\n",
    "    )  # Generate small standard deviations for noise for each asset\n",
    "\n",
    "    # Synthetic features\n",
    "    X: np.ndarray = (\n",
    "        np.random.randn(n_tot, n_x) / 50\n",
    "    )  # Generate synthetic features from a normal distribution\n",
    "    X2: np.ndarray = (\n",
    "        np.random.randn(n_tot, int((n_x + 1) / 2)) / 50\n",
    "    )  # Generate auxiliary features from a normal distribution\n",
    "\n",
    "    # Synthetic outputs\n",
    "    Y: np.ndarray = (\n",
    "        a + X @ b + X2 @ c + s * np.random.randn(n_tot, n_y)\n",
    "    )  # Generate synthetic outputs based on linear combinations of features and noise\n",
    "\n",
    "    X: pd.DataFrame = pd.DataFrame(X)  # Convert features to a pandas DataFrame\n",
    "    Y: pd.DataFrame = pd.DataFrame(Y)  # Convert outputs to a pandas DataFrame\n",
    "\n",
    "    # Partition dataset into training and testing sets\n",
    "    return TrainTest(X, n_obs, split), TrainTest(\n",
    "        Y, n_obs, split\n",
    "    )  # Return TrainTest objects for features and outputs\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Test code for synthetic data generation\n",
    "####################################################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters for synthetic data generation\n",
    "    n_x = 5  # Number of features\n",
    "    n_y = 10  # Number of assets\n",
    "    n_tot = 1200  # Total number of observations\n",
    "    n_obs = 104  # Number of observations per batch\n",
    "    split = [0.6, 0.4]  # Split ratios for training and testing\n",
    "    set_seed = 100  # Random seed for reproducibility\n",
    "\n",
    "    # Generate synthetic data\n",
    "    train_test_features, train_test_outputs = synthetic(\n",
    "        n_x, n_y, n_tot, n_obs, split, set_seed\n",
    "    )\n",
    "\n",
    "    # Test the generated feature data\n",
    "    train_features = train_test_features.train()\n",
    "    test_features = train_test_features.test()\n",
    "    print(\"Training Features:\")\n",
    "    print(train_features.head())\n",
    "    print(f\"Number of training feature observations: {len(train_features)}\")\n",
    "    print(\"\\nTest Features:\")\n",
    "    print(test_features.head())\n",
    "    print(f\"Number of test feature observations: {len(test_features)}\")\n",
    "\n",
    "    # Test the generated output data\n",
    "    train_outputs = train_test_outputs.train()\n",
    "    test_outputs = train_test_outputs.test()\n",
    "    print(\"\\nTraining Outputs:\")\n",
    "    print(train_outputs.head())\n",
    "    print(f\"Number of training output observations: {len(train_outputs)}\")\n",
    "    print(\"\\nTest Outputs:\")\n",
    "    print(test_outputs.head())\n",
    "    print(f\"Number of test output observations: {len(test_outputs)}\")\n",
    "\n",
    "    # Verify the shape of the generated data\n",
    "    assert (\n",
    "        train_features.shape[1] == n_x\n",
    "    ), \"Number of features in training set does not match expected value.\"\n",
    "    assert (\n",
    "        train_outputs.shape[1] == n_y\n",
    "    ), \"Number of assets in training set does not match expected value.\"\n",
    "    assert (\n",
    "        test_features.shape[1] == n_x\n",
    "    ), \"Number of features in test set does not match expected value.\"\n",
    "    assert (\n",
    "        test_outputs.shape[1] == n_y\n",
    "    ), \"Number of assets in test set does not match expected value.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
