{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing p_var...\n",
      "Optimized portfolio weights (Variance): [0.33333333 0.33333333 0.33333333]\n",
      "Variance objective value: 0.0\n",
      "\n",
      "Testing p_mad...\n",
      "Optimized portfolio weights (MAD): [ 0.13355262  0.33328474 -0.66566742]\n",
      "MAD objective value: 3.469446951953614e-18\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "# Risk functions module\n",
    "#\n",
    "# This module defines the financial risk measures to be used in the optimization layer of the E2E\n",
    "# problem.\n",
    "#\n",
    "\n",
    "\n",
    "def p_var(z: cp.Expression, c: float, x: np.ndarray) -> cp.Expression:\n",
    "    \"\"\"\n",
    "    Compute the squared error for the given input.\n",
    "\n",
    "    :param z: A cvxpy expression (decision variable)\n",
    "    :param c: A constant threshold or target value\n",
    "    :param x: A numpy array (weights or features)\n",
    "    :return: The squared error expression\n",
    "    \"\"\"\n",
    "    return cp.square(x @ z - c)\n",
    "\n",
    "\n",
    "def p_mad(z: cp.Expression, c: float, x: np.ndarray) -> cp.Expression:\n",
    "    \"\"\"\n",
    "    Compute the mean absolute deviation for the given input.\n",
    "\n",
    "    :param z: A cvxpy expression (decision variable)\n",
    "    :param c: A constant threshold or target value\n",
    "    :param x: A numpy array (weights or features)\n",
    "    :return: The absolute deviation expression\n",
    "    \"\"\"\n",
    "    return cp.abs(x @ z - c)\n",
    "\n",
    "\n",
    "# Define test data\n",
    "z = cp.Variable(3)  # Decision variable (portfolio weights)\n",
    "c = 0.02  # Centering parameter (expected return)\n",
    "x = np.array(\n",
    "    [\n",
    "        [0.05, 0.02, -0.01],  # Realized returns for multiple scenarios\n",
    "        [0.03, -0.01, 0.04],\n",
    "        [-0.02, 0.01, 0.01],\n",
    "    ]\n",
    ")\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Test variance function (p_var)\n",
    "    print(\"\\nTesting p_var...\")\n",
    "    var_expr = p_var(z, c, x[0])  # Apply p_var to the first row of x\n",
    "    objective_var = cp.Minimize(var_expr)  # Minimize the variance\n",
    "    constraints = [\n",
    "        cp.sum(z) == 1,\n",
    "        z >= 0,\n",
    "    ]  # Portfolio constraints: sum of weights = 1, weights >= 0\n",
    "    problem_var = cp.Problem(objective_var, constraints)\n",
    "    var_opt_value = problem_var.solve()\n",
    "\n",
    "    # Output results for variance minimization\n",
    "    print(\"Optimized portfolio weights (Variance):\", z.value)\n",
    "    print(\"Variance objective value:\", var_opt_value)\n",
    "\n",
    "    # Reinitialize decision variable for MAD problem\n",
    "    z = cp.Variable(3)\n",
    "\n",
    "    # Test MAD function (p_mad)\n",
    "    print(\"\\nTesting p_mad...\")\n",
    "    mad_expr = p_mad(z, c, x[0])  # Apply p_mad to the first row of x\n",
    "    objective_mad = cp.Minimize(mad_expr)  # Minimize the MAD\n",
    "    problem_mad = cp.Problem(objective_mad, constraints)\n",
    "    mad_opt_value = problem_mad.solve()\n",
    "\n",
    "    # Output results for MAD minimization\n",
    "    print(\"Optimized portfolio weights (MAD):\", z.value)\n",
    "    print(\"MAD objective value:\", mad_opt_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing single_period_loss...\n",
      "Single period loss: -0.010999999940395355\n",
      "\n",
      "Testing single_period_over_var_loss...\n",
      "Single period loss over volatility: -10.989008903503418\n",
      "\n",
      "Testing sharpe_loss...\n",
      "Sharpe ratio loss: -11.988011360168457\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "# Performance loss functions with type hints and improved comments\n",
    "def single_period_loss(z_star: Tensor, y_perf: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the single-period loss based on the out-of-sample portfolio return.\n",
    "\n",
    "    This function computes the out-of-sample portfolio return for a given portfolio over the next\n",
    "    time step. It computes the loss as the negative return since optimization typically focuses\n",
    "    on minimizing the loss, and maximizing returns translates into minimizing negative returns.\n",
    "\n",
    "    :param z_star: Tensor of shape (n_y, 1) representing the optimal portfolio weights.\n",
    "    :param y_perf: Tensor of shape (perf_period, n_y) representing the realized returns.\n",
    "    :return: A scalar tensor representing the realized return at the first time step (negative).\n",
    "    \"\"\"\n",
    "    # Calculate the portfolio return for the first time step and negate it (since we want to minimize loss)\n",
    "    return -y_perf[0] @ z_star\n",
    "\n",
    "\n",
    "def single_period_over_var_loss(z_star: Tensor, y_perf: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the loss as the portfolio return divided by the portfolio's volatility.\n",
    "\n",
    "    This function computes the portfolio return at the first time step and divides it by the\n",
    "    realized volatility (standard deviation) of the portfolio returns over the performance period.\n",
    "    This provides a return-over-risk measure, which is often used in portfolio analysis.\n",
    "\n",
    "    :param z_star: Tensor of shape (n_y, 1) representing the optimal portfolio weights.\n",
    "    :param y_perf: Tensor of shape (perf_period, n_y) representing the realized returns.\n",
    "    :return: A scalar tensor representing the return over realized volatility (negative).\n",
    "    \"\"\"\n",
    "    # Calculate the portfolio returns over the entire performance period\n",
    "    portfolio_returns = y_perf @ z_star\n",
    "    # Calculate the standard deviation (volatility) of the portfolio returns, adding epsilon for numerical stability\n",
    "    volatility = torch.std(portfolio_returns, unbiased=True) + 1e-6\n",
    "    # Calculate the return at the first time step and divide by the volatility, then negate for loss\n",
    "    return -portfolio_returns[0] / volatility\n",
    "\n",
    "\n",
    "def sharpe_loss(z_star: Tensor, y_perf: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the loss based on the Sharpe ratio over a performance period.\n",
    "\n",
    "    This function computes a simplified Sharpe ratio, which is the ratio of the mean portfolio\n",
    "    return to its standard deviation (volatility) over the performance period. The loss is defined\n",
    "    as the negative Sharpe ratio to allow for minimization.\n",
    "\n",
    "    :param z_star: Tensor of shape (n_y, 1) representing the optimal portfolio weights.\n",
    "    :param y_perf: Tensor of shape (perf_period, n_y) representing the realized returns.\n",
    "    :return: A scalar tensor representing the negative Sharpe ratio.\n",
    "    \"\"\"\n",
    "    # Calculate the portfolio returns over the entire performance period\n",
    "    portfolio_returns = y_perf @ z_star\n",
    "    # Calculate the mean return of the portfolio\n",
    "    mean_return = torch.mean(portfolio_returns)\n",
    "    # Calculate the standard deviation (volatility) of the portfolio returns, adding epsilon for numerical stability\n",
    "    volatility = torch.std(portfolio_returns, unbiased=True) + 1e-6\n",
    "    # Calculate the Sharpe ratio and negate it for loss\n",
    "    return -mean_return / volatility\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example portfolio weights (3 assets)\n",
    "    z_star = torch.tensor([0.3, 0.5, 0.2])\n",
    "    # Realized returns for 3 assets over 3 periods\n",
    "    y_perf = torch.tensor(\n",
    "        [\n",
    "            [0.01, 0.02, -0.01],\n",
    "            [0.03, -0.01, 0.04],\n",
    "            [0.02, 0.01, 0.01],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Test the single-period loss function\n",
    "    print(\"Testing single_period_loss...\")\n",
    "    loss_sp = single_period_loss(z_star, y_perf)\n",
    "    print(f\"Single period loss: {loss_sp.item()}\")\n",
    "\n",
    "    # Test the single-period-over-volatility loss function\n",
    "    print(\"\\nTesting single_period_over_var_loss...\")\n",
    "    loss_sp_var = single_period_over_var_loss(z_star, y_perf)\n",
    "    print(f\"Single period loss over volatility: {loss_sp_var.item()}\")\n",
    "\n",
    "    # Test the Sharpe ratio loss function\n",
    "    print(\"\\nTesting sharpe_loss...\")\n",
    "    loss_sharpe = sharpe_loss(z_star, y_perf)\n",
    "    print(f\"Sharpe ratio loss: {loss_sharpe.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SlidingWindow...\n",
      "x (features): torch.Size([11, 3])\n",
      "y (realizations): torch.Size([10, 2])\n",
      "y_perf (performance window): torch.Size([5, 2])\n",
      "\n",
      "Testing Backtest...\n",
      "                rets       tri\n",
      "Date                          \n",
      "2020-03-11  0.568441  1.568441\n",
      "2020-03-12  0.599594  2.508869\n",
      "2020-03-13 -0.507123  1.236564\n",
      "2020-03-14 -0.236355  0.944296\n",
      "2020-03-15 -1.187913 -0.177445\n",
      "Mean return: -0.3105\n",
      "Volatility: 1.0426\n",
      "Sharpe ratio: -0.2978\n",
      "\n",
      "Testing Backtest stats calculation...\n",
      "            rets       tri\n",
      "Date                      \n",
      "2020-04-05  0.05  1.050000\n",
      "2020-04-06 -0.02  1.029000\n",
      "2020-04-07  0.03  1.059870\n",
      "2020-04-08  0.04  1.102265\n",
      "2020-04-09 -0.01  1.091242\n",
      "Mean return: 0.0176\n",
      "Volatility: 0.0279\n",
      "Sharpe ratio: 0.6324\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "####################################################################################################\n",
    "# SlidingWindow Dataset to index data using a sliding window\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "class SlidingWindow(Dataset):\n",
    "    \"\"\"Dataset class for creating a sliding window from time series data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        Y: pd.DataFrame,\n",
    "        n_obs: int,\n",
    "        perf_period: int,\n",
    "        dtype: torch.dtype = torch.float32,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the SlidingWindow dataset.\n",
    "\n",
    "        :param X: DataFrame containing the complete feature dataset.\n",
    "        :param Y: DataFrame containing the complete asset return dataset.\n",
    "        :param n_obs: Number of observations in the sliding window.\n",
    "        :param perf_period: Number of future observations used for out-of-sample performance evaluation.\n",
    "        :param dtype: The desired data type for tensors (default is torch.float32).\n",
    "        :param device: Device on which to place the tensors (e.g., 'cpu' or 'cuda' for GPU).\n",
    "        \"\"\"\n",
    "        self.X = torch.tensor(\n",
    "            X.values, dtype=dtype, device=device\n",
    "        )  # Convert feature dataset to tensor\n",
    "        self.Y = torch.tensor(\n",
    "            Y.values, dtype=dtype, device=device\n",
    "        )  # Convert asset return dataset to tensor\n",
    "        self.n_obs = n_obs  # Number of observations in the sliding window\n",
    "        self.perf_period = (\n",
    "            perf_period  # Number of future observations for performance evaluation\n",
    "        )\n",
    "\n",
    "    def __getitem__(\n",
    "        self, index: int\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Retrieve a single window of data.\n",
    "\n",
    "        :param index: Index of the sliding window.\n",
    "        :return: Tuple (x, y, y_perf):\n",
    "            x: Features window of shape (n_obs + 1, n_x).\n",
    "            y: Realizations window of shape (n_obs, n_y).\n",
    "            y_perf: Future performance window of shape (perf_period, n_y).\n",
    "        \"\"\"\n",
    "        # Retrieve features for the sliding window (n_obs + 1 observations)\n",
    "        x = self.X[index : index + self.n_obs + 1]\n",
    "        # Retrieve asset returns for the sliding window (n_obs observations)\n",
    "        y = self.Y[index : index + self.n_obs]\n",
    "        # Retrieve future performance data (perf_period observations)\n",
    "        y_perf = self.Y[index + self.n_obs : index + self.n_obs + self.perf_period]\n",
    "        return (x, y, y_perf)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the number of windows that can be created from the dataset.\n",
    "\n",
    "        :return: Length of the dataset, considering the sliding windows.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            len(self.X) - self.n_obs - self.perf_period\n",
    "        )  # Total number of sliding windows available\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Backtest class to store out-of-sample results\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "class Backtest:\n",
    "    \"\"\"Class to store out-of-sample results for a backtest.\"\"\"\n",
    "\n",
    "    def __init__(self, len_test: int, n_y: int, dates: pd.DatetimeIndex) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Backtest object.\n",
    "\n",
    "        :param len_test: Number of scenarios in the out-of-sample evaluation period.\n",
    "        :param n_y: Number of assets in the portfolio.\n",
    "        :param dates: DatetimeIndex containing the corresponding dates.\n",
    "        \"\"\"\n",
    "        self.weights = np.zeros(\n",
    "            (len_test, n_y)\n",
    "        )  # Initialize portfolio weights over time\n",
    "        self.rets = np.zeros(\n",
    "            len_test\n",
    "        )  # Initialize realized portfolio returns over time\n",
    "        self.dates = dates[\n",
    "            -len_test:\n",
    "        ]  # Keep only the dates for the out-of-sample period\n",
    "\n",
    "    def stats(self) -> None:\n",
    "        \"\"\"\n",
    "        Compute and store the cumulative returns, mean return, volatility, and Sharpe ratio.\n",
    "\n",
    "        This method calculates key performance metrics of the portfolio, including:\n",
    "        - Cumulative returns (Total Return Index), which show the total growth of the portfolio over time.\n",
    "        - Annualized mean return, which is an estimate of the average return the portfolio would achieve per year.\n",
    "        - Volatility, which measures the risk by calculating the standard deviation of returns.\n",
    "        - Sharpe ratio, which indicates the risk-adjusted return of the portfolio.\n",
    "        \"\"\"\n",
    "        # Calculate cumulative returns (Total Return Index)\n",
    "        tri = np.cumprod(self.rets + 1)\n",
    "        # Calculate the annualized mean return using the final cumulative return and the number of periods\n",
    "        self.mean = (tri[-1]) ** (1 / len(tri)) - 1\n",
    "        # Calculate the volatility (standard deviation) of the portfolio returns\n",
    "        self.vol = np.std(self.rets)\n",
    "        # Calculate the Sharpe ratio (mean return divided by volatility)\n",
    "        self.sharpe = self.mean / self.vol\n",
    "        # Create a DataFrame containing realized returns and cumulative returns, indexed by dates\n",
    "        if len(self.dates) == len(self.rets):\n",
    "            self.rets = pd.DataFrame(\n",
    "                {\"Date\": self.dates, \"rets\": self.rets, \"tri\": tri}\n",
    "            ).set_index(\"Date\")\n",
    "        else:\n",
    "            raise ValueError(\"Length of dates and returns must be equal.\")\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# InSample class to store in-sample results\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "class InSample:\n",
    "    \"\"\"Class to store the in-sample results of neural network training.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the InSample object.\n",
    "        \"\"\"\n",
    "        self.loss = []  # List to hold training losses\n",
    "        self.gamma = []  # List to hold gamma values (hyperparameter)\n",
    "        self.delta = []  # List to hold delta values (hyperparameter)\n",
    "        self.val_loss = []  # List to hold validation losses (optional)\n",
    "\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Return a DataFrame containing the training statistics.\n",
    "\n",
    "        :return: DataFrame with columns representing different metrics during training.\n",
    "        \"\"\"\n",
    "        # Return a DataFrame based on available data, adjusting columns accordingly\n",
    "        if not self.delta and not self.val_loss:\n",
    "            return pd.DataFrame(\n",
    "                list(zip(self.loss, self.gamma)), columns=[\"loss\", \"gamma\"]\n",
    "            )\n",
    "        elif not self.delta:\n",
    "            return pd.DataFrame(\n",
    "                list(zip(self.loss, self.val_loss, self.gamma)),\n",
    "                columns=[\"loss\", \"val_loss\", \"gamma\"],\n",
    "            )\n",
    "        elif not self.val_loss:\n",
    "            return pd.DataFrame(\n",
    "                list(zip(self.loss, self.gamma, self.delta)),\n",
    "                columns=[\"loss\", \"gamma\", \"delta\"],\n",
    "            )\n",
    "        else:\n",
    "            return pd.DataFrame(\n",
    "                list(zip(self.loss, self.val_loss, self.gamma, self.delta)),\n",
    "                columns=[\"loss\", \"val_loss\", \"gamma\", \"delta\"],\n",
    "            )\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# CrossVal class to store cross-validation results\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "class CrossVal:\n",
    "    \"\"\"Class to store cross-validation results of neural network training.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the CrossVal object.\n",
    "        \"\"\"\n",
    "        self.lr = []  # List to hold learning rates\n",
    "        self.epochs = []  # List to hold the number of epochs in each run\n",
    "        self.val_loss = []  # List to hold validation losses\n",
    "\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Return a DataFrame containing the cross-validation statistics.\n",
    "\n",
    "        :return: DataFrame with learning rate, epochs, and validation loss.\n",
    "        \"\"\"\n",
    "        # Create and return a DataFrame with learning rates, epochs, and validation losses\n",
    "        return pd.DataFrame(\n",
    "            list(zip(self.lr, self.epochs, self.val_loss)),\n",
    "            columns=[\"lr\", \"epochs\", \"val_loss\"],\n",
    "        )\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Test code for the Backtest class\n",
    "####################################################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    X = pd.DataFrame(\n",
    "        np.random.randn(100, 3)\n",
    "    )  # Create feature dataset with 100 samples and 3 features\n",
    "    Y = pd.DataFrame(\n",
    "        np.random.randn(100, 2)\n",
    "    )  # Create asset return dataset with 100 samples and 2 assets\n",
    "    dates = pd.date_range(start=\"2020-01-01\", periods=100, freq=\"D\")\n",
    "\n",
    "    # Check if GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize SlidingWindow with given parameters\n",
    "    n_obs = 10\n",
    "    perf_period = 5\n",
    "    sliding_window = SlidingWindow(X, Y, n_obs, perf_period, device=device)\n",
    "\n",
    "    # Fetch a sample window\n",
    "    print(\"Testing SlidingWindow...\")\n",
    "    x, y, y_perf = sliding_window[0]\n",
    "    print(f\"x (features): {x.shape}\")\n",
    "    print(f\"y (realizations): {y.shape}\")\n",
    "    print(f\"y_perf (performance window): {y_perf.shape}\")\n",
    "\n",
    "    # Initialize Backtest with given parameters\n",
    "    len_test = 30\n",
    "    backtest_obj = Backtest(len_test=len_test, n_y=2, dates=dates)\n",
    "\n",
    "    # Simulate some portfolio returns\n",
    "    backtest_obj.rets = np.random.randn(len_test)\n",
    "\n",
    "    print(\"\\nTesting Backtest...\")\n",
    "    backtest_obj.stats()\n",
    "    print(backtest_obj.rets.head())\n",
    "    print(f\"Mean return: {backtest_obj.mean:.4f}\")\n",
    "    print(f\"Volatility: {backtest_obj.vol:.4f}\")\n",
    "    print(f\"Sharpe ratio: {backtest_obj.sharpe:.4f}\")\n",
    "\n",
    "    # Test Backtest stats calculation\n",
    "    print(\"\\nTesting Backtest stats calculation...\")\n",
    "    backtest_obj.rets = np.array([0.05, -0.02, 0.03, 0.04, -0.01])\n",
    "    backtest_obj.dates = dates[\n",
    "        -len(backtest_obj.rets) :\n",
    "    ]  # Adjust dates to match returns length\n",
    "    backtest_obj.stats()\n",
    "    print(backtest_obj.rets.head())\n",
    "    print(f\"Mean return: {backtest_obj.mean:.4f}\")\n",
    "    print(f\"Volatility: {backtest_obj.vol:.4f}\")\n",
    "    print(f\"Sharpe ratio: {backtest_obj.sharpe:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "   Feature_0  Feature_1  Feature_2  Feature_3  Feature_4\n",
      "0  -0.966517   0.417190  -0.688667  -0.062657  -0.563649\n",
      "1  -0.410931  -0.367224  -0.706757   0.302450  -0.627928\n",
      "2   0.015687  -0.728732  -1.534299   0.912435   0.499527\n",
      "3   2.288682  -1.043973  -0.208960  -0.128507   0.989369\n",
      "4  -0.507108  -1.171042   1.207809  -0.316650   0.878644\n",
      "Number of training observations: 700\n",
      "\n",
      "Test Data:\n",
      "     Feature_0  Feature_1  Feature_2  Feature_3  Feature_4\n",
      "650  -0.769562   0.547303   1.364082  -1.378803  -0.196991\n",
      "651  -0.610782   0.142851   0.010018   1.345381  -0.108405\n",
      "652   1.374784  -0.315100   0.926482   0.228663   1.729885\n",
      "653  -1.477015  -1.163858   1.831622  -0.733265  -0.669803\n",
      "654  -0.875724   1.148446  -1.253458   0.022450  -0.265622\n",
      "Number of test observations: 350\n",
      "\n",
      "Updated Training Data:\n",
      "   Feature_0  Feature_1  Feature_2  Feature_3  Feature_4\n",
      "0  -0.966517   0.417190  -0.688667  -0.062657  -0.563649\n",
      "1  -0.410931  -0.367224  -0.706757   0.302450  -0.627928\n",
      "2   0.015687  -0.728732  -1.534299   0.912435   0.499527\n",
      "3   2.288682  -1.043973  -0.208960  -0.128507   0.989369\n",
      "4  -0.507108  -1.171042   1.207809  -0.316650   0.878644\n",
      "Number of updated training observations: 600\n",
      "\n",
      "Updated Test Data:\n",
      "     Feature_0  Feature_1  Feature_2  Feature_3  Feature_4\n",
      "550   0.079505   0.065351   0.079757  -0.859233  -0.206125\n",
      "551   0.760322  -0.179817   0.161782   0.167037   0.417395\n",
      "552   0.558651  -0.174151  -0.911335  -1.218715   1.017686\n",
      "553   0.770250   0.649365   0.811204  -0.161556  -0.624876\n",
      "554  -0.980407  -1.089350   1.212159  -0.493145   1.054859\n",
      "Number of updated test observations: 450\n"
     ]
    }
   ],
   "source": [
    "# DataLoad module\n",
    "#\n",
    "####################################################################################################\n",
    "# Import libraries\n",
    "####################################################################################################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "import numpy as np\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# TrainTest class\n",
    "####################################################################################################\n",
    "class TrainTest:\n",
    "    def __init__(self, data: pd.DataFrame, n_obs: int, split: list[float]) -> None:\n",
    "        \"\"\"\n",
    "        Object to hold the training, validation, and testing datasets.\n",
    "\n",
    "        :param data: pandas DataFrame with time series data.\n",
    "        :param n_obs: Number of observations per batch.\n",
    "        :param split: List of ratios that control the partition of data into training, testing, and validation sets.\n",
    "        \"\"\"\n",
    "        self.data: pd.DataFrame = data  # Store the input data as a DataFrame\n",
    "        self.n_obs: int = n_obs  # Set the number of observations per batch\n",
    "        self.split: list[float] = (\n",
    "            split  # Set the split ratios for training, validation, and testing\n",
    "        )\n",
    "\n",
    "        n_obs_tot: int = self.data.shape[\n",
    "            0\n",
    "        ]  # Calculate the total number of observations in the dataset\n",
    "        numel: np.ndarray = n_obs_tot * np.cumsum(\n",
    "            split\n",
    "        )  # Calculate the cumulative number of elements based on split ratios\n",
    "        self.numel: list[int] = [\n",
    "            round(i) for i in numel\n",
    "        ]  # Round the cumulative elements to get the indices for splits\n",
    "\n",
    "    def split_update(self, split: list[float]) -> None:\n",
    "        \"\"\"\n",
    "        Update the list outlining the split ratio of training, validation, and testing datasets.\n",
    "\n",
    "        :param split: List of ratios that control the partition of data into training, testing, and validation sets.\n",
    "        \"\"\"\n",
    "        self.split: list[float] = (\n",
    "            split  # Update the split ratios with the new list provided\n",
    "        )\n",
    "        n_obs_tot: int = self.data.shape[\n",
    "            0\n",
    "        ]  # Calculate the total number of observations in the dataset\n",
    "        numel: np.ndarray = n_obs_tot * np.cumsum(\n",
    "            split\n",
    "        )  # Calculate the cumulative number of elements based on new split ratios\n",
    "        self.numel: list[int] = [\n",
    "            round(i) for i in numel\n",
    "        ]  # Round the cumulative elements to get the indices for splits\n",
    "\n",
    "    def train(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Return the training subset of observations.\n",
    "\n",
    "        :return: pandas DataFrame containing the training data subset.\n",
    "        \"\"\"\n",
    "        return self.data[\n",
    "            : self.numel[0]\n",
    "        ]  # Return the data from the start up to the end of the training set\n",
    "\n",
    "    def test(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Return the test subset of observations.\n",
    "\n",
    "        :return: pandas DataFrame containing the test data subset.\n",
    "        \"\"\"\n",
    "        return self.data[\n",
    "            self.numel[0] - self.n_obs : self.numel[1]\n",
    "        ]  # Return the data for the test set, including overlap\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data for testing\n",
    "    n_tot = 1000  # Total number of observations\n",
    "    n_features = 5  # Number of features\n",
    "    split_ratios = [0.7, 0.3]  # 70% training, 30% testing\n",
    "    n_obs = 50  # Number of observations per batch\n",
    "\n",
    "    # Create a synthetic DataFrame with random data\n",
    "    data = pd.DataFrame(\n",
    "        np.random.randn(n_tot, n_features),\n",
    "        columns=[f\"Feature_{i}\" for i in range(n_features)],\n",
    "    )\n",
    "\n",
    "    # Initialize TrainTest object\n",
    "    train_test_obj = TrainTest(data=data, n_obs=n_obs, split=split_ratios)\n",
    "\n",
    "    # Test the training data split\n",
    "    train_data = train_test_obj.train()\n",
    "    print(\"Training Data:\")\n",
    "    print(train_data.head())\n",
    "    print(f\"Number of training observations: {len(train_data)}\")\n",
    "\n",
    "    # Test the test data split\n",
    "    test_data = train_test_obj.test()\n",
    "    print(\"\\nTest Data:\")\n",
    "    print(test_data.head())\n",
    "    print(f\"Number of test observations: {len(test_data)}\")\n",
    "\n",
    "    # Update split ratios and test again\n",
    "    new_split_ratios = [0.6, 0.4]  # Update split ratios\n",
    "    train_test_obj.split_update(split=new_split_ratios)\n",
    "\n",
    "    # Test the updated training data split\n",
    "    updated_train_data = train_test_obj.train()\n",
    "    print(\"\\nUpdated Training Data:\")\n",
    "    print(updated_train_data.head())\n",
    "    print(f\"Number of updated training observations: {len(updated_train_data)}\")\n",
    "\n",
    "    # Test the updated test data split\n",
    "    updated_test_data = train_test_obj.test()\n",
    "    print(\"\\nUpdated Test Data:\")\n",
    "    print(updated_test_data.head())\n",
    "    print(f\"Number of updated test observations: {len(updated_test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      "          0         1         2         3         4\n",
      "0  0.037531 -0.007538  0.036639  0.000060 -0.001520\n",
      "1  0.000079 -0.003700 -0.049743 -0.034093 -0.022725\n",
      "2 -0.059466  0.000666 -0.004978 -0.009004  0.002649\n",
      "3  0.000444  0.006347 -0.015048 -0.025928  0.001903\n",
      "4 -0.008474 -0.023720 -0.007309 -0.025420  0.031723\n",
      "Number of training feature observations: 720\n",
      "\n",
      "Test Features:\n",
      "            0         1         2         3         4\n",
      "616  0.014443  0.017019 -0.003300 -0.056563 -0.017051\n",
      "617  0.018964 -0.016458 -0.003853 -0.011657 -0.022333\n",
      "618 -0.022941  0.020247 -0.012068  0.003381  0.019936\n",
      "619  0.006151  0.024943 -0.006490 -0.010648 -0.017412\n",
      "620 -0.031169 -0.034504  0.012386  0.022285 -0.043454\n",
      "Number of test feature observations: 584\n",
      "\n",
      "Training Outputs:\n",
      "          0         1         2         3         4         5         6  \\\n",
      "0  0.020391 -0.024384 -0.050628 -0.016314 -0.044117  0.017402  0.090213   \n",
      "1 -0.001323  0.039438  0.068975 -0.035036  0.059205 -0.009392 -0.091626   \n",
      "2 -0.009475 -0.028947  0.089823 -0.018012  0.025656 -0.011985 -0.050961   \n",
      "3 -0.051537 -0.012596  0.051167 -0.006315  0.005678  0.046476 -0.037532   \n",
      "4 -0.016574  0.052634  0.000314  0.013466  0.029502 -0.026224  0.000905   \n",
      "\n",
      "          7         8         9  \n",
      "0  0.195578  0.055969 -0.010406  \n",
      "1  0.022831 -0.056277  0.004937  \n",
      "2 -0.112770  0.026023  0.100238  \n",
      "3  0.042647 -0.042372 -0.044804  \n",
      "4 -0.004659 -0.046663  0.068020  \n",
      "Number of training output observations: 720\n",
      "\n",
      "Test Outputs:\n",
      "            0         1         2         3         4         5         6  \\\n",
      "616  0.014203  0.029007 -0.032045 -0.003457 -0.008616  0.027413 -0.099575   \n",
      "617 -0.002404 -0.055210 -0.005516  0.008291 -0.002596  0.053899  0.041505   \n",
      "618 -0.028518  0.072404  0.042859 -0.040510  0.051107 -0.006973 -0.113639   \n",
      "619  0.000575  0.018074  0.017655  0.030531  0.117955 -0.056270 -0.133210   \n",
      "620  0.007498 -0.056192 -0.011489  0.000290  0.015717  0.015404  0.052565   \n",
      "\n",
      "            7         8         9  \n",
      "616  0.041896 -0.052044  0.115593  \n",
      "617  0.059450  0.037099  0.051167  \n",
      "618  0.036960 -0.121276  0.155090  \n",
      "619  0.046830  0.081611  0.026692  \n",
      "620 -0.160624  0.032781 -0.008716  \n",
      "Number of test output observations: 584\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "# Generate linear synthetic data\n",
    "####################################################################################################\n",
    "def synthetic(\n",
    "    n_x: int = 5,\n",
    "    n_y: int = 10,\n",
    "    n_tot: int = 1200,\n",
    "    n_obs: int = 104,\n",
    "    split: list[float] = [0.6, 0.4],\n",
    "    set_seed: int = 100,\n",
    ") -> tuple[TrainTest, TrainTest]:\n",
    "    \"\"\"\n",
    "    Generates synthetic (normally-distributed) asset and factor data.\n",
    "\n",
    "    :param n_x: Number of features.\n",
    "    :param n_y: Number of assets.\n",
    "    :param n_tot: Number of observations in the whole dataset.\n",
    "    :param n_obs: Number of observations per batch.\n",
    "    :param split: List of floats representing train-validation-test split percentages (must sum up to one).\n",
    "    :param set_seed: Integer seed for replicability of the numpy RNG.\n",
    "\n",
    "    :return: Tuple of TrainTest objects for features and asset data split into train, validation, and test subsets.\n",
    "    \"\"\"\n",
    "    np.random.seed(set_seed)  # Set the random seed for reproducibility\n",
    "\n",
    "    # 'True' prediction bias and weights\n",
    "    a: np.ndarray = (\n",
    "        np.sort(np.random.rand(n_y) / 250) + 0.0001\n",
    "    )  # Generate small bias terms for each asset\n",
    "    b: np.ndarray = (\n",
    "        np.random.randn(n_x, n_y) / 5\n",
    "    )  # Generate random weights for linear relationships between features and assets\n",
    "    c: np.ndarray = np.random.randn(\n",
    "        int((n_x + 1) / 2), n_y\n",
    "    )  # Generate additional random weights for auxiliary features\n",
    "\n",
    "    # Noise standard deviation\n",
    "    s: np.ndarray = (\n",
    "        np.sort(np.random.rand(n_y)) / 20 + 0.02\n",
    "    )  # Generate small standard deviations for noise for each asset\n",
    "\n",
    "    # Synthetic features\n",
    "    X: np.ndarray = (\n",
    "        np.random.randn(n_tot, n_x) / 50\n",
    "    )  # Generate synthetic features from a normal distribution\n",
    "    X2: np.ndarray = (\n",
    "        np.random.randn(n_tot, int((n_x + 1) / 2)) / 50\n",
    "    )  # Generate auxiliary features from a normal distribution\n",
    "\n",
    "    # Synthetic outputs\n",
    "    Y: np.ndarray = (\n",
    "        a + X @ b + X2 @ c + s * np.random.randn(n_tot, n_y)\n",
    "    )  # Generate synthetic outputs based on linear combinations of features and noise\n",
    "\n",
    "    X: pd.DataFrame = pd.DataFrame(X)  # Convert features to a pandas DataFrame\n",
    "    Y: pd.DataFrame = pd.DataFrame(Y)  # Convert outputs to a pandas DataFrame\n",
    "\n",
    "    # Partition dataset into training and testing sets\n",
    "    return TrainTest(X, n_obs, split), TrainTest(\n",
    "        Y, n_obs, split\n",
    "    )  # Return TrainTest objects for features and outputs\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Test code for synthetic data generation\n",
    "####################################################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters for synthetic data generation\n",
    "    n_x = 5  # Number of features\n",
    "    n_y = 10  # Number of assets\n",
    "    n_tot = 1200  # Total number of observations\n",
    "    n_obs = 104  # Number of observations per batch\n",
    "    split = [0.6, 0.4]  # Split ratios for training and testing\n",
    "    set_seed = 100  # Random seed for reproducibility\n",
    "\n",
    "    # Generate synthetic data\n",
    "    train_test_features, train_test_outputs = synthetic(\n",
    "        n_x, n_y, n_tot, n_obs, split, set_seed\n",
    "    )\n",
    "\n",
    "    # Test the generated feature data\n",
    "    train_features = train_test_features.train()\n",
    "    test_features = train_test_features.test()\n",
    "    print(\"Training Features:\")\n",
    "    print(train_features.head())\n",
    "    print(f\"Number of training feature observations: {len(train_features)}\")\n",
    "    print(\"\\nTest Features:\")\n",
    "    print(test_features.head())\n",
    "    print(f\"Number of test feature observations: {len(test_features)}\")\n",
    "\n",
    "    # Test the generated output data\n",
    "    train_outputs = train_test_outputs.train()\n",
    "    test_outputs = train_test_outputs.test()\n",
    "    print(\"\\nTraining Outputs:\")\n",
    "    print(train_outputs.head())\n",
    "    print(f\"Number of training output observations: {len(train_outputs)}\")\n",
    "    print(\"\\nTest Outputs:\")\n",
    "    print(test_outputs.head())\n",
    "    print(f\"Number of test output observations: {len(test_outputs)}\")\n",
    "\n",
    "    # Verify the shape of the generated data\n",
    "    assert (\n",
    "        train_features.shape[1] == n_x\n",
    "    ), \"Number of features in training set does not match expected value.\"\n",
    "    assert (\n",
    "        train_outputs.shape[1] == n_y\n",
    "    ), \"Number of assets in training set does not match expected value.\"\n",
    "    assert (\n",
    "        test_features.shape[1] == n_x\n",
    "    ), \"Number of features in test set does not match expected value.\"\n",
    "    assert (\n",
    "        test_outputs.shape[1] == n_y\n",
    "    ), \"Number of assets in test set does not match expected value.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "# Generate non-linear synthetic data\n",
    "####################################################################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unittest\n",
    "\n",
    "\n",
    "def synthetic_nl(\n",
    "    n_x: int = 5,\n",
    "    n_y: int = 10,\n",
    "    n_tot: int = 1200,\n",
    "    n_obs: int = 104,\n",
    "    split: list[float] = [0.6, 0.4],\n",
    "    set_seed: int = 100,\n",
    ") -> tuple[TrainTest, TrainTest]:\n",
    "    \"\"\"\n",
    "    Generates synthetic (normally-distributed) factor data and mixes them using a quadratic model\n",
    "    of linear, squared, and cross products to produce the asset data.\n",
    "\n",
    "    :param n_x: Number of features.\n",
    "    :param n_y: Number of assets.\n",
    "    :param n_tot: Number of observations in the whole dataset.\n",
    "    :param n_obs: Number of observations per batch.\n",
    "    :param split: List of floats representing train-validation-test split percentages (must sum up to one).\n",
    "    :param set_seed: Integer seed for replicability of the numpy RNG.\n",
    "    :return: Tuple of TrainTest objects for features and asset data split into train, validation, and test subsets.\n",
    "    \"\"\"\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(set_seed)\n",
    "\n",
    "    # Generate 'True' prediction bias and weights\n",
    "    a: np.ndarray = (\n",
    "        np.sort(np.random.rand(n_y) / 200) + 0.0005\n",
    "    )  # Bias terms for each asset\n",
    "    b: np.ndarray = np.random.randn(n_x, n_y) / 4  # Linear relationship weights\n",
    "    c: np.ndarray = np.random.randn(\n",
    "        int((n_x + 1) / 2), n_y\n",
    "    )  # Auxiliary feature weights\n",
    "    d: np.ndarray = np.random.randn(n_x**2, n_y) / n_x  # Cross-product weights\n",
    "\n",
    "    # Generate noise standard deviation for each asset\n",
    "    s: np.ndarray = np.sort(np.random.rand(n_y)) / 20 + 0.02\n",
    "\n",
    "    # Generate synthetic features\n",
    "    X: np.ndarray = np.random.randn(n_tot, n_x) / 50  # Main features\n",
    "    X2: np.ndarray = (\n",
    "        np.random.randn(n_tot, int((n_x + 1) / 2)) / 50\n",
    "    )  # Auxiliary features\n",
    "    X_cross: np.ndarray = 100 * (X[:, :, None] * X[:, None, :]).reshape(\n",
    "        n_tot, n_x**2\n",
    "    )  # Cross-product features\n",
    "    X_cross = X_cross - X_cross.mean(axis=0)  # Center cross-product features\n",
    "\n",
    "    # Generate synthetic outputs\n",
    "    Y: np.ndarray = a + X @ b + X2 @ c + X_cross @ d + s * np.random.randn(n_tot, n_y)\n",
    "\n",
    "    # Convert features and outputs to pandas DataFrames\n",
    "    X_df: pd.DataFrame = pd.DataFrame(X)\n",
    "    Y_df: pd.DataFrame = pd.DataFrame(Y)\n",
    "\n",
    "    # Partition dataset into training and testing sets\n",
    "    return TrainTest(X_df, n_obs, split), TrainTest(Y_df, n_obs, split)\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Unit Test for synthetic_nl function\n",
    "####################################################################################################\n",
    "def test_synthetic_nl() -> None:\n",
    "    n_x = 5\n",
    "    n_y = 10\n",
    "    n_tot = 1200\n",
    "    n_obs = 104\n",
    "    split = [0.6, 0.4]\n",
    "    set_seed = 100\n",
    "\n",
    "    features, outputs = synthetic_nl(\n",
    "        n_x=n_x,\n",
    "        n_y=n_y,\n",
    "        n_tot=n_tot,\n",
    "        n_obs=n_obs,\n",
    "        split=split,\n",
    "        set_seed=set_seed,\n",
    "    )\n",
    "\n",
    "    assert isinstance(\n",
    "        features, TrainTest\n",
    "    ), \"Features should be an instance of TrainTest.\"\n",
    "    assert isinstance(outputs, TrainTest), \"Outputs should be an instance of TrainTest.\"\n",
    "    assert features.data.shape == (n_tot, n_x), \"Features data shape mismatch.\"\n",
    "    assert outputs.data.shape == (n_tot, n_y), \"Outputs data shape mismatch.\"\n",
    "    expected_train_len = int(n_tot * split[0])\n",
    "    assert (\n",
    "        len(features.train()) == expected_train_len\n",
    "    ), \"Training split size mismatch for features.\"\n",
    "    assert (\n",
    "        len(outputs.train()) == expected_train_len\n",
    "    ), \"Training split size mismatch for outputs.\"\n",
    "\n",
    "    # Test reproducibility\n",
    "    features_1, outputs_1 = synthetic_nl(\n",
    "        n_x=n_x,\n",
    "        n_y=n_y,\n",
    "        n_tot=n_tot,\n",
    "        n_obs=n_obs,\n",
    "        split=split,\n",
    "        set_seed=set_seed,\n",
    "    )\n",
    "    features_2, outputs_2 = synthetic_nl(\n",
    "        n_x=n_x,\n",
    "        n_y=n_y,\n",
    "        n_tot=n_tot,\n",
    "        n_obs=n_obs,\n",
    "        split=split,\n",
    "        set_seed=set_seed,\n",
    "    )\n",
    "    pd.testing.assert_frame_equal(\n",
    "        features_1.data, features_2.data, \"Feature data mismatch for reproducibility.\"\n",
    "    )\n",
    "    pd.testing.assert_frame_equal(\n",
    "        outputs_1.data, outputs_2.data, \"Output data mismatch for reproducibility.\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_synthetic_nl()\n",
    "    print(\"All tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed successfully.\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "# Generate non-linear synthetic data\n",
    "####################################################################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Tuple, Optional\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "import pandas_datareader as pdr\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "\n",
    "\n",
    "# Function to generate synthetic data using a 3-layer neural network\n",
    "def synthetic_NN(\n",
    "    n_x: int = 5,\n",
    "    n_y: int = 10,\n",
    "    n_tot: int = 1200,\n",
    "    n_obs: int = 104,\n",
    "    split: List[float] = [0.6, 0.4],\n",
    "    set_seed: int = 45678,\n",
    ") -> Tuple[\"TrainTest\", \"TrainTest\"]:\n",
    "    \"\"\"\n",
    "    Generates synthetic (normally-distributed) factor data and mixes them using a 3-layer neural network.\n",
    "\n",
    "    :param n_x: Number of features.\n",
    "    :param n_y: Number of assets.\n",
    "    :param n_tot: Number of observations in the whole dataset.\n",
    "    :param n_obs: Number of observations per batch.\n",
    "    :param split: List of floats representing train-validation-test split percentages (must sum up to one).\n",
    "    :param set_seed: Integer seed for reproducibility.\n",
    "    :return: Tuple of TrainTest objects for feature and asset data.\n",
    "    \"\"\"\n",
    "    np.random.seed(set_seed)  # Set random seed for reproducibility\n",
    "\n",
    "    # Generate synthetic features (n_tot samples with n_x features each)\n",
    "    X: np.ndarray = (\n",
    "        np.random.randn(n_tot, n_x) * 10 + 0.5\n",
    "    )  # Scale and shift features to have varied range\n",
    "\n",
    "    # Initialize neural network\n",
    "    synth: synthetic3layer = synthetic3layer(\n",
    "        n_x, n_y, set_seed\n",
    "    ).double()  # Create an instance of the 3-layer neural network\n",
    "\n",
    "    # Generate synthetic outputs using the neural network\n",
    "    Y: torch.Tensor = synth(\n",
    "        torch.from_numpy(X)\n",
    "    )  # Convert X to a torch tensor and pass it through the neural network\n",
    "\n",
    "    # Convert synthetic features and outputs to pandas DataFrames\n",
    "    X_df: pd.DataFrame = pd.DataFrame(X)  # Convert features to DataFrame\n",
    "    Y_df: pd.DataFrame = (\n",
    "        pd.DataFrame(Y.detach().numpy()) / 10\n",
    "    )  # Convert outputs to DataFrame and scale down\n",
    "\n",
    "    # Return TrainTest objects for features and outputs\n",
    "    return TrainTest(X_df, n_obs, split), TrainTest(Y_df, n_obs, split)\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Neural Network Module\n",
    "####################################################################################################\n",
    "class synthetic3layer(nn.Module):\n",
    "    def __init__(self, n_x: int, n_y: int, set_seed: int) -> None:\n",
    "        \"\"\"\n",
    "        Initialize a 3-layer neural network to synthesize data.\n",
    "\n",
    "        :param n_x: Number of input features.\n",
    "        :param n_y: Number of output features.\n",
    "        :param set_seed: Integer seed for reproducibility.\n",
    "        \"\"\"\n",
    "        super().__init__()  # Call the parent class (nn.Module) initializer\n",
    "        torch.manual_seed(\n",
    "            set_seed\n",
    "        )  # Set random seed for torch to ensure reproducible weights\n",
    "\n",
    "        # Define a neural network with 3 hidden layers\n",
    "        self.pred_layer: nn.Sequential = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                n_x, int(0.5 * (n_x + n_y))\n",
    "            ),  # First linear layer to project input features to an intermediate size\n",
    "            nn.ReLU(),  # ReLU activation function to introduce non-linearity\n",
    "            nn.Linear(\n",
    "                int(0.5 * (n_x + n_y)), int(0.6 * (n_x + n_y))\n",
    "            ),  # Second linear layer to another intermediate size\n",
    "            nn.ReLU(),  # ReLU activation function\n",
    "            nn.Linear(\n",
    "                int(0.6 * (n_x + n_y)), n_y\n",
    "            ),  # Third linear layer to project to output size\n",
    "            nn.ReLU(),  # ReLU activation function\n",
    "            nn.Linear(n_y, n_y),  # Final linear layer to produce the final outputs\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the neural network to generate synthetic outputs.\n",
    "\n",
    "        :param X: Features. (n_obs x n_x) torch tensor with feature time series data.\n",
    "        :return: Synthetically generated output. (n_obs x n_y) torch tensor of outputs.\n",
    "        \"\"\"\n",
    "        # Apply the prediction layer to each input tensor in the batch and stack the results\n",
    "        return torch.stack([self.pred_layer(x_t) for x_t in X])\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Test code to detect bugs\n",
    "####################################################################################################\n",
    "def test_synthetic_nn() -> None:\n",
    "    try:\n",
    "        # Define parameters for the test\n",
    "        n_x = 5\n",
    "        n_y = 10\n",
    "        n_tot = 1200\n",
    "        n_obs = 104\n",
    "        split = [0.6, 0.4]\n",
    "        set_seed = 45678\n",
    "\n",
    "        # Generate synthetic data\n",
    "        features, outputs = synthetic_NN(n_x, n_y, n_tot, n_obs, split, set_seed)\n",
    "\n",
    "        # Check if generated features and outputs are pandas DataFrames\n",
    "        assert isinstance(\n",
    "            features.data, pd.DataFrame\n",
    "        ), \"Features should be a pandas DataFrame.\"\n",
    "        assert isinstance(\n",
    "            outputs.data, pd.DataFrame\n",
    "        ), \"Outputs should be a pandas DataFrame.\"\n",
    "\n",
    "        # Check the dimensions of the generated data\n",
    "        assert features.data.shape == (\n",
    "            n_tot,\n",
    "            n_x,\n",
    "        ), f\"Expected features shape {(n_tot, n_x)}, but got {features.data.shape}.\"\n",
    "        assert outputs.data.shape == (\n",
    "            n_tot,\n",
    "            n_y,\n",
    "        ), f\"Expected outputs shape {(n_tot, n_y)}, but got {outputs.data.shape}.\"\n",
    "\n",
    "        # Check reproducibility\n",
    "        features_1, outputs_1 = synthetic_NN(n_x, n_y, n_tot, n_obs, split, set_seed)\n",
    "        features_2, outputs_2 = synthetic_NN(n_x, n_y, n_tot, n_obs, split, set_seed)\n",
    "        pd.testing.assert_frame_equal(\n",
    "            features_1.data,\n",
    "            features_2.data,\n",
    "            \"Feature data mismatch for reproducibility.\",\n",
    "        )\n",
    "        pd.testing.assert_frame_equal(\n",
    "            outputs_1.data, outputs_2.data, \"Output data mismatch for reproducibility.\"\n",
    "        )\n",
    "\n",
    "        print(\"All tests passed successfully.\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test failed: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Run the test function\n",
    "if __name__ == \"__main__\":\n",
    "    test_synthetic_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# base_mod: CvxpyLayer that declares the portfolio optimization problem\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "def base_mod(n_y: int, n_obs: int, prisk: Callable) -> CvxpyLayer:\n",
    "    \"\"\"\n",
    "    Base optimization problem declared as a CvxpyLayer object.\n",
    "\n",
    "    :param n_y: Number of assets.\n",
    "    :param n_obs: Number of scenarios in the dataset.\n",
    "    :param prisk: Portfolio risk function.\n",
    "    :return: CvxpyLayer representing the optimization layer.\n",
    "    \"\"\"\n",
    "    # Variables\n",
    "    z: cp.Variable = cp.Variable((n_y, 1), nonneg=True)  # Portfolio weights (long-only positions)\n",
    "\n",
    "    # Parameters\n",
    "    y_hat: cp.Parameter = cp.Parameter(n_y)  # Predicted outcomes (e.g., expected returns)\n",
    "    \n",
    "    # Constraints\n",
    "    constraints: list[cp.Constraint] = [\n",
    "        cp.sum(z) == 1  # Budget constraint: sum of weights equals 1\n",
    "    ]\n",
    "\n",
    "    # Objective function\n",
    "    objective: cp.Minimize = cp.Minimize(-y_hat @ z)  # Maximize returns by minimizing negative expected returns\n",
    "\n",
    "    # Construct optimization problem and differentiable layer\n",
    "    problem: cp.Problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    return CvxpyLayer(problem, parameters=[y_hat], variables=[z])\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# nominal: CvxpyLayer that declares the nominal portfolio optimization problem\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "def nominal(n_y: int, n_obs: int, prisk: Callable) -> CvxpyLayer:\n",
    "    \"\"\"\n",
    "    Nominal optimization problem declared as a CvxpyLayer object.\n",
    "\n",
    "    :param n_y: Number of assets.\n",
    "    :param n_obs: Number of scenarios in the dataset.\n",
    "    :param prisk: Portfolio risk function.\n",
    "    :return: CvxpyLayer representing the optimization layer.\n",
    "    \"\"\"\n",
    "    # Variables\n",
    "    z: cp.Variable = cp.Variable((n_y, 1), nonneg=True)  # Portfolio weights (long-only positions)\n",
    "    c_aux: cp.Variable = cp.Variable()  # Auxiliary variable for risk calculation\n",
    "    obj_aux: cp.Variable = cp.Variable(n_obs)  # Objective auxiliary variable for each scenario\n",
    "    mu_aux: cp.Variable = cp.Variable()  # Expected portfolio return\n",
    "\n",
    "    # Parameters\n",
    "    ep: cp.Parameter = cp.Parameter((n_obs, n_y))  # Scenario matrix of residuals\n",
    "    y_hat: cp.Parameter = cp.Parameter(n_y)  # Predicted outcomes (e.g., expected returns)\n",
    "    gamma: cp.Parameter = cp.Parameter(nonneg=True)  # Risk aversion coefficient\n",
    "    \n",
    "    # Constraints\n",
    "    constraints: list[cp.Constraint] = [\n",
    "        cp.sum(z) == 1,  # Budget constraint: sum of weights equals 1\n",
    "        mu_aux == y_hat @ z  # Calculate expected return\n",
    "    ]\n",
    "    for i in range(n_obs):\n",
    "        constraints.append(obj_aux[i] >= prisk(z, c_aux, ep[i]))  # Add risk constraints for each scenario\n",
    "\n",
    "    # Objective function\n",
    "    objective: cp.Minimize = cp.Minimize((1 / n_obs) * cp.sum(obj_aux) - gamma * mu_aux)  # Minimize risk-adjusted return\n",
    "\n",
    "    # Construct optimization problem and differentiable layer\n",
    "    problem: cp.Problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    return CvxpyLayer(problem, parameters=[ep, y_hat, gamma], variables=[z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An unexpected error occurred: Thank you for using Alpha Vantage! This is a premium endpoint. You may subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly unlock all premium endpoints\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import pandas_datareader.data as pdr\n",
    "\n",
    "####################################################################################################\n",
    "# Option 4: Factors from Kenneth French's data library and asset data from AlphaVantage\n",
    "# https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html\n",
    "# https://www.alphavantage.co\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def AV(\n",
    "    start: str,\n",
    "    end: str,\n",
    "    split: list[float],\n",
    "    freq: str = \"weekly\",\n",
    "    n_obs: int = 104,\n",
    "    n_y: int | None = None,\n",
    "    use_cache: bool = False,\n",
    "    save_results: bool = False,\n",
    "    AV_key: str | None = \"YDNA9HH8P2IW985M\",\n",
    ") -> tuple[TrainTest, TrainTest]:\n",
    "    \"\"\"\n",
    "    Load data from Kenneth French's data library and from AlphaVantage.\n",
    "\n",
    "    :param start: Start date of time series.\n",
    "    :param end: End date of time series.\n",
    "    :param split: List of floats representing train-validation-test split percentages.\n",
    "    :param freq: Data frequency (daily, weekly, monthly). Default is 'weekly'.\n",
    "    :param n_obs: Number of observations per batch. Default is 104.\n",
    "    :param n_y: Number of features to select. If None, the maximum number (8) is used. Default is None.\n",
    "    :param use_cache: Whether to load cached data or download new data. Default is False.\n",
    "    :param save_results: Whether to save the data for future use. Default is False.\n",
    "    :param AV_key: AlphaVantage API key for accessing their data. Default is 'YDNA9HH8P2IW985M'.\n",
    "    :return: Tuple containing TrainTest objects for features and asset data.\n",
    "    \"\"\"\n",
    "    if use_cache:\n",
    "        # Load cached data\n",
    "        X: pd.DataFrame = pd.read_pickle(f\"./cache/factor_{freq}.pkl\")\n",
    "        Y: pd.DataFrame = pd.read_pickle(f\"./cache/asset_{freq}.pkl\")\n",
    "    else:\n",
    "        # Define list of tickers to be downloaded\n",
    "        tick_list: list[str] = [\n",
    "            \"AAPL\",\n",
    "            \"MSFT\",\n",
    "            \"AMZN\",\n",
    "            \"C\",\n",
    "            \"JPM\",\n",
    "            \"BAC\",\n",
    "            \"XOM\",\n",
    "            \"HAL\",\n",
    "            \"MCD\",\n",
    "            \"WMT\",\n",
    "            \"COST\",\n",
    "            \"CAT\",\n",
    "            \"LMT\",\n",
    "            \"JNJ\",\n",
    "            \"PFE\",\n",
    "            \"DIS\",\n",
    "            \"VZ\",\n",
    "            \"T\",\n",
    "            \"ED\",\n",
    "            \"NEM\",\n",
    "        ]\n",
    "\n",
    "        # Select the first n_y tickers if specified\n",
    "        if n_y is not None:\n",
    "            tick_list = tick_list[:n_y]\n",
    "\n",
    "        # Ensure API key is provided\n",
    "        if AV_key is None:\n",
    "            print(\n",
    "                \"\"\"A personal AlphaVantage API key is required to load the asset pricing data.\n",
    "                If you do not have a key, you can get one from www.alphavantage.co (free for academic users)\"\"\"\n",
    "            )\n",
    "            AV_key = input(\"Enter your AlphaVantage API key: \")\n",
    "\n",
    "        # Initialize TimeSeries object to interact with AlphaVantage API\n",
    "        ts: TimeSeries = TimeSeries(\n",
    "            key=AV_key, output_format=\"pandas\", indexing_type=\"date\"\n",
    "        )\n",
    "\n",
    "        # Download asset data from AlphaVantage\n",
    "        Y_list: list[pd.Series] = []\n",
    "        for tick in tick_list:\n",
    "            data, _ = ts.get_daily_adjusted(symbol=tick, outputsize=\"full\")\n",
    "            data = data[\"5. adjusted close\"]\n",
    "            Y_list.append(data)\n",
    "            time.sleep(12.5)  # AlphaVantage rate limit to avoid API ban\n",
    "        Y: pd.DataFrame = pd.concat(Y_list, axis=1)\n",
    "        Y = Y[::-1]\n",
    "        Y = Y.loc[\"1999-01-01\":end].pct_change()\n",
    "        Y = Y.loc[start:end]\n",
    "        Y.columns = tick_list\n",
    "\n",
    "        # Download factor data from Kenneth French's library\n",
    "        dl_freq: str = \"_daily\"\n",
    "        X: pd.DataFrame = pdr.get_data_famafrench(\n",
    "            f\"F-F_Research_Data_5_Factors_2x3{dl_freq}\", start=start, end=end\n",
    "        )[0]\n",
    "        rf_df: pd.Series = X[\"RF\"]\n",
    "        X = X.drop([\"RF\"], axis=1)\n",
    "        mom_df: pd.DataFrame = pdr.get_data_famafrench(\n",
    "            f\"F-F_Momentum_Factor{dl_freq}\", start=start, end=end\n",
    "        )[0]\n",
    "        st_df: pd.DataFrame = pdr.get_data_famafrench(\n",
    "            f\"F-F_ST_Reversal_Factor{dl_freq}\", start=start, end=end\n",
    "        )[0]\n",
    "        lt_df: pd.DataFrame = pdr.get_data_famafrench(\n",
    "            f\"F-F_LT_Reversal_Factor{dl_freq}\", start=start, end=end\n",
    "        )[0]\n",
    "\n",
    "        # Concatenate all factors into a single DataFrame\n",
    "        X = pd.concat([X, mom_df, st_df, lt_df], axis=1) / 100\n",
    "\n",
    "        # Convert daily returns to weekly returns if specified\n",
    "        if freq in [\"weekly\", \"_weekly\"]:\n",
    "            Y = Y.resample(\"W-FRI\").agg(lambda x: (x + 1).prod() - 1)\n",
    "            X = X.resample(\"W-FRI\").agg(lambda x: (x + 1).prod() - 1)\n",
    "\n",
    "        # Save the data if requested\n",
    "        if save_results:\n",
    "            X.to_pickle(f\"./cache/factor_{freq}.pkl\")\n",
    "            Y.to_pickle(f\"./cache/asset_{freq}.pkl\")\n",
    "\n",
    "    # Partition dataset into training and testing sets. Lag the data by one observation\n",
    "    return TrainTest(X[:-1], n_obs, split), TrainTest(Y[1:], n_obs, split)\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Test code to verify functionality and detect bugs\n",
    "####################################################################################################\n",
    "def test_AV() -> None:\n",
    "    \"\"\"\n",
    "    Test the AV function to ensure correctness of generated data.\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define parameters for the test\n",
    "        start = \"2020-01-01\"\n",
    "        end = \"2023-01-01\"\n",
    "        split = [0.7, 0.3]\n",
    "        freq = \"weekly\"\n",
    "        n_obs = 104\n",
    "        n_y = 5\n",
    "        use_cache = False\n",
    "        save_results = False\n",
    "        AV_key = \"YDNA9HH8P2IW985M\"\n",
    "\n",
    "        # Generate synthetic data\n",
    "        features, outputs = AV(\n",
    "            start, end, split, freq, n_obs, n_y, use_cache, save_results, AV_key\n",
    "        )\n",
    "\n",
    "        # Check if generated features and outputs are pandas DataFrames\n",
    "        assert isinstance(\n",
    "            features.data, pd.DataFrame\n",
    "        ), \"Features should be a pandas DataFrame.\"\n",
    "        assert isinstance(\n",
    "            outputs.data, pd.DataFrame\n",
    "        ), \"Outputs should be a pandas DataFrame.\"\n",
    "\n",
    "        # Check the dimensions of the generated data\n",
    "        assert (\n",
    "            features.data.shape[0] == outputs.data.shape[0]\n",
    "        ), \"Features and outputs should have matching number of rows.\"\n",
    "\n",
    "        print(\"All tests passed successfully.\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test failed: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Run the test function\n",
    "if __name__ == \"__main__\":\n",
    "    test_AV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Statistical Analysis Function\n",
    "####################################################################################################\n",
    "def statanalysis(X: pd.DataFrame, Y: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Conduct a pairwise statistical significance analysis of each feature in X against each asset in Y.\n",
    "\n",
    "    :param X: DataFrame containing the time series of features (independent variables).\n",
    "    :param Y: DataFrame containing the time series of asset returns (dependent variables).\n",
    "    :return: DataFrame containing p-values obtained from regressing each individual feature against each individual asset.\n",
    "    \"\"\"\n",
    "    # Initialize an empty DataFrame to store p-values\n",
    "    stats: pd.DataFrame = pd.DataFrame(columns=X.columns, index=Y.columns)\n",
    "\n",
    "    # Iterate over each asset (Y) and each feature (X) to perform OLS regression\n",
    "    # OLS (Ordinary Least Squares) is used to estimate the relationship between each feature and asset return.\n",
    "    # For each asset, regress it against each feature with a constant term to obtain the p-value.\n",
    "    for ticker in Y.columns:\n",
    "        for feature in X.columns:\n",
    "            # Perform OLS regression and store the p-value of the feature\n",
    "            stats.loc[ticker, feature] = (\n",
    "                sm.OLS(Y[ticker].values, sm.add_constant(X[feature]).values)\n",
    "                .fit()\n",
    "                .pvalues[1]\n",
    "            )\n",
    "\n",
    "    # Convert p-values to float and round to two decimal places\n",
    "    return stats.astype(float).round(2)\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Test code to verify functionality and avoid bugs\n",
    "####################################################################################################\n",
    "def test_statanalysis() -> None:\n",
    "    \"\"\"\n",
    "    Test the statanalysis function to ensure it correctly calculates p-values for feature-asset relationships.\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate random data for testing\n",
    "        np.random.seed(42)\n",
    "        X_test = pd.DataFrame(\n",
    "            np.random.randn(100, 5), columns=[f\"Feature_{i+1}\" for i in range(5)]\n",
    "        )\n",
    "        Y_test = pd.DataFrame(\n",
    "            np.random.randn(100, 3), columns=[f\"Asset_{i+1}\" for i in range(3)]\n",
    "        )\n",
    "\n",
    "        # Run the statistical analysis\n",
    "        stats_result = statanalysis(X_test, Y_test)\n",
    "\n",
    "        # Check if the result is a DataFrame\n",
    "        assert isinstance(\n",
    "            stats_result, pd.DataFrame\n",
    "        ), \"The result should be a pandas DataFrame.\"\n",
    "\n",
    "        # Check if the dimensions of the result match expectations\n",
    "        assert stats_result.shape == (\n",
    "            3,\n",
    "            5,\n",
    "        ), f\"Expected shape (3, 5), but got {stats_result.shape}.\"\n",
    "\n",
    "        # Check if all p-values are between 0 and 1\n",
    "        assert (\n",
    "            stats_result.map(lambda x: 0 <= x <= 1).all().all()\n",
    "        ), \"All p-values should be between 0 and 1.\"\n",
    "\n",
    "        print(\"All tests passed successfully.\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test failed: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Run the test function\n",
    "if __name__ == \"__main__\":\n",
    "    test_statanalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "num_cores: int = (\n",
    "    psutil.cpu_count()\n",
    ")  # Get the number of CPU cores available in the system.\n",
    "torch.set_num_threads(\n",
    "    num_cores\n",
    ")  # Set the number of threads for PyTorch to utilize all CPU cores.\n",
    "if psutil.MACOS:\n",
    "    num_cores = 0  # Set num_cores to 0 on macOS systems (specific behavior).\n",
    "\n",
    "num_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# base_mod: CvxpyLayer that declares the portfolio optimization problem\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "def base_mod(n_y: int, n_obs: int, prisk: Callable) -> CvxpyLayer:\n",
    "    \"\"\"\n",
    "    Base optimization problem declared as a CvxpyLayer object.\n",
    "\n",
    "    :param n_y: Number of assets.\n",
    "    :param n_obs: Number of scenarios in the dataset.\n",
    "    :param prisk: Portfolio risk function.\n",
    "    :return: CvxpyLayer representing the optimization layer.\n",
    "    \"\"\"\n",
    "    # Variables\n",
    "    z: cp.Variable = cp.Variable(\n",
    "        (n_y, 1), nonneg=True\n",
    "    )  # Portfolio weights (long-only positions)\n",
    "\n",
    "    # Parameters\n",
    "    y_hat: cp.Parameter = cp.Parameter(\n",
    "        n_y\n",
    "    )  # Predicted outcomes (e.g., expected returns)\n",
    "\n",
    "    # Constraints\n",
    "    constraints: list[cp.Constraint] = [\n",
    "        cp.sum(z) == 1  # Budget constraint: sum of weights equals 1\n",
    "    ]\n",
    "\n",
    "    # Objective function\n",
    "    objective: cp.Minimize = cp.Minimize(\n",
    "        -y_hat @ z\n",
    "    )  # Maximize returns by minimizing negative expected returns\n",
    "\n",
    "    # Construct optimization problem and differentiable layer\n",
    "    problem: cp.Problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    return CvxpyLayer(problem, parameters=[y_hat], variables=[z])\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# nominal: CvxpyLayer that declares the nominal portfolio optimization problem\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "def nominal(n_y: int, n_obs: int, prisk: Callable) -> CvxpyLayer:\n",
    "    \"\"\"\n",
    "    Nominal optimization problem declared as a CvxpyLayer object.\n",
    "\n",
    "    :param n_y: Number of assets.\n",
    "    :param n_obs: Number of scenarios in the dataset.\n",
    "    :param prisk: Portfolio risk function.\n",
    "    :return: CvxpyLayer representing the optimization layer.\n",
    "    \"\"\"\n",
    "    # Variables\n",
    "    z: cp.Variable = cp.Variable(\n",
    "        (n_y, 1), nonneg=True\n",
    "    )  # Portfolio weights (long-only positions)\n",
    "    c_aux: cp.Variable = cp.Variable()  # Auxiliary variable for risk calculation\n",
    "    obj_aux: cp.Variable = cp.Variable(\n",
    "        n_obs\n",
    "    )  # Objective auxiliary variable for each scenario\n",
    "    mu_aux: cp.Variable = cp.Variable()  # Expected portfolio return\n",
    "\n",
    "    # Parameters\n",
    "    ep: cp.Parameter = cp.Parameter((n_obs, n_y))  # Scenario matrix of residuals\n",
    "    y_hat: cp.Parameter = cp.Parameter(\n",
    "        n_y\n",
    "    )  # Predicted outcomes (e.g., expected returns)\n",
    "    gamma: cp.Parameter = cp.Parameter(nonneg=True)  # Risk aversion coefficient\n",
    "\n",
    "    # Constraints\n",
    "    constraints: list[cp.Constraint] = [\n",
    "        cp.sum(z) == 1,  # Budget constraint: sum of weights equals 1\n",
    "        mu_aux == y_hat @ z,  # Calculate expected return\n",
    "    ]\n",
    "    for i in range(n_obs):\n",
    "        constraints.append(\n",
    "            obj_aux[i] >= prisk(z, c_aux, ep[i])\n",
    "        )  # Add risk constraints for each scenario\n",
    "\n",
    "    # Objective function\n",
    "    objective: cp.Minimize = cp.Minimize(\n",
    "        (1 / n_obs) * cp.sum(obj_aux) - gamma * mu_aux\n",
    "    )  # Minimize risk-adjusted return\n",
    "\n",
    "    # Construct optimization problem and differentiable layer\n",
    "    problem: cp.Problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    return CvxpyLayer(problem, parameters=[ep, y_hat, gamma], variables=[z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Total Variation: sum_t abs(p_t - q_t) <= delta\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "def tv(n_y: int, n_obs: int, prisk: Callable) -> CvxpyLayer:\n",
    "    \"\"\"\n",
    "    DRO layer using the 'Total Variation' distance to define the probability ambiguity set.\n",
    "    From Ben-Tal et al. (2013).\n",
    "\n",
    "    :param n_y: Number of assets.\n",
    "    :param n_obs: Number of scenarios in the dataset.\n",
    "    :param prisk: Portfolio risk function.\n",
    "    :return: CvxpyLayer representing the optimization layer.\n",
    "    \"\"\"\n",
    "    # Variables\n",
    "    z: cp.Variable = cp.Variable(\n",
    "        (n_y, 1), nonneg=True\n",
    "    )  # Decision variable representing portfolio weights with nonnegative constraint.\n",
    "    c_aux: cp.Variable = (\n",
    "        cp.Variable()\n",
    "    )  # Auxiliary variable used in risk function calculation.\n",
    "    lambda_aux: cp.Variable = cp.Variable(\n",
    "        nonneg=True\n",
    "    )  # Auxiliary variable representing Lagrange multiplier for the Total Variation constraint (nonnegative).\n",
    "    eta_aux: cp.Variable = (\n",
    "        cp.Variable()\n",
    "    )  # Auxiliary variable representing an additional term in the optimization.\n",
    "    beta_aux: cp.Variable = cp.Variable(\n",
    "        n_obs\n",
    "    )  # Auxiliary variable representing scenario-specific values for optimization.\n",
    "    mu_aux: cp.Variable = (\n",
    "        cp.Variable()\n",
    "    )  # Auxiliary variable representing the expected return of the portfolio.\n",
    "\n",
    "    # Parameters\n",
    "    ep: cp.Parameter = cp.Parameter(\n",
    "        (n_obs, n_y)\n",
    "    )  # Parameter representing the different scenarios for asset returns.\n",
    "    y_hat: cp.Parameter = cp.Parameter(\n",
    "        n_y\n",
    "    )  # Parameter representing the expected returns for each asset.\n",
    "    gamma: cp.Parameter = cp.Parameter(\n",
    "        nonneg=True\n",
    "    )  # Parameter representing the risk-aversion coefficient (nonnegative).\n",
    "    delta: cp.Parameter = cp.Parameter(\n",
    "        nonneg=True\n",
    "    )  # Parameter representing the size of the ambiguity set for Total Variation (nonnegative).\n",
    "\n",
    "    # Constraints\n",
    "    constraints: list[cp.Constraint] = [\n",
    "        cp.sum(z) == 1,\n",
    "        beta_aux >= -lambda_aux,\n",
    "        mu_aux == y_hat @ z,\n",
    "    ]  # Constraints for portfolio weights, auxiliary variables, and expected return.\n",
    "    for i in range(n_obs):\n",
    "        constraints.append(\n",
    "            beta_aux[i] >= prisk(z, c_aux, ep[i]) - eta_aux\n",
    "        )  # Constraint to enforce risk bounds for each scenario.\n",
    "        constraints.append(\n",
    "            lambda_aux >= prisk(z, c_aux, ep[i]) - eta_aux\n",
    "        )  # Constraint for Lagrange multiplier to handle variation.\n",
    "\n",
    "    # Objective function\n",
    "    objective: cp.Minimize = cp.Minimize(\n",
    "        eta_aux + delta * lambda_aux + (1 / n_obs) * cp.sum(beta_aux) - gamma * mu_aux\n",
    "    )  # Objective to minimize the sum of risk terms and maximize expected return.\n",
    "\n",
    "    # Construct optimization problem and differentiable layer\n",
    "    problem: cp.Problem = cp.Problem(\n",
    "        objective, constraints\n",
    "    )  # Define the optimization problem with the objective and constraints.\n",
    "\n",
    "    return CvxpyLayer(\n",
    "        problem, parameters=[ep, y_hat, gamma, delta], variables=[z]\n",
    "    )  # Return the CvxpyLayer that represents the optimization problem.\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Hellinger distance: sum_t (sqrt(p_t) - sqrt(q_t))^2 <= delta\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "def hellinger(n_y: int, n_obs: int, prisk: Callable) -> CvxpyLayer:\n",
    "    \"\"\"\n",
    "    DRO layer using the Hellinger distance to define the probability ambiguity set.\n",
    "    From Ben-Tal et al. (2013).\n",
    "\n",
    "    :param n_y: Number of assets.\n",
    "    :param n_obs: Number of scenarios in the dataset.\n",
    "    :param prisk: Portfolio risk function.\n",
    "    :return: CvxpyLayer representing the optimization layer.\n",
    "    \"\"\"\n",
    "    # Variables\n",
    "    z: cp.Variable = cp.Variable(\n",
    "        (n_y, 1), nonneg=True\n",
    "    )  # Decision variable representing portfolio weights with nonnegative constraint.\n",
    "    c_aux: cp.Variable = (\n",
    "        cp.Variable()\n",
    "    )  # Auxiliary variable used in risk function calculation.\n",
    "    lambda_aux: cp.Variable = cp.Variable(\n",
    "        nonneg=True\n",
    "    )  # Auxiliary variable representing Lagrange multiplier for Hellinger constraint (nonnegative).\n",
    "    xi_aux: cp.Variable = (\n",
    "        cp.Variable()\n",
    "    )  # Auxiliary variable representing an additional term in the optimization.\n",
    "    beta_aux: cp.Variable = cp.Variable(\n",
    "        n_obs, nonneg=True\n",
    "    )  # Scenario-specific auxiliary variable for optimization with nonnegative constraint.\n",
    "    tau_aux: cp.Variable = cp.Variable(\n",
    "        n_obs, nonneg=True\n",
    "    )  # Auxiliary variable for scenario-specific terms, nonnegative.\n",
    "    mu_aux: cp.Variable = (\n",
    "        cp.Variable()\n",
    "    )  # Auxiliary variable representing the expected return of the portfolio.\n",
    "\n",
    "    # Parameters\n",
    "    ep: cp.Parameter = cp.Parameter(\n",
    "        (n_obs, n_y)\n",
    "    )  # Parameter representing the different scenarios for asset returns.\n",
    "    y_hat: cp.Parameter = cp.Parameter(\n",
    "        n_y\n",
    "    )  # Parameter representing the expected returns for each asset.\n",
    "    gamma: cp.Parameter = cp.Parameter(\n",
    "        nonneg=True\n",
    "    )  # Parameter representing the risk-aversion coefficient (nonnegative).\n",
    "    delta: cp.Parameter = cp.Parameter(\n",
    "        nonneg=True\n",
    "    )  # Parameter representing the size of the ambiguity set for Hellinger distance (nonnegative).\n",
    "\n",
    "    # Constraints\n",
    "    constraints: list[cp.Constraint] = [\n",
    "        cp.sum(z) == 1,\n",
    "        mu_aux == y_hat @ z,\n",
    "    ]  # Constraints to ensure portfolio weights sum to 1 and compute expected return.\n",
    "    for i in range(n_obs):\n",
    "        constraints.append(\n",
    "            xi_aux + lambda_aux >= prisk(z, c_aux, ep[i]) + tau_aux[i]\n",
    "        )  # Constraint to handle Hellinger distance using auxiliary variables.\n",
    "        constraints.append(\n",
    "            beta_aux[i] >= cp.quad_over_lin(lambda_aux, tau_aux[i])\n",
    "        )  # Quadratic constraint for scenario-specific optimization.\n",
    "\n",
    "    # Objective function\n",
    "    objective: cp.Minimize = cp.Minimize(\n",
    "        xi_aux\n",
    "        + (delta - 1) * lambda_aux\n",
    "        + (1 / n_obs) * cp.sum(beta_aux)\n",
    "        - gamma * mu_aux\n",
    "    )  # Objective to minimize the sum of risk terms and maximize expected return.\n",
    "\n",
    "    # Construct optimization problem and differentiable layer\n",
    "    problem: cp.Problem = cp.Problem(\n",
    "        objective, constraints\n",
    "    )  # Define the optimization problem with the objective and constraints.\n",
    "\n",
    "    return CvxpyLayer(\n",
    "        problem, parameters=[ep, y_hat, gamma, delta], variables=[z]\n",
    "    )  # Return the CvxpyLayer that represents the optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# E2E neural network module\n",
    "####################################################################################################\n",
    "class e2e_net(nn.Module):\n",
    "    \"\"\"End-to-end DRO learning neural net module.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_x: int,\n",
    "        n_y: int,\n",
    "        n_obs: int,\n",
    "        opt_layer: str = 'nominal',\n",
    "        prisk: str = 'p_var',\n",
    "        perf_loss: str = 'sharpe_loss',\n",
    "        pred_model: str = 'linear',\n",
    "        pred_loss_factor: Optional[float] = 0.5,\n",
    "        perf_period: int = 13,\n",
    "        train_pred: bool = True,\n",
    "        train_gamma: bool = True,\n",
    "        train_delta: bool = True,\n",
    "        set_seed: Optional[int] = None,\n",
    "        cache_path: str = './cache/'\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the end-to-end learning neural net module.\n",
    "\n",
    "        :param n_x: Number of input features in the prediction model.\n",
    "        :param n_y: Number of outputs from the prediction model.\n",
    "        :param n_obs: Number of scenarios for residuals calculation.\n",
    "        :param opt_layer: Optimization layer type ('nominal', 'base_mod', etc.).\n",
    "        :param prisk: Portfolio risk function used in the optimization layer.\n",
    "        :param perf_loss: Performance loss function.\n",
    "        :param pred_model: Type of prediction model ('linear', '2layer', '3layer').\n",
    "        :param pred_loss_factor: Trade-off between prediction loss and performance loss.\n",
    "        :param perf_period: Number of lookahead realizations for performance loss.\n",
    "        :param train_pred: Whether to train the prediction layer.\n",
    "        :param train_gamma: Whether to train the risk appetite parameter gamma.\n",
    "        :param train_delta: Whether to train the robustness parameter delta.\n",
    "        :param set_seed: Random seed for replicability.\n",
    "        :param cache_path: Path to cache model data.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Set random seed for replicability\n",
    "        if set_seed is not None:\n",
    "            torch.manual_seed(set_seed)\n",
    "            self.seed: Optional[int] = set_seed\n",
    "\n",
    "        self.n_x: int = n_x\n",
    "        self.n_y: int = n_y\n",
    "        self.n_obs: int = n_obs\n",
    "\n",
    "        # Prediction loss function\n",
    "        if pred_loss_factor is not None:\n",
    "            self.pred_loss_factor: float = pred_loss_factor\n",
    "            self.pred_loss: Optional[nn.Module] = torch.nn.MSELoss()\n",
    "        else:\n",
    "            self.pred_loss: Optional[nn.Module] = None\n",
    "\n",
    "        # Define performance loss\n",
    "        #self.perf_loss: Callable = eval('lf.' + perf_loss)\n",
    "        if perf_loss == 'sharpe_loss':\n",
    "            self.perf_loss: Callable = lf.sharpe_loss\n",
    "        elif perf_loss == 'sortino_loss':\n",
    "            self.perf_loss = lf.sortino_loss\n",
    "        self.perf_period: int = perf_period\n",
    "\n",
    "        # Register 'gamma' (risk-return trade-off parameter)\n",
    "        self.gamma: nn.Parameter = nn.Parameter(torch.FloatTensor(1).uniform_(0.02, 0.1))\n",
    "        self.gamma.requires_grad = train_gamma\n",
    "        self.gamma_init: float = self.gamma.item()\n",
    "\n",
    "        # Determine model type and optionally register 'delta'\n",
    "        if opt_layer == 'nominal':\n",
    "            self.model_type: str = 'nom'\n",
    "        elif opt_layer == 'base_mod':\n",
    "            self.gamma.requires_grad = False\n",
    "            self.model_type: str = 'base_mod'\n",
    "        else:\n",
    "            # Register 'delta' (ambiguity sizing parameter) for DRO layer\n",
    "            if opt_layer == 'hellinger':\n",
    "                ub: float = (1 - 1 / (n_obs**0.5)) / 2\n",
    "                lb: float = (1 - 1 / (n_obs**0.5)) / 10\n",
    "            else:\n",
    "                ub = (1 - 1 / n_obs) / 2\n",
    "                lb = (1 - 1 / n_obs) / 10\n",
    "            self.delta: nn.Parameter = nn.Parameter(torch.FloatTensor(1).uniform_(lb, ub))\n",
    "            self.delta.requires_grad = train_delta\n",
    "            self.delta_init: float = self.delta.item()\n",
    "            self.model_type = 'dro'\n",
    "\n",
    "        # Prediction model\n",
    "        self.pred_model: str = pred_model\n",
    "        if pred_model == 'linear':\n",
    "            self.pred_layer: nn.Module = nn.Linear(n_x, n_y)\n",
    "            self.pred_layer.weight.requires_grad = train_pred\n",
    "            self.pred_layer.bias.requires_grad = train_pred\n",
    "        elif pred_model == '2layer':\n",
    "            self.pred_layer = nn.Sequential(\n",
    "                nn.Linear(n_x, int(0.5 * (n_x + n_y))),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(int(0.5 * (n_x + n_y)), n_y),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_y, n_y),\n",
    "            )\n",
    "        elif pred_model == '3layer':\n",
    "            self.pred_layer = nn.Sequential(\n",
    "                nn.Linear(n_x, int(0.5 * (n_x + n_y))),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(int(0.5 * (n_x + n_y)), int(0.6 * (n_x + n_y))),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(int(0.6 * (n_x + n_y)), n_y),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_y, n_y),\n",
    "            )\n",
    "\n",
    "        # Optimization model\n",
    "        self.opt_layer: CvxpyLayer = eval(opt_layer)(n_y, n_obs, eval('rf.' + prisk))\n",
    "\n",
    "        # Cache path for storing model data\n",
    "        self.cache_path: str = cache_path\n",
    "\n",
    "        # Store initial model state\n",
    "        if train_gamma and train_delta:\n",
    "            self.init_state_path: str = f\"{cache_path}{self.model_type}_initial_state_{pred_model}\"\n",
    "        elif train_delta and not train_gamma:\n",
    "            self.init_state_path = f\"{cache_path}{self.model_type}_initial_state_{pred_model}_TrainGamma{train_gamma}\"\n",
    "        elif train_gamma and not train_delta:\n",
    "            self.init_state_path = f\"{cache_path}{self.model_type}_initial_state_{pred_model}_TrainDelta{train_delta}\"\n",
    "        else:\n",
    "            self.init_state_path = f\"{cache_path}{self.model_type}_initial_state_{pred_model}_TrainGamma{train_gamma}_TrainDelta{train_delta}\"\n",
    "        torch.save(self.state_dict(), self.init_state_path)\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    # forward: forward pass of the e2e neural net\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def forward(self, X, Y):\n",
    "        \"\"\"Forward pass of the NN module\n",
    "\n",
    "        The inputs 'X' are passed through the prediction layer to yield predictions 'Y_hat'. The\n",
    "        residuals from prediction are then calcuclated as 'ep = Y - Y_hat'. Finally, the residuals\n",
    "        are passed to the optimization layer to find the optimal decision z_star.\n",
    "\n",
    "        Inputs\n",
    "        X: Features. ([n_obs+1] x n_x) torch tensor with feature timeseries data\n",
    "        Y: Realizations. (n_obs x n_y) torch tensor with asset timeseries data\n",
    "\n",
    "        Other\n",
    "        ep: Residuals. (n_obs x n_y) matrix of the residual between realizations and predictions\n",
    "\n",
    "        Outputs\n",
    "        y_hat: Prediction. (n_y x 1) vector of outputs of the prediction layer\n",
    "        z_star: Optimal solution. (n_y x 1) vector of asset weights\n",
    "        \"\"\"\n",
    "        # Multiple predictions Y_hat from X\n",
    "        Y_hat = torch.stack([self.pred_layer(x_t) for x_t in X])\n",
    "\n",
    "        # Calculate residuals and process them\n",
    "        ep = Y - Y_hat[:-1]\n",
    "        y_hat = Y_hat[-1]\n",
    "\n",
    "        # Optimization solver arguments (from CVXPY for ECOS/SCS solver)\n",
    "        solver_args = {\"solve_method\": \"ECOS\", \"max_iters\": 120, \"abstol\": 1e-7}\n",
    "        # solver_args = {'solve_method': 'SCS', 'eps': 1e-7, 'acceleration_lookback': 5,\n",
    "        # 'max_iters':20000}\n",
    "\n",
    "        # Optimize z per scenario\n",
    "        # Determine whether nominal or dro model\n",
    "        if self.model_type == \"nom\":\n",
    "            (z_star,) = self.opt_layer(ep, y_hat, self.gamma, solver_args=solver_args)\n",
    "        elif self.model_type == \"dro\":\n",
    "            (z_star,) = self.opt_layer(\n",
    "                ep, y_hat, self.gamma, self.delta, solver_args=solver_args\n",
    "            )\n",
    "        elif self.model_type == \"base_mod\":\n",
    "            (z_star,) = self.opt_layer(y_hat, solver_args=solver_args)\n",
    "\n",
    "        return z_star, y_hat\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    # net_train: Train the e2e neural net\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def net_train(self, train_set, val_set=None, epochs=None, lr=None):\n",
    "        \"\"\"Neural net training module\n",
    "\n",
    "        Inputs\n",
    "        train_set: SlidingWindow object containing features x, realizations y and performance\n",
    "        realizations y_perf\n",
    "        val_set: SlidingWindow object containing features x, realizations y and performance\n",
    "        realizations y_perf\n",
    "        epochs: Number of training epochs\n",
    "        lr: learning rate\n",
    "\n",
    "        Output\n",
    "        Trained model\n",
    "        (Optional) val_loss: Validation loss\n",
    "        \"\"\"\n",
    "\n",
    "        # Assign number of epochs and learning rate\n",
    "        if epochs is None:\n",
    "            epochs = self.epochs\n",
    "        if lr is None:\n",
    "            lr = self.lr\n",
    "\n",
    "        # Define the optimizer and its parameters\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        # Number of elements in training set\n",
    "        n_train = len(train_set)\n",
    "\n",
    "        # Train the neural network\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # TRAINING: forward + backward pass\n",
    "            train_loss = 0\n",
    "            optimizer.zero_grad()\n",
    "            for t, (x, y, y_perf) in enumerate(train_set):\n",
    "\n",
    "                # Forward pass: predict and optimize\n",
    "                z_star, y_hat = self(x.squeeze(), y.squeeze())\n",
    "\n",
    "                # Loss function\n",
    "                if self.pred_loss is None:\n",
    "                    loss = (1 / n_train) * self.perf_loss(z_star, y_perf.squeeze())\n",
    "                else:\n",
    "                    loss = (1 / n_train) * (\n",
    "                        self.perf_loss(z_star, y_perf.squeeze())\n",
    "                        + (self.pred_loss_factor / self.n_y)\n",
    "                        * self.pred_loss(y_hat, y_perf.squeeze()[0])\n",
    "                    )\n",
    "\n",
    "                # Backward pass: backpropagation\n",
    "                loss.backward()\n",
    "\n",
    "                # Accumulate loss of the fully trained model\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Ensure that gamma, delta > 0 after taking a descent step\n",
    "            for name, param in self.named_parameters():\n",
    "                if name == \"gamma\":\n",
    "                    param.data.clamp_(0.0001)\n",
    "                if name == \"delta\":\n",
    "                    param.data.clamp_(0.0001)\n",
    "\n",
    "        # Compute and return the validation loss of the model\n",
    "        if val_set is not None:\n",
    "\n",
    "            # Number of elements in validation set\n",
    "            n_val = len(val_set)\n",
    "\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for t, (x, y, y_perf) in enumerate(val_set):\n",
    "\n",
    "                    # Predict and optimize\n",
    "                    z_val, y_val = self(x.squeeze(), y.squeeze())\n",
    "\n",
    "                    # Loss function\n",
    "                    if self.pred_loss_factor is None:\n",
    "                        loss = (1 / n_val) * self.perf_loss(z_val, y_perf.squeeze())\n",
    "                    else:\n",
    "                        loss = (1 / n_val) * (\n",
    "                            self.perf_loss(z_val, y_perf.squeeze())\n",
    "                            + (self.pred_loss_factor / self.n_y)\n",
    "                            * self.pred_loss(y_val, y_perf.squeeze()[0])\n",
    "                        )\n",
    "\n",
    "                    # Accumulate loss\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            return val_loss\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    # net_cv: Cross validation of the e2e neural net for hyperparameter tuning\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def net_cv(self, X, Y, lr_list, epoch_list, n_val=4):\n",
    "        \"\"\"Neural net cross-validation module\n",
    "\n",
    "        Inputs\n",
    "        X: Features. TrainTest object of feature timeseries data\n",
    "        Y: Realizations. TrainTest object of asset time series data\n",
    "        epochs: number of training passes\n",
    "        lr_list: List of candidate learning rates\n",
    "        epoch_list: List of candidate number of epochs\n",
    "        n_val: Number of validation folds from the training dataset\n",
    "\n",
    "        Output\n",
    "        Trained model\n",
    "        \"\"\"\n",
    "        results = pc.CrossVal()\n",
    "        X_temp = dl.TrainTest(X.train(), X.n_obs, [1, 0])\n",
    "        Y_temp = dl.TrainTest(Y.train(), Y.n_obs, [1, 0])\n",
    "        for epochs in epoch_list:\n",
    "            for lr in lr_list:\n",
    "\n",
    "                # Train the neural network\n",
    "                print(\"================================================\")\n",
    "                print(f\"Training E2E {self.model_type} model: lr={lr}, epochs={epochs}\")\n",
    "\n",
    "                val_loss_tot = []\n",
    "                for i in range(n_val - 1, -1, -1):\n",
    "\n",
    "                    # Partition training dataset into training and validation subset\n",
    "                    split = [round(1 - 0.2 * (i + 1), 2), 0.2]\n",
    "                    X_temp.split_update(split)\n",
    "                    Y_temp.split_update(split)\n",
    "\n",
    "                    # Construct training and validation DataLoader objects\n",
    "                    train_set = DataLoader(\n",
    "                        pc.SlidingWindow(\n",
    "                            X_temp.train(), Y_temp.train(), self.n_obs, self.perf_period\n",
    "                        )\n",
    "                    )\n",
    "                    val_set = DataLoader(\n",
    "                        pc.SlidingWindow(\n",
    "                            X_temp.test(), Y_temp.test(), self.n_obs, self.perf_period\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # Reset learnable parameters gamma and delta\n",
    "                    self.load_state_dict(torch.load(self.init_state_path))\n",
    "\n",
    "                    if self.pred_model == \"linear\":\n",
    "                        # Initialize the prediction layer weights to OLS regression weights\n",
    "                        X_train, Y_train = X_temp.train(), Y_temp.train()\n",
    "                        X_train.insert(0, \"ones\", 1.0)\n",
    "\n",
    "                        X_train = Variable(\n",
    "                            torch.tensor(X_train.values, dtype=torch.double)\n",
    "                        )\n",
    "                        Y_train = Variable(\n",
    "                            torch.tensor(Y_train.values, dtype=torch.double)\n",
    "                        )\n",
    "\n",
    "                        Theta = torch.inverse(X_train.T @ X_train) @ (\n",
    "                            X_train.T @ Y_train\n",
    "                        )\n",
    "                        Theta = Theta.T\n",
    "                        del X_train, Y_train\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            self.pred_layer.bias.copy_(Theta[:, 0])\n",
    "                            self.pred_layer.weight.copy_(Theta[:, 1:])\n",
    "\n",
    "                    val_loss = self.net_train(\n",
    "                        train_set, val_set=val_set, lr=lr, epochs=epochs\n",
    "                    )\n",
    "                    val_loss_tot.append(val_loss)\n",
    "\n",
    "                    print(f\"Fold: {n_val-i} / {n_val}, val_loss: {val_loss}\")\n",
    "\n",
    "                # Store results\n",
    "                results.val_loss.append(np.mean(val_loss_tot))\n",
    "                results.lr.append(lr)\n",
    "                results.epochs.append(epochs)\n",
    "                print(\"================================================\")\n",
    "\n",
    "        # Convert results to dataframe\n",
    "        self.cv_results = results.df()\n",
    "        self.cv_results.to_pickle(self.init_state_path + \"_results.pkl\")\n",
    "\n",
    "        # Select and store the optimal hyperparameters\n",
    "        idx = self.cv_results.val_loss.idxmin()\n",
    "        self.lr = self.cv_results.lr[idx]\n",
    "        self.epochs = self.cv_results.epochs[idx]\n",
    "\n",
    "        # Print optimal parameters\n",
    "        print(\n",
    "            f\"CV E2E {self.model_type} with hyperparameters: lr={self.lr}, epochs={self.epochs}\"\n",
    "        )\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    # net_roll_test: Test the e2e neural net\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def net_roll_test(self, X, Y, n_roll=4, lr=None, epochs=None):\n",
    "        \"\"\"Neural net rolling window out-of-sample test\n",
    "\n",
    "        Inputs\n",
    "        X: Features. ([n_obs+1] x n_x) torch tensor with feature timeseries data\n",
    "        Y: Realizations. (n_obs x n_y) torch tensor with asset timeseries data\n",
    "        n_roll: Number of training periods (i.e., number of times to retrain the model)\n",
    "        lr: Learning rate for test. If 'None', the optimal learning rate is loaded\n",
    "        epochs: Number of epochs for test. If 'None', the optimal # of epochs is loaded\n",
    "\n",
    "        Output\n",
    "        self.portfolio: add the backtest results to the e2e_net object\n",
    "        \"\"\"\n",
    "\n",
    "        # Declare backtest object to hold the test results\n",
    "        portfolio = pc.backtest(\n",
    "            len(Y.test()) - Y.n_obs, self.n_y, Y.test().index[Y.n_obs :]\n",
    "        )\n",
    "\n",
    "        # Store trained gamma and delta values\n",
    "        if self.model_type == \"nom\":\n",
    "            self.gamma_trained = []\n",
    "        elif self.model_type == \"dro\":\n",
    "            self.gamma_trained = []\n",
    "            self.delta_trained = []\n",
    "\n",
    "        # Store the squared L2-norm of the prediction weights and their difference from OLS weights\n",
    "        if self.pred_model == \"linear\":\n",
    "            self.theta_L2 = []\n",
    "            self.theta_dist_L2 = []\n",
    "\n",
    "        # Store initial train/test split\n",
    "        init_split = Y.split\n",
    "\n",
    "        # Window size\n",
    "        win_size = init_split[1] / n_roll\n",
    "\n",
    "        split = [0, 0]\n",
    "        t = 0\n",
    "        for i in range(n_roll):\n",
    "\n",
    "            print(f\"Out-of-sample window: {i+1} / {n_roll}\")\n",
    "\n",
    "            split[0] = init_split[0] + win_size * i\n",
    "            if i < n_roll - 1:\n",
    "                split[1] = win_size\n",
    "            else:\n",
    "                split[1] = 1 - split[0]\n",
    "\n",
    "            X.split_update(split), Y.split_update(split)\n",
    "            train_set = DataLoader(\n",
    "                pc.SlidingWindow(X.train(), Y.train(), self.n_obs, self.perf_period)\n",
    "            )\n",
    "            test_set = DataLoader(pc.SlidingWindow(X.test(), Y.test(), self.n_obs, 0))\n",
    "\n",
    "            # Reset learnable parameters gamma and delta\n",
    "            self.load_state_dict(torch.load(self.init_state_path))\n",
    "\n",
    "            if self.pred_model == \"linear\":\n",
    "                # Initialize the prediction layer weights to OLS regression weights\n",
    "                X_train, Y_train = X.train(), Y.train()\n",
    "                X_train.insert(0, \"ones\", 1.0)\n",
    "\n",
    "                X_train = Variable(torch.tensor(X_train.values, dtype=torch.double))\n",
    "                Y_train = Variable(torch.tensor(Y_train.values, dtype=torch.double))\n",
    "\n",
    "                Theta = torch.inverse(X_train.T @ X_train) @ (X_train.T @ Y_train)\n",
    "                Theta = Theta.T\n",
    "                del X_train, Y_train\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    self.pred_layer.bias.copy_(Theta[:, 0])\n",
    "                    self.pred_layer.weight.copy_(Theta[:, 1:])\n",
    "\n",
    "            # Train model using all available data preceding the test window\n",
    "            self.net_train(train_set, lr=lr, epochs=epochs)\n",
    "\n",
    "            # Store trained values of gamma and delta\n",
    "            if self.model_type == \"nom\":\n",
    "                self.gamma_trained.append(self.gamma.item())\n",
    "            elif self.model_type == \"dro\":\n",
    "                self.gamma_trained.append(self.gamma.item())\n",
    "                self.delta_trained.append(self.delta.item())\n",
    "\n",
    "            # Store the squared L2 norm of theta and distance between theta and OLS weights\n",
    "            if self.pred_model == \"linear\":\n",
    "                theta_L2 = torch.sum(self.pred_layer.weight**2, axis=()) + torch.sum(\n",
    "                    self.pred_layer.bias**2, axis=()\n",
    "                )\n",
    "                theta_dist_L2 = torch.sum(\n",
    "                    (self.pred_layer.weight - Theta[:, 1:]) ** 2, axis=()\n",
    "                ) + torch.sum((self.pred_layer.bias - Theta[:, 0]) ** 2, axis=())\n",
    "                self.theta_L2.append(theta_L2)\n",
    "                self.theta_dist_L2.append(theta_dist_L2)\n",
    "\n",
    "            # Test model\n",
    "            with torch.no_grad():\n",
    "                for j, (x, y, y_perf) in enumerate(test_set):\n",
    "\n",
    "                    # Predict and optimize\n",
    "                    z_star, _ = self(x.squeeze(), y.squeeze())\n",
    "\n",
    "                    # Store portfolio weights and returns for each time step 't'\n",
    "                    portfolio.weights[t] = z_star.squeeze()\n",
    "                    portfolio.rets[t] = y_perf.squeeze() @ portfolio.weights[t]\n",
    "                    t += 1\n",
    "\n",
    "        # Reset dataset\n",
    "        X, Y = X.split_update(init_split), Y.split_update(init_split)\n",
    "\n",
    "        # Calculate the portfolio statistics using the realized portfolio returns\n",
    "        portfolio.stats()\n",
    "\n",
    "        self.portfolio = portfolio\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    # load_cv_results: Load cross validation results\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def load_cv_results(self, cv_results):\n",
    "        \"\"\"Load cross validation results\n",
    "\n",
    "        Inputs\n",
    "        cv_results: pd.dataframe containing the cross validation results\n",
    "\n",
    "        Outputs\n",
    "        self.lr: Load the optimal learning rate\n",
    "        self.epochs: Load the optimal number of epochs\n",
    "        \"\"\"\n",
    "\n",
    "        # Store the cross validation results within the object\n",
    "        self.cv_results = cv_results\n",
    "\n",
    "        # Select and store the optimal hyperparameters\n",
    "        idx = cv_results.val_loss.idxmin()\n",
    "        self.lr = cv_results.lr[idx]\n",
    "        self.epochs = cv_results.epochs[idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
