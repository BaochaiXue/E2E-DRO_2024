{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing p_var...\n",
      "Optimized portfolio weights (Variance): [0.33333333 0.33333333 0.33333333]\n",
      "Variance objective value: 0.0\n",
      "\n",
      "Testing p_mad...\n",
      "Optimized portfolio weights (MAD): [ 0.13355262  0.33328474 -0.66566742]\n",
      "MAD objective value: 3.469446951953614e-18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\23865\\anaconda3\\envs\\py312\\Lib\\site-packages\\diffcp\\cone_program.py:154: SyntaxWarning: invalid escape sequence '\\i'\n",
      "  \"\"\"Solves a cone program, returns its derivative as an abstract linear map.\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "# Risk functions module\n",
    "#\n",
    "# This module defines the financial risk measures to be used in the optimization layer of the E2E\n",
    "# problem.\n",
    "#\n",
    "\n",
    "\n",
    "def p_var(z: cp.Expression, c: float, x: np.ndarray) -> cp.Expression:\n",
    "    \"\"\"\n",
    "    Compute the squared error for the given input.\n",
    "\n",
    "    :param z: A cvxpy expression (decision variable)\n",
    "    :param c: A constant threshold or target value\n",
    "    :param x: A numpy array (weights or features)\n",
    "    :return: The squared error expression\n",
    "    \"\"\"\n",
    "    return cp.square(x @ z - c)\n",
    "\n",
    "\n",
    "def p_mad(z: cp.Expression, c: float, x: np.ndarray) -> cp.Expression:\n",
    "    \"\"\"\n",
    "    Compute the mean absolute deviation for the given input.\n",
    "\n",
    "    :param z: A cvxpy expression (decision variable)\n",
    "    :param c: A constant threshold or target value\n",
    "    :param x: A numpy array (weights or features)\n",
    "    :return: The absolute deviation expression\n",
    "    \"\"\"\n",
    "    return cp.abs(x @ z - c)\n",
    "\n",
    "\n",
    "# Define test data\n",
    "z = cp.Variable(3)  # Decision variable (portfolio weights)\n",
    "c = 0.02  # Centering parameter (expected return)\n",
    "x = np.array(\n",
    "    [\n",
    "        [0.05, 0.02, -0.01],  # Realized returns for multiple scenarios\n",
    "        [0.03, -0.01, 0.04],\n",
    "        [-0.02, 0.01, 0.01],\n",
    "    ]\n",
    ")\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Test variance function (p_var)\n",
    "    print(\"\\nTesting p_var...\")\n",
    "    var_expr = p_var(z, c, x[0])  # Apply p_var to the first row of x\n",
    "    objective_var = cp.Minimize(var_expr)  # Minimize the variance\n",
    "    constraints = [\n",
    "        cp.sum(z) == 1,\n",
    "        z >= 0,\n",
    "    ]  # Portfolio constraints: sum of weights = 1, weights >= 0\n",
    "    problem_var = cp.Problem(objective_var, constraints)\n",
    "    var_opt_value = problem_var.solve()\n",
    "\n",
    "    # Output results for variance minimization\n",
    "    print(\"Optimized portfolio weights (Variance):\", z.value)\n",
    "    print(\"Variance objective value:\", var_opt_value)\n",
    "\n",
    "    # Reinitialize decision variable for MAD problem\n",
    "    z = cp.Variable(3)\n",
    "\n",
    "    # Test MAD function (p_mad)\n",
    "    print(\"\\nTesting p_mad...\")\n",
    "    mad_expr = p_mad(z, c, x[0])  # Apply p_mad to the first row of x\n",
    "    objective_mad = cp.Minimize(mad_expr)  # Minimize the MAD\n",
    "    problem_mad = cp.Problem(objective_mad, constraints)\n",
    "    mad_opt_value = problem_mad.solve()\n",
    "\n",
    "    # Output results for MAD minimization\n",
    "    print(\"Optimized portfolio weights (MAD):\", z.value)\n",
    "    print(\"MAD objective value:\", mad_opt_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing single_period_loss...\n",
      "Single period loss: -0.010999999940395355\n",
      "\n",
      "Testing single_period_over_var_loss...\n",
      "Single period loss over volatility: -10.989008903503418\n",
      "\n",
      "Testing sharpe_loss...\n",
      "Sharpe ratio loss: -11.988011360168457\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "# Performance loss functions with type hints and improved comments\n",
    "def single_period_loss(z_star: Tensor, y_perf: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the single-period loss based on the out-of-sample portfolio return.\n",
    "\n",
    "    This function computes the out-of-sample portfolio return for a given portfolio over the next\n",
    "    time step. It computes the loss as the negative return since optimization typically focuses\n",
    "    on minimizing the loss, and maximizing returns translates into minimizing negative returns.\n",
    "\n",
    "    :param z_star: Tensor of shape (n_y, 1) representing the optimal portfolio weights.\n",
    "    :param y_perf: Tensor of shape (perf_period, n_y) representing the realized returns.\n",
    "    :return: A scalar tensor representing the realized return at the first time step (negative).\n",
    "    \"\"\"\n",
    "    # Calculate the portfolio return for the first time step and negate it (since we want to minimize loss)\n",
    "    return -y_perf[0] @ z_star\n",
    "\n",
    "\n",
    "def single_period_over_var_loss(z_star: Tensor, y_perf: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the loss as the portfolio return divided by the portfolio's volatility.\n",
    "\n",
    "    This function computes the portfolio return at the first time step and divides it by the\n",
    "    realized volatility (standard deviation) of the portfolio returns over the performance period.\n",
    "    This provides a return-over-risk measure, which is often used in portfolio analysis.\n",
    "\n",
    "    :param z_star: Tensor of shape (n_y, 1) representing the optimal portfolio weights.\n",
    "    :param y_perf: Tensor of shape (perf_period, n_y) representing the realized returns.\n",
    "    :return: A scalar tensor representing the return over realized volatility (negative).\n",
    "    \"\"\"\n",
    "    # Calculate the portfolio returns over the entire performance period\n",
    "    portfolio_returns = y_perf @ z_star\n",
    "    # Calculate the standard deviation (volatility) of the portfolio returns, adding epsilon for numerical stability\n",
    "    volatility = torch.std(portfolio_returns, unbiased=True) + 1e-6\n",
    "    # Calculate the return at the first time step and divide by the volatility, then negate for loss\n",
    "    return -portfolio_returns[0] / volatility\n",
    "\n",
    "\n",
    "def sharpe_loss(z_star: Tensor, y_perf: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the loss based on the Sharpe ratio over a performance period.\n",
    "\n",
    "    This function computes a simplified Sharpe ratio, which is the ratio of the mean portfolio\n",
    "    return to its standard deviation (volatility) over the performance period. The loss is defined\n",
    "    as the negative Sharpe ratio to allow for minimization.\n",
    "\n",
    "    :param z_star: Tensor of shape (n_y, 1) representing the optimal portfolio weights.\n",
    "    :param y_perf: Tensor of shape (perf_period, n_y) representing the realized returns.\n",
    "    :return: A scalar tensor representing the negative Sharpe ratio.\n",
    "    \"\"\"\n",
    "    # Calculate the portfolio returns over the entire performance period\n",
    "    portfolio_returns = y_perf @ z_star\n",
    "    # Calculate the mean return of the portfolio\n",
    "    mean_return = torch.mean(portfolio_returns)\n",
    "    # Calculate the standard deviation (volatility) of the portfolio returns, adding epsilon for numerical stability\n",
    "    volatility = torch.std(portfolio_returns, unbiased=True) + 1e-6\n",
    "    # Calculate the Sharpe ratio and negate it for loss\n",
    "    return -mean_return / volatility\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example portfolio weights (3 assets)\n",
    "    z_star = torch.tensor([0.3, 0.5, 0.2])\n",
    "    # Realized returns for 3 assets over 3 periods\n",
    "    y_perf = torch.tensor(\n",
    "        [\n",
    "            [0.01, 0.02, -0.01],\n",
    "            [0.03, -0.01, 0.04],\n",
    "            [0.02, 0.01, 0.01],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Test the single-period loss function\n",
    "    print(\"Testing single_period_loss...\")\n",
    "    loss_sp = single_period_loss(z_star, y_perf)\n",
    "    print(f\"Single period loss: {loss_sp.item()}\")\n",
    "\n",
    "    # Test the single-period-over-volatility loss function\n",
    "    print(\"\\nTesting single_period_over_var_loss...\")\n",
    "    loss_sp_var = single_period_over_var_loss(z_star, y_perf)\n",
    "    print(f\"Single period loss over volatility: {loss_sp_var.item()}\")\n",
    "\n",
    "    # Test the Sharpe ratio loss function\n",
    "    print(\"\\nTesting sharpe_loss...\")\n",
    "    loss_sharpe = sharpe_loss(z_star, y_perf)\n",
    "    print(f\"Sharpe ratio loss: {loss_sharpe.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SlidingWindow...\n",
      "x (features): torch.Size([11, 3])\n",
      "y (realizations): torch.Size([10, 2])\n",
      "y_perf (performance window): torch.Size([5, 2])\n",
      "\n",
      "Testing Backtest...\n",
      "                rets       tri\n",
      "Date                          \n",
      "2020-03-11  0.773763  1.773763\n",
      "2020-03-12  0.145725  2.032245\n",
      "2020-03-13 -1.994339 -2.020740\n",
      "2020-03-14  1.620361 -5.295068\n",
      "2020-03-15  0.074920 -5.691778\n",
      "Mean return: nan\n",
      "Volatility: 1.0844\n",
      "Sharpe ratio: nan\n",
      "\n",
      "Testing Backtest stats calculation...\n",
      "            rets       tri\n",
      "Date                      \n",
      "2020-04-05  0.05  1.050000\n",
      "2020-04-06 -0.02  1.029000\n",
      "2020-04-07  0.03  1.059870\n",
      "2020-04-08  0.04  1.102265\n",
      "2020-04-09 -0.01  1.091242\n",
      "Mean return: 0.0176\n",
      "Volatility: 0.0279\n",
      "Sharpe ratio: 0.6324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23865\\AppData\\Local\\Temp\\ipykernel_14712\\684581267.py:114: RuntimeWarning: invalid value encountered in scalar power\n",
      "  self.mean = (tri[-1]) ** (1 / len(tri)) - 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "####################################################################################################\n",
    "# SlidingWindow Dataset to index data using a sliding window\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "class SlidingWindow(Dataset):\n",
    "    \"\"\"Dataset class for creating a sliding window from time series data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        Y: pd.DataFrame,\n",
    "        n_obs: int,\n",
    "        perf_period: int,\n",
    "        dtype: torch.dtype = torch.float32,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the SlidingWindow dataset.\n",
    "\n",
    "        :param X: DataFrame containing the complete feature dataset.\n",
    "        :param Y: DataFrame containing the complete asset return dataset.\n",
    "        :param n_obs: Number of observations in the sliding window.\n",
    "        :param perf_period: Number of future observations used for out-of-sample performance evaluation.\n",
    "        :param dtype: The desired data type for tensors (default is torch.float32).\n",
    "        :param device: Device on which to place the tensors (e.g., 'cpu' or 'cuda' for GPU).\n",
    "        \"\"\"\n",
    "        self.X = torch.tensor(\n",
    "            X.values, dtype=dtype, device=device\n",
    "        )  # Convert feature dataset to tensor\n",
    "        self.Y = torch.tensor(\n",
    "            Y.values, dtype=dtype, device=device\n",
    "        )  # Convert asset return dataset to tensor\n",
    "        self.n_obs = n_obs  # Number of observations in the sliding window\n",
    "        self.perf_period = (\n",
    "            perf_period  # Number of future observations for performance evaluation\n",
    "        )\n",
    "\n",
    "    def __getitem__(\n",
    "        self, index: int\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Retrieve a single window of data.\n",
    "\n",
    "        :param index: Index of the sliding window.\n",
    "        :return: Tuple (x, y, y_perf):\n",
    "            x: Features window of shape (n_obs + 1, n_x).\n",
    "            y: Realizations window of shape (n_obs, n_y).\n",
    "            y_perf: Future performance window of shape (perf_period, n_y).\n",
    "        \"\"\"\n",
    "        # Retrieve features for the sliding window (n_obs + 1 observations)\n",
    "        x = self.X[index : index + self.n_obs + 1]\n",
    "        # Retrieve asset returns for the sliding window (n_obs observations)\n",
    "        y = self.Y[index : index + self.n_obs]\n",
    "        # Retrieve future performance data (perf_period observations)\n",
    "        y_perf = self.Y[index + self.n_obs : index + self.n_obs + self.perf_period]\n",
    "        return (x, y, y_perf)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the number of windows that can be created from the dataset.\n",
    "\n",
    "        :return: Length of the dataset, considering the sliding windows.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            len(self.X) - self.n_obs - self.perf_period\n",
    "        )  # Total number of sliding windows available\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Backtest class to store out-of-sample results\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "class Backtest:\n",
    "    \"\"\"Class to store out-of-sample results for a backtest.\"\"\"\n",
    "\n",
    "    def __init__(self, len_test: int, n_y: int, dates: pd.DatetimeIndex) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Backtest object.\n",
    "\n",
    "        :param len_test: Number of scenarios in the out-of-sample evaluation period.\n",
    "        :param n_y: Number of assets in the portfolio.\n",
    "        :param dates: DatetimeIndex containing the corresponding dates.\n",
    "        \"\"\"\n",
    "        self.weights = np.zeros(\n",
    "            (len_test, n_y)\n",
    "        )  # Initialize portfolio weights over time\n",
    "        self.rets = np.zeros(\n",
    "            len_test\n",
    "        )  # Initialize realized portfolio returns over time\n",
    "        self.dates = dates[\n",
    "            -len_test:\n",
    "        ]  # Keep only the dates for the out-of-sample period\n",
    "\n",
    "    def stats(self) -> None:\n",
    "        \"\"\"\n",
    "        Compute and store the cumulative returns, mean return, volatility, and Sharpe ratio.\n",
    "\n",
    "        This method calculates key performance metrics of the portfolio, including:\n",
    "        - Cumulative returns (Total Return Index), which show the total growth of the portfolio over time.\n",
    "        - Annualized mean return, which is an estimate of the average return the portfolio would achieve per year.\n",
    "        - Volatility, which measures the risk by calculating the standard deviation of returns.\n",
    "        - Sharpe ratio, which indicates the risk-adjusted return of the portfolio.\n",
    "        \"\"\"\n",
    "        # Calculate cumulative returns (Total Return Index)\n",
    "        tri = np.cumprod(self.rets + 1)\n",
    "        # Calculate the annualized mean return using the final cumulative return and the number of periods\n",
    "        self.mean = (tri[-1]) ** (1 / len(tri)) - 1\n",
    "        # Calculate the volatility (standard deviation) of the portfolio returns\n",
    "        self.vol = np.std(self.rets)\n",
    "        # Calculate the Sharpe ratio (mean return divided by volatility)\n",
    "        self.sharpe = self.mean / self.vol\n",
    "        # Create a DataFrame containing realized returns and cumulative returns, indexed by dates\n",
    "        if len(self.dates) == len(self.rets):\n",
    "            self.rets = pd.DataFrame(\n",
    "                {\"Date\": self.dates, \"rets\": self.rets, \"tri\": tri}\n",
    "            ).set_index(\"Date\")\n",
    "        else:\n",
    "            raise ValueError(\"Length of dates and returns must be equal.\")\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# InSample class to store in-sample results\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "class InSample:\n",
    "    \"\"\"Class to store the in-sample results of neural network training.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the InSample object.\n",
    "        \"\"\"\n",
    "        self.loss = []  # List to hold training losses\n",
    "        self.gamma = []  # List to hold gamma values (hyperparameter)\n",
    "        self.delta = []  # List to hold delta values (hyperparameter)\n",
    "        self.val_loss = []  # List to hold validation losses (optional)\n",
    "\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Return a DataFrame containing the training statistics.\n",
    "\n",
    "        :return: DataFrame with columns representing different metrics during training.\n",
    "        \"\"\"\n",
    "        # Return a DataFrame based on available data, adjusting columns accordingly\n",
    "        if not self.delta and not self.val_loss:\n",
    "            return pd.DataFrame(\n",
    "                list(zip(self.loss, self.gamma)), columns=[\"loss\", \"gamma\"]\n",
    "            )\n",
    "        elif not self.delta:\n",
    "            return pd.DataFrame(\n",
    "                list(zip(self.loss, self.val_loss, self.gamma)),\n",
    "                columns=[\"loss\", \"val_loss\", \"gamma\"],\n",
    "            )\n",
    "        elif not self.val_loss:\n",
    "            return pd.DataFrame(\n",
    "                list(zip(self.loss, self.gamma, self.delta)),\n",
    "                columns=[\"loss\", \"gamma\", \"delta\"],\n",
    "            )\n",
    "        else:\n",
    "            return pd.DataFrame(\n",
    "                list(zip(self.loss, self.val_loss, self.gamma, self.delta)),\n",
    "                columns=[\"loss\", \"val_loss\", \"gamma\", \"delta\"],\n",
    "            )\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# CrossVal class to store cross-validation results\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "class CrossVal:\n",
    "    \"\"\"Class to store cross-validation results of neural network training.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the CrossVal object.\n",
    "        \"\"\"\n",
    "        self.lr = []  # List to hold learning rates\n",
    "        self.epochs = []  # List to hold the number of epochs in each run\n",
    "        self.val_loss = []  # List to hold validation losses\n",
    "\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Return a DataFrame containing the cross-validation statistics.\n",
    "\n",
    "        :return: DataFrame with learning rate, epochs, and validation loss.\n",
    "        \"\"\"\n",
    "        # Create and return a DataFrame with learning rates, epochs, and validation losses\n",
    "        return pd.DataFrame(\n",
    "            list(zip(self.lr, self.epochs, self.val_loss)),\n",
    "            columns=[\"lr\", \"epochs\", \"val_loss\"],\n",
    "        )\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Test code for the Backtest class\n",
    "####################################################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    X = pd.DataFrame(\n",
    "        np.random.randn(100, 3)\n",
    "    )  # Create feature dataset with 100 samples and 3 features\n",
    "    Y = pd.DataFrame(\n",
    "        np.random.randn(100, 2)\n",
    "    )  # Create asset return dataset with 100 samples and 2 assets\n",
    "    dates = pd.date_range(start=\"2020-01-01\", periods=100, freq=\"D\")\n",
    "\n",
    "    # Check if GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize SlidingWindow with given parameters\n",
    "    n_obs = 10\n",
    "    perf_period = 5\n",
    "    sliding_window = SlidingWindow(X, Y, n_obs, perf_period, device=device)\n",
    "\n",
    "    # Fetch a sample window\n",
    "    print(\"Testing SlidingWindow...\")\n",
    "    x, y, y_perf = sliding_window[0]\n",
    "    print(f\"x (features): {x.shape}\")\n",
    "    print(f\"y (realizations): {y.shape}\")\n",
    "    print(f\"y_perf (performance window): {y_perf.shape}\")\n",
    "\n",
    "    # Initialize Backtest with given parameters\n",
    "    len_test = 30\n",
    "    backtest_obj = Backtest(len_test=len_test, n_y=2, dates=dates)\n",
    "\n",
    "    # Simulate some portfolio returns\n",
    "    backtest_obj.rets = np.random.randn(len_test)\n",
    "\n",
    "    print(\"\\nTesting Backtest...\")\n",
    "    backtest_obj.stats()\n",
    "    print(backtest_obj.rets.head())\n",
    "    print(f\"Mean return: {backtest_obj.mean:.4f}\")\n",
    "    print(f\"Volatility: {backtest_obj.vol:.4f}\")\n",
    "    print(f\"Sharpe ratio: {backtest_obj.sharpe:.4f}\")\n",
    "\n",
    "    # Test Backtest stats calculation\n",
    "    print(\"\\nTesting Backtest stats calculation...\")\n",
    "    backtest_obj.rets = np.array([0.05, -0.02, 0.03, 0.04, -0.01])\n",
    "    backtest_obj.dates = dates[\n",
    "        -len(backtest_obj.rets) :\n",
    "    ]  # Adjust dates to match returns length\n",
    "    backtest_obj.stats()\n",
    "    print(backtest_obj.rets.head())\n",
    "    print(f\"Mean return: {backtest_obj.mean:.4f}\")\n",
    "    print(f\"Volatility: {backtest_obj.vol:.4f}\")\n",
    "    print(f\"Sharpe ratio: {backtest_obj.sharpe:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "   Feature_0  Feature_1  Feature_2  Feature_3  Feature_4\n",
      "0  -0.450604  -1.439658  -1.514197  -0.737642  -1.067823\n",
      "1  -1.669584  -0.030337   1.239172  -0.264097   0.650229\n",
      "2   1.015774   0.041716   0.940475  -0.457522   1.841928\n",
      "3   1.001157  -0.613481  -2.163387   0.994279   0.951825\n",
      "4  -0.310661  -1.043266   0.699210   1.708873   0.325062\n",
      "Number of training observations: 700\n",
      "\n",
      "Test Data:\n",
      "     Feature_0  Feature_1  Feature_2  Feature_3  Feature_4\n",
      "650   0.335713   0.792808   1.480048   1.312989   0.165560\n",
      "651   0.103609  -0.320448  -0.387474   0.163370  -1.974616\n",
      "652   1.042307   2.523618  -0.082401   1.489710  -0.061015\n",
      "653  -0.901812   0.375389  -1.090880  -0.427800   0.903261\n",
      "654  -0.572097  -0.137925   0.463012   0.020806  -1.400109\n",
      "Number of test observations: 350\n",
      "\n",
      "Updated Training Data:\n",
      "   Feature_0  Feature_1  Feature_2  Feature_3  Feature_4\n",
      "0  -0.450604  -1.439658  -1.514197  -0.737642  -1.067823\n",
      "1  -1.669584  -0.030337   1.239172  -0.264097   0.650229\n",
      "2   1.015774   0.041716   0.940475  -0.457522   1.841928\n",
      "3   1.001157  -0.613481  -2.163387   0.994279   0.951825\n",
      "4  -0.310661  -1.043266   0.699210   1.708873   0.325062\n",
      "Number of updated training observations: 600\n",
      "\n",
      "Updated Test Data:\n",
      "     Feature_0  Feature_1  Feature_2  Feature_3  Feature_4\n",
      "550   1.789952  -1.109992  -0.208193  -1.549868   1.512496\n",
      "551  -0.798761  -0.679674   1.060025  -1.077785  -0.211718\n",
      "552   1.769337   0.511086   1.030146  -1.847515  -0.368578\n",
      "553   0.116662  -0.994679  -0.262735  -0.149809   1.418848\n",
      "554  -0.385099   1.038582   0.564302   0.544080   1.610045\n",
      "Number of updated test observations: 450\n"
     ]
    }
   ],
   "source": [
    "# DataLoad module\n",
    "#\n",
    "####################################################################################################\n",
    "# Import libraries\n",
    "####################################################################################################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "import numpy as np\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# TrainTest class\n",
    "####################################################################################################\n",
    "class TrainTest:\n",
    "    def __init__(self, data: pd.DataFrame, n_obs: int, split: list[float]) -> None:\n",
    "        \"\"\"\n",
    "        Object to hold the training, validation, and testing datasets.\n",
    "\n",
    "        :param data: pandas DataFrame with time series data.\n",
    "        :param n_obs: Number of observations per batch.\n",
    "        :param split: List of ratios that control the partition of data into training, testing, and validation sets.\n",
    "        \"\"\"\n",
    "        self.data: pd.DataFrame = data  # Store the input data as a DataFrame\n",
    "        self.n_obs: int = n_obs  # Set the number of observations per batch\n",
    "        self.split: list[float] = (\n",
    "            split  # Set the split ratios for training, validation, and testing\n",
    "        )\n",
    "\n",
    "        n_obs_tot: int = self.data.shape[\n",
    "            0\n",
    "        ]  # Calculate the total number of observations in the dataset\n",
    "        numel: np.ndarray = n_obs_tot * np.cumsum(\n",
    "            split\n",
    "        )  # Calculate the cumulative number of elements based on split ratios\n",
    "        self.numel: list[int] = [\n",
    "            round(i) for i in numel\n",
    "        ]  # Round the cumulative elements to get the indices for splits\n",
    "\n",
    "    def split_update(self, split: list[float]) -> None:\n",
    "        \"\"\"\n",
    "        Update the list outlining the split ratio of training, validation, and testing datasets.\n",
    "\n",
    "        :param split: List of ratios that control the partition of data into training, testing, and validation sets.\n",
    "        \"\"\"\n",
    "        self.split: list[float] = (\n",
    "            split  # Update the split ratios with the new list provided\n",
    "        )\n",
    "        n_obs_tot: int = self.data.shape[\n",
    "            0\n",
    "        ]  # Calculate the total number of observations in the dataset\n",
    "        numel: np.ndarray = n_obs_tot * np.cumsum(\n",
    "            split\n",
    "        )  # Calculate the cumulative number of elements based on new split ratios\n",
    "        self.numel: list[int] = [\n",
    "            round(i) for i in numel\n",
    "        ]  # Round the cumulative elements to get the indices for splits\n",
    "\n",
    "    def train(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Return the training subset of observations.\n",
    "\n",
    "        :return: pandas DataFrame containing the training data subset.\n",
    "        \"\"\"\n",
    "        return self.data[\n",
    "            : self.numel[0]\n",
    "        ]  # Return the data from the start up to the end of the training set\n",
    "\n",
    "    def test(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Return the test subset of observations.\n",
    "\n",
    "        :return: pandas DataFrame containing the test data subset.\n",
    "        \"\"\"\n",
    "        return self.data[\n",
    "            self.numel[0] - self.n_obs : self.numel[1]\n",
    "        ]  # Return the data for the test set, including overlap\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data for testing\n",
    "    n_tot = 1000  # Total number of observations\n",
    "    n_features = 5  # Number of features\n",
    "    split_ratios = [0.7, 0.3]  # 70% training, 30% testing\n",
    "    n_obs = 50  # Number of observations per batch\n",
    "\n",
    "    # Create a synthetic DataFrame with random data\n",
    "    data = pd.DataFrame(\n",
    "        np.random.randn(n_tot, n_features),\n",
    "        columns=[f\"Feature_{i}\" for i in range(n_features)],\n",
    "    )\n",
    "\n",
    "    # Initialize TrainTest object\n",
    "    train_test_obj = TrainTest(data=data, n_obs=n_obs, split=split_ratios)\n",
    "\n",
    "    # Test the training data split\n",
    "    train_data = train_test_obj.train()\n",
    "    print(\"Training Data:\")\n",
    "    print(train_data.head())\n",
    "    print(f\"Number of training observations: {len(train_data)}\")\n",
    "\n",
    "    # Test the test data split\n",
    "    test_data = train_test_obj.test()\n",
    "    print(\"\\nTest Data:\")\n",
    "    print(test_data.head())\n",
    "    print(f\"Number of test observations: {len(test_data)}\")\n",
    "\n",
    "    # Update split ratios and test again\n",
    "    new_split_ratios = [0.6, 0.4]  # Update split ratios\n",
    "    train_test_obj.split_update(split=new_split_ratios)\n",
    "\n",
    "    # Test the updated training data split\n",
    "    updated_train_data = train_test_obj.train()\n",
    "    print(\"\\nUpdated Training Data:\")\n",
    "    print(updated_train_data.head())\n",
    "    print(f\"Number of updated training observations: {len(updated_train_data)}\")\n",
    "\n",
    "    # Test the updated test data split\n",
    "    updated_test_data = train_test_obj.test()\n",
    "    print(\"\\nUpdated Test Data:\")\n",
    "    print(updated_test_data.head())\n",
    "    print(f\"Number of updated test observations: {len(updated_test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      "          0         1         2         3         4\n",
      "0  0.037531 -0.007538  0.036639  0.000060 -0.001520\n",
      "1  0.000079 -0.003700 -0.049743 -0.034093 -0.022725\n",
      "2 -0.059466  0.000666 -0.004978 -0.009004  0.002649\n",
      "3  0.000444  0.006347 -0.015048 -0.025928  0.001903\n",
      "4 -0.008474 -0.023720 -0.007309 -0.025420  0.031723\n",
      "Number of training feature observations: 720\n",
      "\n",
      "Test Features:\n",
      "            0         1         2         3         4\n",
      "616  0.014443  0.017019 -0.003300 -0.056563 -0.017051\n",
      "617  0.018964 -0.016458 -0.003853 -0.011657 -0.022333\n",
      "618 -0.022941  0.020247 -0.012068  0.003381  0.019936\n",
      "619  0.006151  0.024943 -0.006490 -0.010648 -0.017412\n",
      "620 -0.031169 -0.034504  0.012386  0.022285 -0.043454\n",
      "Number of test feature observations: 584\n",
      "\n",
      "Training Outputs:\n",
      "          0         1         2         3         4         5         6  \\\n",
      "0  0.020391 -0.024384 -0.050628 -0.016314 -0.044117  0.017402  0.090213   \n",
      "1 -0.001323  0.039438  0.068975 -0.035036  0.059205 -0.009392 -0.091626   \n",
      "2 -0.009475 -0.028947  0.089823 -0.018012  0.025656 -0.011985 -0.050961   \n",
      "3 -0.051537 -0.012596  0.051167 -0.006315  0.005678  0.046476 -0.037532   \n",
      "4 -0.016574  0.052634  0.000314  0.013466  0.029502 -0.026224  0.000905   \n",
      "\n",
      "          7         8         9  \n",
      "0  0.195578  0.055969 -0.010406  \n",
      "1  0.022831 -0.056277  0.004937  \n",
      "2 -0.112770  0.026023  0.100238  \n",
      "3  0.042647 -0.042372 -0.044804  \n",
      "4 -0.004659 -0.046663  0.068020  \n",
      "Number of training output observations: 720\n",
      "\n",
      "Test Outputs:\n",
      "            0         1         2         3         4         5         6  \\\n",
      "616  0.014203  0.029007 -0.032045 -0.003457 -0.008616  0.027413 -0.099575   \n",
      "617 -0.002404 -0.055210 -0.005516  0.008291 -0.002596  0.053899  0.041505   \n",
      "618 -0.028518  0.072404  0.042859 -0.040510  0.051107 -0.006973 -0.113639   \n",
      "619  0.000575  0.018074  0.017655  0.030531  0.117955 -0.056270 -0.133210   \n",
      "620  0.007498 -0.056192 -0.011489  0.000290  0.015717  0.015404  0.052565   \n",
      "\n",
      "            7         8         9  \n",
      "616  0.041896 -0.052044  0.115593  \n",
      "617  0.059450  0.037099  0.051167  \n",
      "618  0.036960 -0.121276  0.155090  \n",
      "619  0.046830  0.081611  0.026692  \n",
      "620 -0.160624  0.032781 -0.008716  \n",
      "Number of test output observations: 584\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "# Generate linear synthetic data\n",
    "####################################################################################################\n",
    "def synthetic(\n",
    "    n_x: int = 5,\n",
    "    n_y: int = 10,\n",
    "    n_tot: int = 1200,\n",
    "    n_obs: int = 104,\n",
    "    split: list[float] = [0.6, 0.4],\n",
    "    set_seed: int = 100,\n",
    ") -> tuple[TrainTest, TrainTest]:\n",
    "    \"\"\"\n",
    "    Generates synthetic (normally-distributed) asset and factor data.\n",
    "\n",
    "    :param n_x: Number of features.\n",
    "    :param n_y: Number of assets.\n",
    "    :param n_tot: Number of observations in the whole dataset.\n",
    "    :param n_obs: Number of observations per batch.\n",
    "    :param split: List of floats representing train-validation-test split percentages (must sum up to one).\n",
    "    :param set_seed: Integer seed for replicability of the numpy RNG.\n",
    "\n",
    "    :return: Tuple of TrainTest objects for features and asset data split into train, validation, and test subsets.\n",
    "    \"\"\"\n",
    "    np.random.seed(set_seed)  # Set the random seed for reproducibility\n",
    "\n",
    "    # 'True' prediction bias and weights\n",
    "    a: np.ndarray = (\n",
    "        np.sort(np.random.rand(n_y) / 250) + 0.0001\n",
    "    )  # Generate small bias terms for each asset\n",
    "    b: np.ndarray = (\n",
    "        np.random.randn(n_x, n_y) / 5\n",
    "    )  # Generate random weights for linear relationships between features and assets\n",
    "    c: np.ndarray = np.random.randn(\n",
    "        int((n_x + 1) / 2), n_y\n",
    "    )  # Generate additional random weights for auxiliary features\n",
    "\n",
    "    # Noise standard deviation\n",
    "    s: np.ndarray = (\n",
    "        np.sort(np.random.rand(n_y)) / 20 + 0.02\n",
    "    )  # Generate small standard deviations for noise for each asset\n",
    "\n",
    "    # Synthetic features\n",
    "    X: np.ndarray = (\n",
    "        np.random.randn(n_tot, n_x) / 50\n",
    "    )  # Generate synthetic features from a normal distribution\n",
    "    X2: np.ndarray = (\n",
    "        np.random.randn(n_tot, int((n_x + 1) / 2)) / 50\n",
    "    )  # Generate auxiliary features from a normal distribution\n",
    "\n",
    "    # Synthetic outputs\n",
    "    Y: np.ndarray = (\n",
    "        a + X @ b + X2 @ c + s * np.random.randn(n_tot, n_y)\n",
    "    )  # Generate synthetic outputs based on linear combinations of features and noise\n",
    "\n",
    "    X: pd.DataFrame = pd.DataFrame(X)  # Convert features to a pandas DataFrame\n",
    "    Y: pd.DataFrame = pd.DataFrame(Y)  # Convert outputs to a pandas DataFrame\n",
    "\n",
    "    # Partition dataset into training and testing sets\n",
    "    return TrainTest(X, n_obs, split), TrainTest(\n",
    "        Y, n_obs, split\n",
    "    )  # Return TrainTest objects for features and outputs\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Test code for synthetic data generation\n",
    "####################################################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters for synthetic data generation\n",
    "    n_x = 5  # Number of features\n",
    "    n_y = 10  # Number of assets\n",
    "    n_tot = 1200  # Total number of observations\n",
    "    n_obs = 104  # Number of observations per batch\n",
    "    split = [0.6, 0.4]  # Split ratios for training and testing\n",
    "    set_seed = 100  # Random seed for reproducibility\n",
    "\n",
    "    # Generate synthetic data\n",
    "    train_test_features, train_test_outputs = synthetic(\n",
    "        n_x, n_y, n_tot, n_obs, split, set_seed\n",
    "    )\n",
    "\n",
    "    # Test the generated feature data\n",
    "    train_features = train_test_features.train()\n",
    "    test_features = train_test_features.test()\n",
    "    print(\"Training Features:\")\n",
    "    print(train_features.head())\n",
    "    print(f\"Number of training feature observations: {len(train_features)}\")\n",
    "    print(\"\\nTest Features:\")\n",
    "    print(test_features.head())\n",
    "    print(f\"Number of test feature observations: {len(test_features)}\")\n",
    "\n",
    "    # Test the generated output data\n",
    "    train_outputs = train_test_outputs.train()\n",
    "    test_outputs = train_test_outputs.test()\n",
    "    print(\"\\nTraining Outputs:\")\n",
    "    print(train_outputs.head())\n",
    "    print(f\"Number of training output observations: {len(train_outputs)}\")\n",
    "    print(\"\\nTest Outputs:\")\n",
    "    print(test_outputs.head())\n",
    "    print(f\"Number of test output observations: {len(test_outputs)}\")\n",
    "\n",
    "    # Verify the shape of the generated data\n",
    "    assert (\n",
    "        train_features.shape[1] == n_x\n",
    "    ), \"Number of features in training set does not match expected value.\"\n",
    "    assert (\n",
    "        train_outputs.shape[1] == n_y\n",
    "    ), \"Number of assets in training set does not match expected value.\"\n",
    "    assert (\n",
    "        test_features.shape[1] == n_x\n",
    "    ), \"Number of features in test set does not match expected value.\"\n",
    "    assert (\n",
    "        test_outputs.shape[1] == n_y\n",
    "    ), \"Number of assets in test set does not match expected value.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "# Generate non-linear synthetic data\n",
    "####################################################################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unittest\n",
    "\n",
    "\n",
    "def synthetic_nl(\n",
    "    n_x: int = 5,\n",
    "    n_y: int = 10,\n",
    "    n_tot: int = 1200,\n",
    "    n_obs: int = 104,\n",
    "    split: list[float] = [0.6, 0.4],\n",
    "    set_seed: int = 100,\n",
    ") -> tuple[TrainTest, TrainTest]:\n",
    "    \"\"\"\n",
    "    Generates synthetic (normally-distributed) factor data and mixes them using a quadratic model\n",
    "    of linear, squared, and cross products to produce the asset data.\n",
    "\n",
    "    :param n_x: Number of features.\n",
    "    :param n_y: Number of assets.\n",
    "    :param n_tot: Number of observations in the whole dataset.\n",
    "    :param n_obs: Number of observations per batch.\n",
    "    :param split: List of floats representing train-validation-test split percentages (must sum up to one).\n",
    "    :param set_seed: Integer seed for replicability of the numpy RNG.\n",
    "    :return: Tuple of TrainTest objects for features and asset data split into train, validation, and test subsets.\n",
    "    \"\"\"\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(set_seed)\n",
    "\n",
    "    # Generate 'True' prediction bias and weights\n",
    "    a: np.ndarray = (\n",
    "        np.sort(np.random.rand(n_y) / 200) + 0.0005\n",
    "    )  # Bias terms for each asset\n",
    "    b: np.ndarray = np.random.randn(n_x, n_y) / 4  # Linear relationship weights\n",
    "    c: np.ndarray = np.random.randn(\n",
    "        int((n_x + 1) / 2), n_y\n",
    "    )  # Auxiliary feature weights\n",
    "    d: np.ndarray = np.random.randn(n_x**2, n_y) / n_x  # Cross-product weights\n",
    "\n",
    "    # Generate noise standard deviation for each asset\n",
    "    s: np.ndarray = np.sort(np.random.rand(n_y)) / 20 + 0.02\n",
    "\n",
    "    # Generate synthetic features\n",
    "    X: np.ndarray = np.random.randn(n_tot, n_x) / 50  # Main features\n",
    "    X2: np.ndarray = (\n",
    "        np.random.randn(n_tot, int((n_x + 1) / 2)) / 50\n",
    "    )  # Auxiliary features\n",
    "    X_cross: np.ndarray = 100 * (X[:, :, None] * X[:, None, :]).reshape(\n",
    "        n_tot, n_x**2\n",
    "    )  # Cross-product features\n",
    "    X_cross = X_cross - X_cross.mean(axis=0)  # Center cross-product features\n",
    "\n",
    "    # Generate synthetic outputs\n",
    "    Y: np.ndarray = a + X @ b + X2 @ c + X_cross @ d + s * np.random.randn(n_tot, n_y)\n",
    "\n",
    "    # Convert features and outputs to pandas DataFrames\n",
    "    X_df: pd.DataFrame = pd.DataFrame(X)\n",
    "    Y_df: pd.DataFrame = pd.DataFrame(Y)\n",
    "\n",
    "    # Partition dataset into training and testing sets\n",
    "    return TrainTest(X_df, n_obs, split), TrainTest(Y_df, n_obs, split)\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Unit Test for synthetic_nl function\n",
    "####################################################################################################\n",
    "def test_synthetic_nl() -> None:\n",
    "    n_x = 5\n",
    "    n_y = 10\n",
    "    n_tot = 1200\n",
    "    n_obs = 104\n",
    "    split = [0.6, 0.4]\n",
    "    set_seed = 100\n",
    "\n",
    "    features, outputs = synthetic_nl(\n",
    "        n_x=n_x,\n",
    "        n_y=n_y,\n",
    "        n_tot=n_tot,\n",
    "        n_obs=n_obs,\n",
    "        split=split,\n",
    "        set_seed=set_seed,\n",
    "    )\n",
    "\n",
    "    assert isinstance(\n",
    "        features, TrainTest\n",
    "    ), \"Features should be an instance of TrainTest.\"\n",
    "    assert isinstance(outputs, TrainTest), \"Outputs should be an instance of TrainTest.\"\n",
    "    assert features.data.shape == (n_tot, n_x), \"Features data shape mismatch.\"\n",
    "    assert outputs.data.shape == (n_tot, n_y), \"Outputs data shape mismatch.\"\n",
    "    expected_train_len = int(n_tot * split[0])\n",
    "    assert (\n",
    "        len(features.train()) == expected_train_len\n",
    "    ), \"Training split size mismatch for features.\"\n",
    "    assert (\n",
    "        len(outputs.train()) == expected_train_len\n",
    "    ), \"Training split size mismatch for outputs.\"\n",
    "\n",
    "    # Test reproducibility\n",
    "    features_1, outputs_1 = synthetic_nl(\n",
    "        n_x=n_x,\n",
    "        n_y=n_y,\n",
    "        n_tot=n_tot,\n",
    "        n_obs=n_obs,\n",
    "        split=split,\n",
    "        set_seed=set_seed,\n",
    "    )\n",
    "    features_2, outputs_2 = synthetic_nl(\n",
    "        n_x=n_x,\n",
    "        n_y=n_y,\n",
    "        n_tot=n_tot,\n",
    "        n_obs=n_obs,\n",
    "        split=split,\n",
    "        set_seed=set_seed,\n",
    "    )\n",
    "    pd.testing.assert_frame_equal(\n",
    "        features_1.data, features_2.data, \"Feature data mismatch for reproducibility.\"\n",
    "    )\n",
    "    pd.testing.assert_frame_equal(\n",
    "        outputs_1.data, outputs_2.data, \"Output data mismatch for reproducibility.\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_synthetic_nl()\n",
    "    print(\"All tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed successfully.\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "# Generate non-linear synthetic data\n",
    "####################################################################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List, Tuple, Optional\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "import pandas_datareader as pdr\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "\n",
    "\n",
    "# Function to generate synthetic data using a 3-layer neural network\n",
    "def synthetic_NN(\n",
    "    n_x: int = 5,\n",
    "    n_y: int = 10,\n",
    "    n_tot: int = 1200,\n",
    "    n_obs: int = 104,\n",
    "    split: List[float] = [0.6, 0.4],\n",
    "    set_seed: int = 45678,\n",
    ") -> Tuple[\"TrainTest\", \"TrainTest\"]:\n",
    "    \"\"\"\n",
    "    Generates synthetic (normally-distributed) factor data and mixes them using a 3-layer neural network.\n",
    "\n",
    "    :param n_x: Number of features.\n",
    "    :param n_y: Number of assets.\n",
    "    :param n_tot: Number of observations in the whole dataset.\n",
    "    :param n_obs: Number of observations per batch.\n",
    "    :param split: List of floats representing train-validation-test split percentages (must sum up to one).\n",
    "    :param set_seed: Integer seed for reproducibility.\n",
    "    :return: Tuple of TrainTest objects for feature and asset data.\n",
    "    \"\"\"\n",
    "    np.random.seed(set_seed)  # Set random seed for reproducibility\n",
    "\n",
    "    # Generate synthetic features (n_tot samples with n_x features each)\n",
    "    X: np.ndarray = (\n",
    "        np.random.randn(n_tot, n_x) * 10 + 0.5\n",
    "    )  # Scale and shift features to have varied range\n",
    "\n",
    "    # Initialize neural network\n",
    "    synth: synthetic3layer = synthetic3layer(\n",
    "        n_x, n_y, set_seed\n",
    "    ).double()  # Create an instance of the 3-layer neural network\n",
    "\n",
    "    # Generate synthetic outputs using the neural network\n",
    "    Y: torch.Tensor = synth(\n",
    "        torch.from_numpy(X)\n",
    "    )  # Convert X to a torch tensor and pass it through the neural network\n",
    "\n",
    "    # Convert synthetic features and outputs to pandas DataFrames\n",
    "    X_df: pd.DataFrame = pd.DataFrame(X)  # Convert features to DataFrame\n",
    "    Y_df: pd.DataFrame = (\n",
    "        pd.DataFrame(Y.detach().numpy()) / 10\n",
    "    )  # Convert outputs to DataFrame and scale down\n",
    "\n",
    "    # Return TrainTest objects for features and outputs\n",
    "    return TrainTest(X_df, n_obs, split), TrainTest(Y_df, n_obs, split)\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Neural Network Module\n",
    "####################################################################################################\n",
    "class synthetic3layer(nn.Module):\n",
    "    def __init__(self, n_x: int, n_y: int, set_seed: int) -> None:\n",
    "        \"\"\"\n",
    "        Initialize a 3-layer neural network to synthesize data.\n",
    "\n",
    "        :param n_x: Number of input features.\n",
    "        :param n_y: Number of output features.\n",
    "        :param set_seed: Integer seed for reproducibility.\n",
    "        \"\"\"\n",
    "        super().__init__()  # Call the parent class (nn.Module) initializer\n",
    "        torch.manual_seed(\n",
    "            set_seed\n",
    "        )  # Set random seed for torch to ensure reproducible weights\n",
    "\n",
    "        # Define a neural network with 3 hidden layers\n",
    "        self.pred_layer: nn.Sequential = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                n_x, int(0.5 * (n_x + n_y))\n",
    "            ),  # First linear layer to project input features to an intermediate size\n",
    "            nn.ReLU(),  # ReLU activation function to introduce non-linearity\n",
    "            nn.Linear(\n",
    "                int(0.5 * (n_x + n_y)), int(0.6 * (n_x + n_y))\n",
    "            ),  # Second linear layer to another intermediate size\n",
    "            nn.ReLU(),  # ReLU activation function\n",
    "            nn.Linear(\n",
    "                int(0.6 * (n_x + n_y)), n_y\n",
    "            ),  # Third linear layer to project to output size\n",
    "            nn.ReLU(),  # ReLU activation function\n",
    "            nn.Linear(n_y, n_y),  # Final linear layer to produce the final outputs\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the neural network to generate synthetic outputs.\n",
    "\n",
    "        :param X: Features. (n_obs x n_x) torch tensor with feature time series data.\n",
    "        :return: Synthetically generated output. (n_obs x n_y) torch tensor of outputs.\n",
    "        \"\"\"\n",
    "        # Apply the prediction layer to each input tensor in the batch and stack the results\n",
    "        return torch.stack([self.pred_layer(x_t) for x_t in X])\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Test code to detect bugs\n",
    "####################################################################################################\n",
    "def test_synthetic_nn() -> None:\n",
    "    try:\n",
    "        # Define parameters for the test\n",
    "        n_x = 5\n",
    "        n_y = 10\n",
    "        n_tot = 1200\n",
    "        n_obs = 104\n",
    "        split = [0.6, 0.4]\n",
    "        set_seed = 45678\n",
    "\n",
    "        # Generate synthetic data\n",
    "        features, outputs = synthetic_NN(n_x, n_y, n_tot, n_obs, split, set_seed)\n",
    "\n",
    "        # Check if generated features and outputs are pandas DataFrames\n",
    "        assert isinstance(\n",
    "            features.data, pd.DataFrame\n",
    "        ), \"Features should be a pandas DataFrame.\"\n",
    "        assert isinstance(\n",
    "            outputs.data, pd.DataFrame\n",
    "        ), \"Outputs should be a pandas DataFrame.\"\n",
    "\n",
    "        # Check the dimensions of the generated data\n",
    "        assert features.data.shape == (\n",
    "            n_tot,\n",
    "            n_x,\n",
    "        ), f\"Expected features shape {(n_tot, n_x)}, but got {features.data.shape}.\"\n",
    "        assert outputs.data.shape == (\n",
    "            n_tot,\n",
    "            n_y,\n",
    "        ), f\"Expected outputs shape {(n_tot, n_y)}, but got {outputs.data.shape}.\"\n",
    "\n",
    "        # Check reproducibility\n",
    "        features_1, outputs_1 = synthetic_NN(n_x, n_y, n_tot, n_obs, split, set_seed)\n",
    "        features_2, outputs_2 = synthetic_NN(n_x, n_y, n_tot, n_obs, split, set_seed)\n",
    "        pd.testing.assert_frame_equal(\n",
    "            features_1.data,\n",
    "            features_2.data,\n",
    "            \"Feature data mismatch for reproducibility.\",\n",
    "        )\n",
    "        pd.testing.assert_frame_equal(\n",
    "            outputs_1.data, outputs_2.data, \"Output data mismatch for reproducibility.\"\n",
    "        )\n",
    "\n",
    "        print(\"All tests passed successfully.\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test failed: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Run the test function\n",
    "if __name__ == \"__main__\":\n",
    "    test_synthetic_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# base_mod: CvxpyLayer that declares the portfolio optimization problem\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "def base_mod(n_y: int, n_obs: int, prisk: Callable) -> CvxpyLayer:\n",
    "    \"\"\"\n",
    "    Base optimization problem declared as a CvxpyLayer object.\n",
    "\n",
    "    :param n_y: Number of assets.\n",
    "    :param n_obs: Number of scenarios in the dataset.\n",
    "    :param prisk: Portfolio risk function.\n",
    "    :return: CvxpyLayer representing the optimization layer.\n",
    "    \"\"\"\n",
    "    # Variables\n",
    "    z: cp.Variable = cp.Variable((n_y, 1), nonneg=True)  # Portfolio weights (long-only positions)\n",
    "\n",
    "    # Parameters\n",
    "    y_hat: cp.Parameter = cp.Parameter(n_y)  # Predicted outcomes (e.g., expected returns)\n",
    "    \n",
    "    # Constraints\n",
    "    constraints: list[cp.Constraint] = [\n",
    "        cp.sum(z) == 1  # Budget constraint: sum of weights equals 1\n",
    "    ]\n",
    "\n",
    "    # Objective function\n",
    "    objective: cp.Minimize = cp.Minimize(-y_hat @ z)  # Maximize returns by minimizing negative expected returns\n",
    "\n",
    "    # Construct optimization problem and differentiable layer\n",
    "    problem: cp.Problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    return CvxpyLayer(problem, parameters=[y_hat], variables=[z])\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# nominal: CvxpyLayer that declares the nominal portfolio optimization problem\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "def nominal(n_y: int, n_obs: int, prisk: Callable) -> CvxpyLayer:\n",
    "    \"\"\"\n",
    "    Nominal optimization problem declared as a CvxpyLayer object.\n",
    "\n",
    "    :param n_y: Number of assets.\n",
    "    :param n_obs: Number of scenarios in the dataset.\n",
    "    :param prisk: Portfolio risk function.\n",
    "    :return: CvxpyLayer representing the optimization layer.\n",
    "    \"\"\"\n",
    "    # Variables\n",
    "    z: cp.Variable = cp.Variable((n_y, 1), nonneg=True)  # Portfolio weights (long-only positions)\n",
    "    c_aux: cp.Variable = cp.Variable()  # Auxiliary variable for risk calculation\n",
    "    obj_aux: cp.Variable = cp.Variable(n_obs)  # Objective auxiliary variable for each scenario\n",
    "    mu_aux: cp.Variable = cp.Variable()  # Expected portfolio return\n",
    "\n",
    "    # Parameters\n",
    "    ep: cp.Parameter = cp.Parameter((n_obs, n_y))  # Scenario matrix of residuals\n",
    "    y_hat: cp.Parameter = cp.Parameter(n_y)  # Predicted outcomes (e.g., expected returns)\n",
    "    gamma: cp.Parameter = cp.Parameter(nonneg=True)  # Risk aversion coefficient\n",
    "    \n",
    "    # Constraints\n",
    "    constraints: list[cp.Constraint] = [\n",
    "        cp.sum(z) == 1,  # Budget constraint: sum of weights equals 1\n",
    "        mu_aux == y_hat @ z  # Calculate expected return\n",
    "    ]\n",
    "    for i in range(n_obs):\n",
    "        constraints.append(obj_aux[i] >= prisk(z, c_aux, ep[i]))  # Add risk constraints for each scenario\n",
    "\n",
    "    # Objective function\n",
    "    objective: cp.Minimize = cp.Minimize((1 / n_obs) * cp.sum(obj_aux) - gamma * mu_aux)  # Minimize risk-adjusted return\n",
    "\n",
    "    # Construct optimization problem and differentiable layer\n",
    "    problem: cp.Problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    return CvxpyLayer(problem, parameters=[ep, y_hat, gamma], variables=[z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An unexpected error occurred: Thank you for using Alpha Vantage! This is a premium endpoint. You may subscribe to any of the premium plans at https://www.alphavantage.co/premium/ to instantly unlock all premium endpoints\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import pandas_datareader.data as pdr\n",
    "\n",
    "####################################################################################################\n",
    "# Option 4: Factors from Kenneth French's data library and asset data from AlphaVantage\n",
    "# https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html\n",
    "# https://www.alphavantage.co\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def AV(\n",
    "    start: str,\n",
    "    end: str,\n",
    "    split: list[float],\n",
    "    freq: str = \"weekly\",\n",
    "    n_obs: int = 104,\n",
    "    n_y: int | None = None,\n",
    "    use_cache: bool = False,\n",
    "    save_results: bool = False,\n",
    "    AV_key: str | None = \"YDNA9HH8P2IW985M\",\n",
    ") -> tuple[TrainTest, TrainTest]:\n",
    "    \"\"\"\n",
    "    Load data from Kenneth French's data library and from AlphaVantage.\n",
    "\n",
    "    :param start: Start date of time series.\n",
    "    :param end: End date of time series.\n",
    "    :param split: List of floats representing train-validation-test split percentages.\n",
    "    :param freq: Data frequency (daily, weekly, monthly). Default is 'weekly'.\n",
    "    :param n_obs: Number of observations per batch. Default is 104.\n",
    "    :param n_y: Number of features to select. If None, the maximum number (8) is used. Default is None.\n",
    "    :param use_cache: Whether to load cached data or download new data. Default is False.\n",
    "    :param save_results: Whether to save the data for future use. Default is False.\n",
    "    :param AV_key: AlphaVantage API key for accessing their data. Default is 'YDNA9HH8P2IW985M'.\n",
    "    :return: Tuple containing TrainTest objects for features and asset data.\n",
    "    \"\"\"\n",
    "    if use_cache:\n",
    "        # Load cached data\n",
    "        X: pd.DataFrame = pd.read_pickle(f\"./cache/factor_{freq}.pkl\")\n",
    "        Y: pd.DataFrame = pd.read_pickle(f\"./cache/asset_{freq}.pkl\")\n",
    "    else:\n",
    "        # Define list of tickers to be downloaded\n",
    "        tick_list: list[str] = [\n",
    "            \"AAPL\",\n",
    "            \"MSFT\",\n",
    "            \"AMZN\",\n",
    "            \"C\",\n",
    "            \"JPM\",\n",
    "            \"BAC\",\n",
    "            \"XOM\",\n",
    "            \"HAL\",\n",
    "            \"MCD\",\n",
    "            \"WMT\",\n",
    "            \"COST\",\n",
    "            \"CAT\",\n",
    "            \"LMT\",\n",
    "            \"JNJ\",\n",
    "            \"PFE\",\n",
    "            \"DIS\",\n",
    "            \"VZ\",\n",
    "            \"T\",\n",
    "            \"ED\",\n",
    "            \"NEM\",\n",
    "        ]\n",
    "\n",
    "        # Select the first n_y tickers if specified\n",
    "        if n_y is not None:\n",
    "            tick_list = tick_list[:n_y]\n",
    "\n",
    "        # Ensure API key is provided\n",
    "        if AV_key is None:\n",
    "            print(\n",
    "                \"\"\"A personal AlphaVantage API key is required to load the asset pricing data.\n",
    "                If you do not have a key, you can get one from www.alphavantage.co (free for academic users)\"\"\"\n",
    "            )\n",
    "            AV_key = input(\"Enter your AlphaVantage API key: \")\n",
    "\n",
    "        # Initialize TimeSeries object to interact with AlphaVantage API\n",
    "        ts: TimeSeries = TimeSeries(\n",
    "            key=AV_key, output_format=\"pandas\", indexing_type=\"date\"\n",
    "        )\n",
    "\n",
    "        # Download asset data from AlphaVantage\n",
    "        Y_list: list[pd.Series] = []\n",
    "        for tick in tick_list:\n",
    "            data, _ = ts.get_daily_adjusted(symbol=tick, outputsize=\"full\")\n",
    "            data = data[\"5. adjusted close\"]\n",
    "            Y_list.append(data)\n",
    "            time.sleep(12.5)  # AlphaVantage rate limit to avoid API ban\n",
    "        Y: pd.DataFrame = pd.concat(Y_list, axis=1)\n",
    "        Y = Y[::-1]\n",
    "        Y = Y.loc[\"1999-01-01\":end].pct_change()\n",
    "        Y = Y.loc[start:end]\n",
    "        Y.columns = tick_list\n",
    "\n",
    "        # Download factor data from Kenneth French's library\n",
    "        dl_freq: str = \"_daily\"\n",
    "        X: pd.DataFrame = pdr.get_data_famafrench(\n",
    "            f\"F-F_Research_Data_5_Factors_2x3{dl_freq}\", start=start, end=end\n",
    "        )[0]\n",
    "        rf_df: pd.Series = X[\"RF\"]\n",
    "        X = X.drop([\"RF\"], axis=1)\n",
    "        mom_df: pd.DataFrame = pdr.get_data_famafrench(\n",
    "            f\"F-F_Momentum_Factor{dl_freq}\", start=start, end=end\n",
    "        )[0]\n",
    "        st_df: pd.DataFrame = pdr.get_data_famafrench(\n",
    "            f\"F-F_ST_Reversal_Factor{dl_freq}\", start=start, end=end\n",
    "        )[0]\n",
    "        lt_df: pd.DataFrame = pdr.get_data_famafrench(\n",
    "            f\"F-F_LT_Reversal_Factor{dl_freq}\", start=start, end=end\n",
    "        )[0]\n",
    "\n",
    "        # Concatenate all factors into a single DataFrame\n",
    "        X = pd.concat([X, mom_df, st_df, lt_df], axis=1) / 100\n",
    "\n",
    "        # Convert daily returns to weekly returns if specified\n",
    "        if freq in [\"weekly\", \"_weekly\"]:\n",
    "            Y = Y.resample(\"W-FRI\").agg(lambda x: (x + 1).prod() - 1)\n",
    "            X = X.resample(\"W-FRI\").agg(lambda x: (x + 1).prod() - 1)\n",
    "\n",
    "        # Save the data if requested\n",
    "        if save_results:\n",
    "            X.to_pickle(f\"./cache/factor_{freq}.pkl\")\n",
    "            Y.to_pickle(f\"./cache/asset_{freq}.pkl\")\n",
    "\n",
    "    # Partition dataset into training and testing sets. Lag the data by one observation\n",
    "    return TrainTest(X[:-1], n_obs, split), TrainTest(Y[1:], n_obs, split)\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Test code to verify functionality and detect bugs\n",
    "####################################################################################################\n",
    "def test_AV() -> None:\n",
    "    \"\"\"\n",
    "    Test the AV function to ensure correctness of generated data.\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define parameters for the test\n",
    "        start = \"2020-01-01\"\n",
    "        end = \"2023-01-01\"\n",
    "        split = [0.7, 0.3]\n",
    "        freq = \"weekly\"\n",
    "        n_obs = 104\n",
    "        n_y = 5\n",
    "        use_cache = False\n",
    "        save_results = False\n",
    "        AV_key = \"YDNA9HH8P2IW985M\"\n",
    "\n",
    "        # Generate synthetic data\n",
    "        features, outputs = AV(\n",
    "            start, end, split, freq, n_obs, n_y, use_cache, save_results, AV_key\n",
    "        )\n",
    "\n",
    "        # Check if generated features and outputs are pandas DataFrames\n",
    "        assert isinstance(\n",
    "            features.data, pd.DataFrame\n",
    "        ), \"Features should be a pandas DataFrame.\"\n",
    "        assert isinstance(\n",
    "            outputs.data, pd.DataFrame\n",
    "        ), \"Outputs should be a pandas DataFrame.\"\n",
    "\n",
    "        # Check the dimensions of the generated data\n",
    "        assert (\n",
    "            features.data.shape[0] == outputs.data.shape[0]\n",
    "        ), \"Features and outputs should have matching number of rows.\"\n",
    "\n",
    "        print(\"All tests passed successfully.\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test failed: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Run the test function\n",
    "if __name__ == \"__main__\":\n",
    "    test_AV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Statistical Analysis Function\n",
    "####################################################################################################\n",
    "def statanalysis(X: pd.DataFrame, Y: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Conduct a pairwise statistical significance analysis of each feature in X against each asset in Y.\n",
    "\n",
    "    :param X: DataFrame containing the time series of features (independent variables).\n",
    "    :param Y: DataFrame containing the time series of asset returns (dependent variables).\n",
    "    :return: DataFrame containing p-values obtained from regressing each individual feature against each individual asset.\n",
    "    \"\"\"\n",
    "    # Initialize an empty DataFrame to store p-values\n",
    "    stats: pd.DataFrame = pd.DataFrame(columns=X.columns, index=Y.columns)\n",
    "\n",
    "    # Iterate over each asset (Y) and each feature (X) to perform OLS regression\n",
    "    # OLS (Ordinary Least Squares) is used to estimate the relationship between each feature and asset return.\n",
    "    # For each asset, regress it against each feature with a constant term to obtain the p-value.\n",
    "    for ticker in Y.columns:\n",
    "        for feature in X.columns:\n",
    "            # Perform OLS regression and store the p-value of the feature\n",
    "            stats.loc[ticker, feature] = (\n",
    "                sm.OLS(Y[ticker].values, sm.add_constant(X[feature]).values)\n",
    "                .fit()\n",
    "                .pvalues[1]\n",
    "            )\n",
    "\n",
    "    # Convert p-values to float and round to two decimal places\n",
    "    return stats.astype(float).round(2)\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Test code to verify functionality and avoid bugs\n",
    "####################################################################################################\n",
    "def test_statanalysis() -> None:\n",
    "    \"\"\"\n",
    "    Test the statanalysis function to ensure it correctly calculates p-values for feature-asset relationships.\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate random data for testing\n",
    "        np.random.seed(42)\n",
    "        X_test = pd.DataFrame(\n",
    "            np.random.randn(100, 5), columns=[f\"Feature_{i+1}\" for i in range(5)]\n",
    "        )\n",
    "        Y_test = pd.DataFrame(\n",
    "            np.random.randn(100, 3), columns=[f\"Asset_{i+1}\" for i in range(3)]\n",
    "        )\n",
    "\n",
    "        # Run the statistical analysis\n",
    "        stats_result = statanalysis(X_test, Y_test)\n",
    "\n",
    "        # Check if the result is a DataFrame\n",
    "        assert isinstance(\n",
    "            stats_result, pd.DataFrame\n",
    "        ), \"The result should be a pandas DataFrame.\"\n",
    "\n",
    "        # Check if the dimensions of the result match expectations\n",
    "        assert stats_result.shape == (\n",
    "            3,\n",
    "            5,\n",
    "        ), f\"Expected shape (3, 5), but got {stats_result.shape}.\"\n",
    "\n",
    "        # Check if all p-values are between 0 and 1\n",
    "        assert (\n",
    "            stats_result.map(lambda x: 0 <= x <= 1).all().all()\n",
    "        ), \"All p-values should be between 0 and 1.\"\n",
    "\n",
    "        print(\"All tests passed successfully.\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"Test failed: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Run the test function\n",
    "if __name__ == \"__main__\":\n",
    "    test_statanalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "num_cores: int = (\n",
    "    psutil.cpu_count()\n",
    ")  # Get the number of CPU cores available in the system.\n",
    "torch.set_num_threads(\n",
    "    num_cores\n",
    ")  # Set the number of threads for PyTorch to utilize all CPU cores.\n",
    "if psutil.MACOS:\n",
    "    num_cores = 0  # Set num_cores to 0 on macOS systems (specific behavior).\n",
    "\n",
    "num_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# base_mod: CvxpyLayer that declares the portfolio optimization problem\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "def base_mod(n_y: int, n_obs: int, prisk: Callable) -> CvxpyLayer:\n",
    "    \"\"\"\n",
    "    Base optimization problem declared as a CvxpyLayer object.\n",
    "\n",
    "    :param n_y: Number of assets.\n",
    "    :param n_obs: Number of scenarios in the dataset.\n",
    "    :param prisk: Portfolio risk function.\n",
    "    :return: CvxpyLayer representing the optimization layer.\n",
    "    \"\"\"\n",
    "    # Variables\n",
    "    z: cp.Variable = cp.Variable(\n",
    "        (n_y, 1), nonneg=True\n",
    "    )  # Portfolio weights (long-only positions)\n",
    "\n",
    "    # Parameters\n",
    "    y_hat: cp.Parameter = cp.Parameter(\n",
    "        n_y\n",
    "    )  # Predicted outcomes (e.g., expected returns)\n",
    "\n",
    "    # Constraints\n",
    "    constraints: list[cp.Constraint] = [\n",
    "        cp.sum(z) == 1  # Budget constraint: sum of weights equals 1\n",
    "    ]\n",
    "\n",
    "    # Objective function\n",
    "    objective: cp.Minimize = cp.Minimize(\n",
    "        -y_hat @ z\n",
    "    )  # Maximize returns by minimizing negative expected returns\n",
    "\n",
    "    # Construct optimization problem and differentiable layer\n",
    "    problem: cp.Problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    return CvxpyLayer(problem, parameters=[y_hat], variables=[z])\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# nominal: CvxpyLayer that declares the nominal portfolio optimization problem\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "def nominal(n_y: int, n_obs: int, prisk: Callable) -> CvxpyLayer:\n",
    "    \"\"\"\n",
    "    Nominal optimization problem declared as a CvxpyLayer object.\n",
    "\n",
    "    :param n_y: Number of assets.\n",
    "    :param n_obs: Number of scenarios in the dataset.\n",
    "    :param prisk: Portfolio risk function.\n",
    "    :return: CvxpyLayer representing the optimization layer.\n",
    "    \"\"\"\n",
    "    # Variables\n",
    "    z: cp.Variable = cp.Variable(\n",
    "        (n_y, 1), nonneg=True\n",
    "    )  # Portfolio weights (long-only positions)\n",
    "    c_aux: cp.Variable = cp.Variable()  # Auxiliary variable for risk calculation\n",
    "    obj_aux: cp.Variable = cp.Variable(\n",
    "        n_obs\n",
    "    )  # Objective auxiliary variable for each scenario\n",
    "    mu_aux: cp.Variable = cp.Variable()  # Expected portfolio return\n",
    "\n",
    "    # Parameters\n",
    "    ep: cp.Parameter = cp.Parameter((n_obs, n_y))  # Scenario matrix of residuals\n",
    "    y_hat: cp.Parameter = cp.Parameter(\n",
    "        n_y\n",
    "    )  # Predicted outcomes (e.g., expected returns)\n",
    "    gamma: cp.Parameter = cp.Parameter(nonneg=True)  # Risk aversion coefficient\n",
    "\n",
    "    # Constraints\n",
    "    constraints: list[cp.Constraint] = [\n",
    "        cp.sum(z) == 1,  # Budget constraint: sum of weights equals 1\n",
    "        mu_aux == y_hat @ z,  # Calculate expected return\n",
    "    ]\n",
    "    for i in range(n_obs):\n",
    "        constraints.append(\n",
    "            obj_aux[i] >= prisk(z, c_aux, ep[i])\n",
    "        )  # Add risk constraints for each scenario\n",
    "\n",
    "    # Objective function\n",
    "    objective: cp.Minimize = cp.Minimize(\n",
    "        (1 / n_obs) * cp.sum(obj_aux) - gamma * mu_aux\n",
    "    )  # Minimize risk-adjusted return\n",
    "\n",
    "    # Construct optimization problem and differentiable layer\n",
    "    problem: cp.Problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    return CvxpyLayer(problem, parameters=[ep, y_hat, gamma], variables=[z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal portfolio weights (base): tensor([[1.0001e-06],\n",
      "        [3.1529e-06],\n",
      "        [1.0000e+00]])\n",
      "Optimal portfolio weights (nominal): tensor([[0.4170],\n",
      "        [0.2629],\n",
      "        [0.3202]])\n",
      "Optimal portfolio weights (TV): tensor([[0.5549],\n",
      "        [0.1979],\n",
      "        [0.2472]])\n",
      "Optimal portfolio weights (Hellinger): tensor([[0.3123],\n",
      "        [0.4710],\n",
      "        [0.2167]])\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Total Variation: sum_t abs(p_t - q_t) <= delta\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "def tv(n_y: int, n_obs: int, prisk: Callable) -> CvxpyLayer:\n",
    "    \"\"\"\n",
    "    DRO layer using the 'Total Variation' distance to define the probability ambiguity set.\n",
    "    From Ben-Tal et al. (2013).\n",
    "\n",
    "    :param n_y: Number of assets.\n",
    "    :param n_obs: Number of scenarios in the dataset.\n",
    "    :param prisk: Portfolio risk function.\n",
    "    :return: CvxpyLayer representing the optimization layer.\n",
    "    \"\"\"\n",
    "    # Variables\n",
    "    z: cp.Variable = cp.Variable(\n",
    "        (n_y, 1), nonneg=True\n",
    "    )  # Decision variable representing portfolio weights with nonnegative constraint.\n",
    "    c_aux: cp.Variable = (\n",
    "        cp.Variable()\n",
    "    )  # Auxiliary variable used in risk function calculation.\n",
    "    lambda_aux: cp.Variable = cp.Variable(\n",
    "        nonneg=True\n",
    "    )  # Auxiliary variable representing Lagrange multiplier for the Total Variation constraint (nonnegative).\n",
    "    eta_aux: cp.Variable = (\n",
    "        cp.Variable()\n",
    "    )  # Auxiliary variable representing an additional term in the optimization.\n",
    "    beta_aux: cp.Variable = cp.Variable(\n",
    "        n_obs\n",
    "    )  # Auxiliary variable representing scenario-specific values for optimization.\n",
    "    mu_aux: cp.Variable = (\n",
    "        cp.Variable()\n",
    "    )  # Auxiliary variable representing the expected return of the portfolio.\n",
    "\n",
    "    # Parameters\n",
    "    ep: cp.Parameter = cp.Parameter(\n",
    "        (n_obs, n_y)\n",
    "    )  # Parameter representing the different scenarios for asset returns.\n",
    "    y_hat: cp.Parameter = cp.Parameter(\n",
    "        n_y\n",
    "    )  # Parameter representing the expected returns for each asset.\n",
    "    gamma: cp.Parameter = cp.Parameter(\n",
    "        nonneg=True\n",
    "    )  # Parameter representing the risk-aversion coefficient (nonnegative).\n",
    "    delta: cp.Parameter = cp.Parameter(\n",
    "        nonneg=True\n",
    "    )  # Parameter representing the size of the ambiguity set for Total Variation (nonnegative).\n",
    "\n",
    "    # Constraints\n",
    "    constraints: list[cp.Constraint] = [\n",
    "        cp.sum(z) == 1,\n",
    "        beta_aux >= -lambda_aux,\n",
    "        mu_aux == y_hat @ z,\n",
    "    ]  # Constraints for portfolio weights, auxiliary variables, and expected return.\n",
    "    for i in range(n_obs):\n",
    "        constraints.append(\n",
    "            beta_aux[i] >= prisk(z, c_aux, ep[i]) - eta_aux\n",
    "        )  # Constraint to enforce risk bounds for each scenario.\n",
    "        constraints.append(\n",
    "            lambda_aux >= prisk(z, c_aux, ep[i]) - eta_aux\n",
    "        )  # Constraint for Lagrange multiplier to handle variation.\n",
    "\n",
    "    # Objective function\n",
    "    objective: cp.Minimize = cp.Minimize(\n",
    "        eta_aux + delta * lambda_aux + (1 / n_obs) * cp.sum(beta_aux) - gamma * mu_aux\n",
    "    )  # Objective to minimize the sum of risk terms and maximize expected return.\n",
    "\n",
    "    # Construct optimization problem and differentiable layer\n",
    "    problem: cp.Problem = cp.Problem(\n",
    "        objective, constraints\n",
    "    )  # Define the optimization problem with the objective and constraints.\n",
    "\n",
    "    return CvxpyLayer(\n",
    "        problem, parameters=[ep, y_hat, gamma, delta], variables=[z]\n",
    "    )  # Return the CvxpyLayer that represents the optimization problem.\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "# Hellinger distance: sum_t (sqrt(p_t) - sqrt(q_t))^2 <= delta\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "def hellinger(n_y: int, n_obs: int, prisk: Callable) -> CvxpyLayer:\n",
    "    \"\"\"\n",
    "    DRO layer using the Hellinger distance to define the probability ambiguity set.\n",
    "    From Ben-Tal et al. (2013).\n",
    "\n",
    "    :param n_y: Number of assets.\n",
    "    :param n_obs: Number of scenarios in the dataset.\n",
    "    :param prisk: Portfolio risk function.\n",
    "    :return: CvxpyLayer representing the optimization layer.\n",
    "    \"\"\"\n",
    "    # Variables\n",
    "    z: cp.Variable = cp.Variable(\n",
    "        (n_y, 1), nonneg=True\n",
    "    )  # Decision variable representing portfolio weights with nonnegative constraint.\n",
    "    c_aux: cp.Variable = (\n",
    "        cp.Variable()\n",
    "    )  # Auxiliary variable used in risk function calculation.\n",
    "    lambda_aux: cp.Variable = cp.Variable(\n",
    "        nonneg=True\n",
    "    )  # Auxiliary variable representing Lagrange multiplier for Hellinger constraint (nonnegative).\n",
    "    xi_aux: cp.Variable = (\n",
    "        cp.Variable()\n",
    "    )  # Auxiliary variable representing an additional term in the optimization.\n",
    "    beta_aux: cp.Variable = cp.Variable(\n",
    "        n_obs, nonneg=True\n",
    "    )  # Scenario-specific auxiliary variable for optimization with nonnegative constraint.\n",
    "    tau_aux: cp.Variable = cp.Variable(\n",
    "        n_obs, nonneg=True\n",
    "    )  # Auxiliary variable for scenario-specific terms, nonnegative.\n",
    "    mu_aux: cp.Variable = (\n",
    "        cp.Variable()\n",
    "    )  # Auxiliary variable representing the expected return of the portfolio.\n",
    "\n",
    "    # Parameters\n",
    "    ep: cp.Parameter = cp.Parameter(\n",
    "        (n_obs, n_y)\n",
    "    )  # Parameter representing the different scenarios for asset returns.\n",
    "    y_hat: cp.Parameter = cp.Parameter(\n",
    "        n_y\n",
    "    )  # Parameter representing the expected returns for each asset.\n",
    "    gamma: cp.Parameter = cp.Parameter(\n",
    "        nonneg=True\n",
    "    )  # Parameter representing the risk-aversion coefficient (nonnegative).\n",
    "    delta: cp.Parameter = cp.Parameter(\n",
    "        nonneg=True\n",
    "    )  # Parameter representing the size of the ambiguity set for Hellinger distance (nonnegative).\n",
    "\n",
    "    # Constraints\n",
    "    constraints: list[cp.Constraint] = [\n",
    "        cp.sum(z) == 1,\n",
    "        mu_aux == y_hat @ z,\n",
    "    ]  # Constraints to ensure portfolio weights sum to 1 and compute expected return.\n",
    "    for i in range(n_obs):\n",
    "        constraints.append(\n",
    "            xi_aux + lambda_aux >= prisk(z, c_aux, ep[i]) + tau_aux[i]\n",
    "        )  # Constraint to handle Hellinger distance using auxiliary variables.\n",
    "        constraints.append(\n",
    "            beta_aux[i] >= cp.quad_over_lin(lambda_aux, tau_aux[i])\n",
    "        )  # Quadratic constraint for scenario-specific optimization.\n",
    "\n",
    "    # Objective function\n",
    "    objective: cp.Minimize = cp.Minimize(\n",
    "        xi_aux\n",
    "        + (delta - 1) * lambda_aux\n",
    "        + (1 / n_obs) * cp.sum(beta_aux)\n",
    "        - gamma * mu_aux\n",
    "    )  # Objective to minimize the sum of risk terms and maximize expected return.\n",
    "\n",
    "    # Construct optimization problem and differentiable layer\n",
    "    problem: cp.Problem = cp.Problem(\n",
    "        objective, constraints\n",
    "    )  # Define the optimization problem with the objective and constraints.\n",
    "\n",
    "    return CvxpyLayer(\n",
    "        problem, parameters=[ep, y_hat, gamma, delta], variables=[z]\n",
    "    )  # Return the CvxpyLayer that represents the optimization problem.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage of the base optimization problem\n",
    "    base_layer: CvxpyLayer = base_mod(3, 10, p_var)\n",
    "    y_hat: torch.Tensor = torch.randn(3)\n",
    "    z_star: torch.Tensor = base_layer(y_hat)[0]\n",
    "    print(f\"Optimal portfolio weights (base): {z_star}\")\n",
    "\n",
    "    # Example usage of the nominal optimization problem\n",
    "    nominal_layer: CvxpyLayer = nominal(3, 10, p_var)\n",
    "    y_hat: torch.Tensor = torch.randn(3)\n",
    "    ep: torch.Tensor = torch.randn(10, 3)\n",
    "    gamma: torch.Tensor = torch.tensor(0.1)\n",
    "    z_star: torch.Tensor = nominal_layer(ep, y_hat, gamma)[0]\n",
    "    print(f\"Optimal portfolio weights (nominal): {z_star}\")\n",
    "\n",
    "    # Example usage of the TV optimization problem\n",
    "    tv_layer: CvxpyLayer = tv(3, 10, p_var)\n",
    "    y_hat: torch.Tensor = torch.randn(3)\n",
    "    ep: torch.Tensor = torch.randn(10, 3)\n",
    "    gamma: torch.Tensor = torch.tensor(0.1)\n",
    "    delta: torch.Tensor = torch.tensor(0.1)\n",
    "    z_star: torch.Tensor = tv_layer(ep, y_hat, gamma, delta)[0]\n",
    "    print(f\"Optimal portfolio weights (TV): {z_star}\")\n",
    "\n",
    "    # Example usage of the Hellinger optimization problem\n",
    "    hellinger_layer: CvxpyLayer = hellinger(3, 10, p_var)\n",
    "    y_hat: torch.Tensor = torch.randn(3)\n",
    "    ep: torch.Tensor = torch.randn(10, 3)\n",
    "    gamma: torch.Tensor = torch.tensor(0.1)\n",
    "    delta: torch.Tensor = torch.tensor(0.1)\n",
    "    z_star: torch.Tensor = hellinger_layer(ep, y_hat, gamma, delta)[0]\n",
    "    print(f\"Optimal portfolio weights (Hellinger): {z_star}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores: 16\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "# Import libraries\n",
    "####################################################################################################\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from collections.abc import Callable\n",
    "import psutil\n",
    "\n",
    "# Set the number of CPU cores for PyTorch\n",
    "num_cores: int = psutil.cpu_count()\n",
    "torch.set_num_threads(num_cores)\n",
    "if psutil.MACOS:\n",
    "    num_cores = 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Number of cores: {num_cores}\")\n",
    "\n",
    "# Check if CUDA is available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# E2E neural network module\n",
    "####################################################################################################\n",
    "class e2e_net(nn.Module):\n",
    "    \"\"\"End-to-end DRO learning neural network module.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_x: int,\n",
    "        n_y: int,\n",
    "        n_obs: int,\n",
    "        opt_layer: str = \"nominal\",\n",
    "        prisk: str = \"p_var\",\n",
    "        perf_loss: str = \"sharpe_loss\",\n",
    "        pred_model: str = \"linear\",\n",
    "        pred_loss_factor: float = 0.5,\n",
    "        perf_period: int = 13,\n",
    "        train_pred: bool = True,\n",
    "        train_gamma: bool = True,\n",
    "        train_delta: bool = True,\n",
    "        set_seed: int | None = None,\n",
    "        cache_path: str = \"./cache/\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the End-to-End Learning Neural Net module.\n",
    "\n",
    "        This module consists of a prediction layer ('pred_layer') and an optimization layer ('opt_layer').\n",
    "        It supports different models such as linear, 2-layer, and 3-layer prediction models. Gamma and\n",
    "        delta parameters can be learned during training.\n",
    "\n",
    "        :param n_x: Number of input features\n",
    "        :param n_y: Number of output assets\n",
    "        :param n_obs: Number of scenarios\n",
    "        :param opt_layer: Optimization layer to use (\"nominal\", \"base_mod\", \"hellinger\")\n",
    "        :param prisk: Portfolio risk function used in the optimization layer\n",
    "        :param perf_loss: Performance loss function\n",
    "        :param pred_model: Type of prediction model (\"linear\", \"2layer\", \"3layer\")\n",
    "        :param pred_loss_factor: Weight factor between prediction loss and performance loss\n",
    "        :param perf_period: Number of lookahead periods used in 'perf_loss'\n",
    "        :param train_pred: Boolean to allow training of the prediction layer\n",
    "        :param train_gamma: Boolean to allow training of the 'gamma' parameter\n",
    "        :param train_delta: Boolean to allow training of the 'delta' parameter\n",
    "        :param set_seed: Optional int to set random seed for replicability\n",
    "        :param cache_path: Path to store model data\n",
    "        \"\"\"\n",
    "        super(e2e_net, self).__init__()\n",
    "\n",
    "        # Set random seed for reproducibility\n",
    "        if set_seed is not None:\n",
    "            torch.manual_seed(set_seed)\n",
    "            self.seed = set_seed\n",
    "\n",
    "        self.n_x: int = n_x\n",
    "        self.n_y: int = n_y\n",
    "        self.n_obs: int = n_obs\n",
    "\n",
    "        # Set prediction loss function if 'pred_loss_factor' is defined\n",
    "        if pred_loss_factor is not None:\n",
    "            self.pred_loss_factor = pred_loss_factor\n",
    "            self.pred_loss = (\n",
    "                torch.nn.MSELoss()\n",
    "            )  # Mean Squared Error Loss for prediction\n",
    "        else:\n",
    "            self.pred_loss = None\n",
    "\n",
    "        # Set performance loss function\n",
    "        if perf_loss == \"sharpe_loss\":\n",
    "            self.perf_loss = sharpe_loss\n",
    "        elif perf_loss == \"single_period_over_var_loss\":\n",
    "            self.perf_loss = single_period_over_var_loss\n",
    "        elif perf_loss == \"single_period_loss\":\n",
    "            self.perf_loss = single_period_loss\n",
    "        else:\n",
    "            # Exception for unsupported performance loss function\n",
    "            raise Exception(\n",
    "                \"Unsupported performance loss function. Choose from 'sharpe_loss', 'single_period_over_var_loss', 'single_period_loss'.\"\n",
    "            )\n",
    "        # Number of lookahead steps for performance evaluation\n",
    "        self.perf_period = perf_period\n",
    "\n",
    "        # Initialize the gamma parameter (risk-return trade-off)\n",
    "        self.gamma = nn.Parameter(torch.FloatTensor(1).uniform_(0.02, 0.1))\n",
    "        self.gamma.requires_grad = train_gamma\n",
    "        self.gamma_init = self.gamma.item()\n",
    "\n",
    "        # Select the optimization model\n",
    "        if opt_layer == \"nominal\":\n",
    "            self.model_type = \"nom\"\n",
    "        elif opt_layer == \"base_mod\":\n",
    "            self.gamma.requires_grad = False  # 'gamma' not trainable in base model\n",
    "            self.model_type = \"base_mod\"\n",
    "        else:\n",
    "            # Initialize delta parameter for DRO layer\n",
    "            if opt_layer == \"hellinger\":\n",
    "                ub = (1 - 1 / (n_obs**0.5)) / 2\n",
    "                lb = (1 - 1 / (n_obs**0.5)) / 10\n",
    "            else:\n",
    "                ub = (1 - 1 / n_obs) / 2\n",
    "                lb = (1 - 1 / n_obs) / 10\n",
    "            self.delta = nn.Parameter(torch.FloatTensor(1).uniform_(lb, ub))\n",
    "            self.delta.requires_grad = train_delta\n",
    "            self.delta_init = self.delta.item()\n",
    "            self.model_type = \"dro\"\n",
    "\n",
    "        # LAYER: Prediction model\n",
    "        self.pred_model = pred_model\n",
    "        if pred_model == \"linear\":\n",
    "            self.pred_layer = nn.Linear(n_x, n_y)  # Linear layer\n",
    "            self.pred_layer.weight.requires_grad = train_pred\n",
    "            self.pred_layer.bias.requires_grad = train_pred\n",
    "        elif pred_model == \"2layer\":\n",
    "            self.pred_layer = nn.Sequential(  # Two hidden layers\n",
    "                nn.Linear(n_x, int(0.5 * (n_x + n_y))),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(int(0.5 * (n_x + n_y)), n_y),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_y, n_y),\n",
    "            )\n",
    "        elif pred_model == \"3layer\":\n",
    "            self.pred_layer = nn.Sequential(  # Three hidden layers\n",
    "                nn.Linear(n_x, int(0.5 * (n_x + n_y))),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(int(0.5 * (n_x + n_y)), int(0.6 * (n_x + n_y))),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(int(0.6 * (n_x + n_y)), n_y),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_y, n_y),\n",
    "            )\n",
    "\n",
    "        # LAYER: Optimization model using CvxpyLayer\n",
    "        self.opt_layer = eval(f\"{opt_layer}\")(n_y, n_obs, eval(f\"rf.{prisk}\"))\n",
    "\n",
    "        # Store the path for model data caching\n",
    "        self.cache_path = cache_path\n",
    "        self._save_initial_state(train_gamma, train_delta)\n",
    "\n",
    "    def _save_initial_state(self, train_gamma: bool, train_delta: bool) -> None:\n",
    "        \"\"\"Stores the initial state of the model for reuse during training.\"\"\"\n",
    "        # Determine the initial model save path based on gamma and delta trainability\n",
    "        if train_gamma and train_delta:\n",
    "            self.init_state_path = (\n",
    "                f\"{self.cache_path}{self.model_type}_initial_state_{self.pred_model}\"\n",
    "            )\n",
    "        elif train_delta and not train_gamma:\n",
    "            self.init_state_path = f\"{self.cache_path}{self.model_type}_initial_state_{self.pred_model}_TrainGamma{train_gamma}\"\n",
    "        elif train_gamma and not train_delta:\n",
    "            self.init_state_path = f\"{self.cache_path}{self.model_type}_initial_state_{self.pred_model}_TrainDelta{train_delta}\"\n",
    "        else:\n",
    "            self.init_state_path = f\"{self.cache_path}{self.model_type}_initial_state_{self.pred_model}_TrainGamma{train_gamma}_TrainDelta{train_delta}\"\n",
    "        torch.save(self.state_dict(), self.init_state_path)\n",
    "\n",
    "    ################################################################################################\n",
    "    # forward: forward pass of the e2e neural net\n",
    "    ################################################################################################\n",
    "    def forward(\n",
    "        self, X: torch.Tensor, Y: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the NN module.\n",
    "\n",
    "        Passes inputs 'X' through the prediction layer to yield predictions 'Y_hat'. Residuals are\n",
    "        calculated as 'ep = Y - Y_hat', and the residuals are passed to the optimization layer to\n",
    "        find the optimal decision 'z_star'.\n",
    "\n",
    "        :param X: Features (n_obs+1 x n_x) tensor of feature time series data\n",
    "        :param Y: Realizations (n_obs x n_y) tensor of asset time series data\n",
    "        :return: (y_hat, z_star) - Prediction and optimized decision vector (asset weights)\n",
    "        \"\"\"\n",
    "        # Multiple predictions Y_hat from X\n",
    "        Y_hat = torch.stack([self.pred_layer(x_t) for x_t in X])\n",
    "\n",
    "        # Calculate residuals\n",
    "        ep = Y - Y_hat[:-1]\n",
    "        y_hat = Y_hat[-1]\n",
    "\n",
    "        # Optimization solver arguments\n",
    "        solver_args = {\"solve_method\": \"ECOS\", \"max_iters\": 120, \"abstol\": 1e-7}\n",
    "\n",
    "        # Optimize z based on the model type\n",
    "        if self.model_type == \"nom\":\n",
    "            (z_star,) = self.opt_layer(ep, y_hat, self.gamma, solver_args=solver_args)\n",
    "        elif self.model_type == \"dro\":\n",
    "            (z_star,) = self.opt_layer(\n",
    "                ep, y_hat, self.gamma, self.delta, solver_args=solver_args\n",
    "            )\n",
    "        elif self.model_type == \"base_mod\":\n",
    "            (z_star,) = self.opt_layer(y_hat, solver_args=solver_args)\n",
    "\n",
    "        return z_star, y_hat\n",
    "\n",
    "    ################################################################################################\n",
    "    # net_train: Train the e2e neural net\n",
    "    ################################################################################################\n",
    "    def net_train(\n",
    "        self,\n",
    "        train_set: DataLoader,\n",
    "        val_set: DataLoader | None = None,\n",
    "        epochs: int | None = None,\n",
    "        lr: float | None = None,\n",
    "    ) -> float | None:\n",
    "        \"\"\"\n",
    "        Train the neural net model.\n",
    "\n",
    "        :param train_set: Training data (SlidingWindow object)\n",
    "        :param val_set: Validation data (SlidingWindow object), optional\n",
    "        :param epochs: Number of training epochs\n",
    "        :param lr: Learning rate\n",
    "        :return: Validation loss (optional)\n",
    "        \"\"\"\n",
    "        # Set default epochs and learning rate if not provided\n",
    "        if epochs is None:\n",
    "            epochs = self.epochs\n",
    "        if lr is None:\n",
    "            lr = self.lr\n",
    "\n",
    "        # Optimizer configuration\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        n_train = len(train_set)  # Number of training samples\n",
    "\n",
    "        # Train the neural network\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = 0\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for t, (x, y, y_perf) in enumerate(train_set):\n",
    "                z_star, y_hat = self(x.squeeze(), y.squeeze())\n",
    "\n",
    "                # Compute the loss\n",
    "                if self.pred_loss is None:\n",
    "                    loss = (1 / n_train) * self.perf_loss(z_star, y_perf.squeeze())\n",
    "                else:\n",
    "                    loss = (1 / n_train) * (\n",
    "                        self.perf_loss(z_star, y_perf.squeeze())\n",
    "                        + (self.pred_loss_factor / self.n_y)\n",
    "                        * self.pred_loss(y_hat, y_perf.squeeze()[0])\n",
    "                    )\n",
    "\n",
    "                # Backpropagate the loss\n",
    "                loss.backward()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Ensure gamma, delta are non-negative\n",
    "            self._clamp_params()\n",
    "\n",
    "        # Validate the model\n",
    "        if val_set is not None:\n",
    "            return self._validate_model(val_set)\n",
    "\n",
    "    def _clamp_params(self) -> None:\n",
    "        \"\"\"Clamps the gamma and delta parameters to ensure non-negativity.\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if name == \"gamma\":\n",
    "                param.data.clamp_(0.0001)\n",
    "            if name == \"delta\":\n",
    "                param.data.clamp_(0.0001)\n",
    "\n",
    "    def _validate_model(self, val_set: DataLoader) -> float:\n",
    "        \"\"\"Compute validation loss for the given validation set.\"\"\"\n",
    "        n_val = len(val_set)\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for t, (x, y, y_perf) in enumerate(val_set):\n",
    "                z_val, y_val = self(x.squeeze(), y.squeeze())\n",
    "                if self.pred_loss is None:\n",
    "                    loss = (1 / n_val) * self.perf_loss(z_val, y_perf.squeeze())\n",
    "                else:\n",
    "                    loss = (1 / n_val) * (\n",
    "                        self.perf_loss(z_val, y_perf.squeeze())\n",
    "                        + (self.pred_loss_factor / self.n_y)\n",
    "                        * self.pred_loss(y_val, y_perf.squeeze()[0])\n",
    "                    )\n",
    "                val_loss += loss.item()\n",
    "        return val_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
