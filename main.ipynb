{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")  # close all previous plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "cache_path: str = \"./cache/exp/\"\n",
    "data_frequency = \"weekly\"\n",
    "start = \"2000-01-01\"\n",
    "end = \"2021-09-30\"  # Data frequency and start/end dates\n",
    "split_ratio_list = [0.6, 0.4]  # Train, validation and test split percentage\n",
    "number_of_observe_per_window: int = 104\n",
    "number_of_asset: int = 20  # Number of assets n_y = 20\n",
    "AV_key: str = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTest:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        number_of_observation_per_window: int,\n",
    "        split_ratio_list: list[float],\n",
    "    ) -> None:\n",
    "        self.data: pd.DataFrame = data\n",
    "        self.number_of_observation_per_window: int = number_of_observation_per_window\n",
    "        self.split_ratio: list[float] = split_ratio_list\n",
    "\n",
    "        num_total_observations: int = self.data.shape[\n",
    "            0\n",
    "        ]  # Calculate the total number of observations in the DataFrame\n",
    "        num_observations_cumulative_split: list[float] = (\n",
    "            num_total_observations * np.cumsum(split_ratio_list)\n",
    "        )  # np.cumsum([0.7, 0.2, 0.1]) = [0.7, 0.9, 1.0]\n",
    "        self.cumulative_number_window_observation: list[int] = [\n",
    "            round(num_observation_cumulative_split)\n",
    "            for num_observation_cumulative_split in num_observations_cumulative_split\n",
    "        ]\n",
    "\n",
    "    def split_update(self, split_ratio_list: list[float]) -> None:\n",
    "        self.split_ratio: list[float] = split_ratio_list\n",
    "        num_observations_total: int = self.data.shape[0]\n",
    "        num_observations_cumulative_split: list[float] = (\n",
    "            num_observations_total * np.cumsum(split_ratio_list)\n",
    "        )  # np.cumsum([0.7, 0.2, 0.1]) = [0.7, 0.9, 1.0]\n",
    "        self.cumulative_number_window_observation = [\n",
    "            round(i) for i in num_observations_cumulative_split\n",
    "        ]\n",
    "\n",
    "    def train(self) -> pd.DataFrame:\n",
    "        return self.data[\n",
    "            : self.cumulative_number_window_observation[0]\n",
    "        ]  # Return the training subset of observations\n",
    "\n",
    "    def test(self):\n",
    "        if (\n",
    "            self.cumulative_number_window_observation[0]\n",
    "            - self.number_of_observation_per_window\n",
    "            < 0\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"The number of observations per window exceeds the number of observations of train data in the dataset.\"\n",
    "            )\n",
    "        return self.data[\n",
    "            self.cumulative_number_window_observation[0]\n",
    "            - self.number_of_observation_per_window : self.cumulative_number_window_observation[\n",
    "                1\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "    def shape(self):\n",
    "        return self.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_original_data: pd.DataFrame = pd.read_pickle(\n",
    "    \"./cache/factor_\" + data_frequency + \".pkl\"\n",
    ")\n",
    "Y_original_data: pd.DataFrame = pd.read_pickle(\n",
    "    \"./cache/asset_\" + data_frequency + \".pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mkt-RF</th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>Mom</th>\n",
       "      <th>ST_Rev</th>\n",
       "      <th>LT_Rev</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>-0.024889</td>\n",
       "      <td>-0.003948</td>\n",
       "      <td>0.005921</td>\n",
       "      <td>-0.008655</td>\n",
       "      <td>0.021933</td>\n",
       "      <td>-0.033839</td>\n",
       "      <td>0.034927</td>\n",
       "      <td>0.002774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-14</th>\n",
       "      <td>0.020696</td>\n",
       "      <td>0.013858</td>\n",
       "      <td>-0.015028</td>\n",
       "      <td>-0.028196</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.015969</td>\n",
       "      <td>-0.001553</td>\n",
       "      <td>0.008910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-21</th>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.060555</td>\n",
       "      <td>-0.025968</td>\n",
       "      <td>-0.048690</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>0.053417</td>\n",
       "      <td>-0.043407</td>\n",
       "      <td>0.020229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-28</th>\n",
       "      <td>-0.057084</td>\n",
       "      <td>0.009003</td>\n",
       "      <td>0.016956</td>\n",
       "      <td>0.013910</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>-0.030094</td>\n",
       "      <td>0.031843</td>\n",
       "      <td>-0.012653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-04</th>\n",
       "      <td>0.044559</td>\n",
       "      <td>-0.001065</td>\n",
       "      <td>-0.026655</td>\n",
       "      <td>-0.019944</td>\n",
       "      <td>-0.014198</td>\n",
       "      <td>0.037680</td>\n",
       "      <td>-0.001666</td>\n",
       "      <td>0.015425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Mkt-RF       SMB       HML       RMW       CMA    Mom     \\\n",
       "Date                                                                     \n",
       "2000-01-07 -0.024889 -0.003948  0.005921 -0.008655  0.021933 -0.033839   \n",
       "2000-01-14  0.020696  0.013858 -0.015028 -0.028196  0.000918  0.015969   \n",
       "2000-01-21  0.000378  0.060555 -0.025968 -0.048690  0.001365  0.053417   \n",
       "2000-01-28 -0.057084  0.009003  0.016956  0.013910  0.016216 -0.030094   \n",
       "2000-02-04  0.044559 -0.001065 -0.026655 -0.019944 -0.014198  0.037680   \n",
       "\n",
       "              ST_Rev    LT_Rev  \n",
       "Date                            \n",
       "2000-01-07  0.034927  0.002774  \n",
       "2000-01-14 -0.001553  0.008910  \n",
       "2000-01-21 -0.043407  0.020229  \n",
       "2000-01-28  0.031843 -0.012653  \n",
       "2000-02-04 -0.001666  0.015425  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_original_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>C</th>\n",
       "      <th>JPM</th>\n",
       "      <th>BAC</th>\n",
       "      <th>XOM</th>\n",
       "      <th>HAL</th>\n",
       "      <th>MCD</th>\n",
       "      <th>WMT</th>\n",
       "      <th>COST</th>\n",
       "      <th>CAT</th>\n",
       "      <th>LMT</th>\n",
       "      <th>JNJ</th>\n",
       "      <th>PFE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>VZ</th>\n",
       "      <th>T</th>\n",
       "      <th>ED</th>\n",
       "      <th>NEM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>-0.032195</td>\n",
       "      <td>-0.045482</td>\n",
       "      <td>-0.086300</td>\n",
       "      <td>-0.030347</td>\n",
       "      <td>-0.058169</td>\n",
       "      <td>-0.029886</td>\n",
       "      <td>0.054369</td>\n",
       "      <td>0.010932</td>\n",
       "      <td>-0.010667</td>\n",
       "      <td>-0.009113</td>\n",
       "      <td>0.019836</td>\n",
       "      <td>0.132809</td>\n",
       "      <td>-0.020110</td>\n",
       "      <td>0.034853</td>\n",
       "      <td>0.063502</td>\n",
       "      <td>0.064274</td>\n",
       "      <td>-0.038464</td>\n",
       "      <td>-0.089726</td>\n",
       "      <td>0.045217</td>\n",
       "      <td>-0.124898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-14</th>\n",
       "      <td>0.009447</td>\n",
       "      <td>0.007268</td>\n",
       "      <td>-0.076337</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.015533</td>\n",
       "      <td>0.037174</td>\n",
       "      <td>-0.014010</td>\n",
       "      <td>-0.052347</td>\n",
       "      <td>0.068455</td>\n",
       "      <td>-0.058394</td>\n",
       "      <td>0.054374</td>\n",
       "      <td>-0.025699</td>\n",
       "      <td>-0.043843</td>\n",
       "      <td>-0.029119</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.078060</td>\n",
       "      <td>-0.042510</td>\n",
       "      <td>-0.048266</td>\n",
       "      <td>-0.065724</td>\n",
       "      <td>-0.029384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-21</th>\n",
       "      <td>0.108224</td>\n",
       "      <td>-0.075724</td>\n",
       "      <td>-0.034086</td>\n",
       "      <td>-0.026897</td>\n",
       "      <td>-0.012723</td>\n",
       "      <td>-0.095248</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.099066</td>\n",
       "      <td>-0.036376</td>\n",
       "      <td>-0.031938</td>\n",
       "      <td>-0.011415</td>\n",
       "      <td>-0.081857</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>-0.040666</td>\n",
       "      <td>-0.052432</td>\n",
       "      <td>-0.024136</td>\n",
       "      <td>0.066596</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>-0.003859</td>\n",
       "      <td>0.018260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-28</th>\n",
       "      <td>-0.087054</td>\n",
       "      <td>-0.053012</td>\n",
       "      <td>-0.005962</td>\n",
       "      <td>-0.005493</td>\n",
       "      <td>0.051412</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>-0.072000</td>\n",
       "      <td>-0.155026</td>\n",
       "      <td>-0.104968</td>\n",
       "      <td>-0.117072</td>\n",
       "      <td>-0.051546</td>\n",
       "      <td>-0.081891</td>\n",
       "      <td>-0.100952</td>\n",
       "      <td>-0.059858</td>\n",
       "      <td>0.003708</td>\n",
       "      <td>0.122137</td>\n",
       "      <td>-0.040460</td>\n",
       "      <td>-0.087209</td>\n",
       "      <td>-0.029797</td>\n",
       "      <td>-0.053327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-02-04</th>\n",
       "      <td>0.062783</td>\n",
       "      <td>0.084580</td>\n",
       "      <td>0.273464</td>\n",
       "      <td>-0.021814</td>\n",
       "      <td>0.065980</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>0.025355</td>\n",
       "      <td>0.027925</td>\n",
       "      <td>-0.044354</td>\n",
       "      <td>0.021404</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>-0.027356</td>\n",
       "      <td>0.013242</td>\n",
       "      <td>-0.028047</td>\n",
       "      <td>0.015914</td>\n",
       "      <td>0.037551</td>\n",
       "      <td>0.016137</td>\n",
       "      <td>0.070064</td>\n",
       "      <td>-0.009521</td>\n",
       "      <td>0.196411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                AAPL      MSFT      AMZN         C       JPM       BAC  \\\n",
       "date                                                                     \n",
       "2000-01-07 -0.032195 -0.045482 -0.086300 -0.030347 -0.058169 -0.029886   \n",
       "2000-01-14  0.009447  0.007268 -0.076337  0.074074  0.015533  0.037174   \n",
       "2000-01-21  0.108224 -0.075724 -0.034086 -0.026897 -0.012723 -0.095248   \n",
       "2000-01-28 -0.087054 -0.053012 -0.005962 -0.005493  0.051412  0.001313   \n",
       "2000-02-04  0.062783  0.084580  0.273464 -0.021814  0.065980  0.002842   \n",
       "\n",
       "                 XOM       HAL       MCD       WMT      COST       CAT  \\\n",
       "date                                                                     \n",
       "2000-01-07  0.054369  0.010932 -0.010667 -0.009113  0.019836  0.132809   \n",
       "2000-01-14 -0.014010 -0.052347  0.068455 -0.058394  0.054374 -0.025699   \n",
       "2000-01-21  0.014925  0.099066 -0.036376 -0.031938 -0.011415 -0.081857   \n",
       "2000-01-28 -0.072000 -0.155026 -0.104968 -0.117072 -0.051546 -0.081891   \n",
       "2000-02-04  0.025355  0.027925 -0.044354  0.021404  0.152174 -0.027356   \n",
       "\n",
       "                 LMT       JNJ       PFE       DIS        VZ         T  \\\n",
       "date                                                                     \n",
       "2000-01-07 -0.020110  0.034853  0.063502  0.064274 -0.038464 -0.089726   \n",
       "2000-01-14 -0.043843 -0.029119  0.072464  0.078060 -0.042510 -0.048266   \n",
       "2000-01-21  0.024390 -0.040666 -0.052432 -0.024136  0.066596  0.023810   \n",
       "2000-01-28 -0.100952 -0.059858  0.003708  0.122137 -0.040460 -0.087209   \n",
       "2000-02-04  0.013242 -0.028047  0.015914  0.037551  0.016137  0.070064   \n",
       "\n",
       "                  ED       NEM  \n",
       "date                            \n",
       "2000-01-07  0.045217 -0.124898  \n",
       "2000-01-14 -0.065724 -0.029384  \n",
       "2000-01-21 -0.003859  0.018260  \n",
       "2000-01-28 -0.029797 -0.053327  \n",
       "2000-02-04 -0.009521  0.196411  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_original_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def AV(\n",
    "    start: str,\n",
    "    end: str,\n",
    "    split_ratio: list,\n",
    "    data_frequency: str = \"weekly\",\n",
    "    num_observations_per_window: int = 104,\n",
    "    num_assets=None,\n",
    "    use_cache: bool = False,\n",
    "    save_results: bool = False,\n",
    "    AV_key: str = None,\n",
    "):\n",
    "    if use_cache:\n",
    "        X: pd.DataFrame = pd.read_pickle(\"./cache/factor_\" + data_frequency + \".pkl\")\n",
    "        Y: pd.DataFrame = pd.read_pickle(\"./cache/asset_\" + data_frequency + \".pkl\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"We cannot download data from AlphaVantage without an API key.\"\n",
    "        )\n",
    "\n",
    "    # Partition dataset into training and testing sets. Lag the data by one observation, since we are predicting future returns, so we don't need the last observation that doesn't have a future return.\n",
    "    # we don't need the first Y observation that doesn't have a corresponding X observation.\n",
    "    return TrainTest(X[:-1], num_observations_per_window, split_ratio), TrainTest(\n",
    "        Y[1:], num_observations_per_window, split_ratio\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1134, 8)\n",
      "(680, 8)\n",
      "(680, 20)\n"
     ]
    }
   ],
   "source": [
    "X_data, Y_data = AV(\n",
    "    start,\n",
    "    end,\n",
    "    split_ratio_list,\n",
    "    data_frequency=data_frequency,\n",
    "    num_observations_per_window=number_of_observe_per_window,\n",
    "    num_assets=number_of_asset,\n",
    "    use_cache=True,\n",
    "    save_results=False,\n",
    "    AV_key=AV_key,\n",
    ")\n",
    "print(X_data.shape())\n",
    "print(X_data.train().shape)\n",
    "print(Y_data.train().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  8\n",
      "Number of assets:  20\n"
     ]
    }
   ],
   "source": [
    "# Number of features and assets\n",
    "n_X: int = X_data.train().shape[1]\n",
    "n_Y: int = Y_data.train().shape[1]\n",
    "print(\"Number of features: \", n_X)\n",
    "print(\"Number of assets: \", n_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low p-values (< 0.05) suggest that the factor significantly affects the stock returns.\n",
    "High p-values (> 0.05) suggest that the factor's effect on the stock returns is not statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Mkt-RF   SMB   HML   RMW   CMA  Mom     ST_Rev  LT_Rev\n",
      "AAPL    0.34  0.37  0.53  0.92  0.70    0.41    0.41    0.53\n",
      "MSFT    0.64  0.85  0.63  0.80  0.20    0.21    0.97    0.22\n",
      "AMZN    0.31  0.02  0.64  0.28  0.63    0.18    0.45    0.34\n",
      "C       0.25  0.69  0.02  0.04  0.21    0.07    0.02    0.24\n",
      "JPM     0.33  0.64  0.00  0.50  0.48    0.18    0.18    0.01\n",
      "BAC     0.16  0.91  0.01  0.16  0.56    0.15    0.06    0.19\n",
      "XOM     0.03  0.77  0.34  0.15  0.10    0.11    0.51    0.04\n",
      "HAL     0.92  0.48  0.47  0.14  0.14    0.42    0.92    0.05\n",
      "MCD     0.48  0.05  0.57  0.02  0.54    0.27    0.81    0.03\n",
      "WMT     0.00  0.01  0.25  0.00  0.40    0.62    0.04    0.03\n",
      "COST    0.00  0.22  0.85  0.01  0.38    0.66    0.39    0.05\n",
      "CAT     0.92  0.27  0.59  0.23  0.07    0.40    0.67    0.51\n",
      "LMT     0.27  0.74  0.04  0.00  0.61    0.32    0.38    0.60\n",
      "JNJ     0.00  0.91  0.39  0.09  0.84    0.82    0.19    0.06\n",
      "PFE     0.06  0.38  0.95  0.91  0.56    0.75    0.50    0.12\n",
      "DIS     0.35  0.67  0.82  0.01  0.39    0.61    0.19    0.04\n",
      "VZ      0.72  0.30  0.01  0.04  0.15    0.09    0.15    0.00\n",
      "T       0.62  0.80  0.00  0.95  0.01    0.14    0.86    0.00\n",
      "ED      0.30  0.94  0.96  0.00  0.23    0.89    0.73    0.16\n",
      "NEM     0.12  0.07  0.05  0.01  0.20    0.05    0.76    0.50\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def statanalysis(X: pd.DataFrame, Y: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Initialize an empty DataFrame to store p-values\n",
    "    # Rows correspond to assets (Y.columns) and columns correspond to features (X.columns)\n",
    "    stats = pd.DataFrame(\n",
    "        columns=X.columns, index=Y.columns\n",
    "    )  # Create an empty DataFrame to store the p-values\n",
    "    for ticker in Y.columns:\n",
    "        for feature in X.columns:\n",
    "            stats.loc[ticker, feature] = (\n",
    "                sm.OLS(Y[ticker].values, sm.add_constant(X[feature]).values)\n",
    "                .fit()\n",
    "                .pvalues[1]  # Get the p-value of the feature\n",
    "            )\n",
    "\n",
    "    return stats.astype(float).round(2)\n",
    "\n",
    "\n",
    "statistical_analysis: pd.DataFrame = statanalysis(X_data.train(), Y_data.train())\n",
    "print(statistical_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SlidingWindow(Dataset):\n",
    "    \"\"\"Sliding window dataset constructor for time series data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        XData: pd.DataFrame,\n",
    "        YData: pd.DataFrame,\n",
    "        num_observations: int,\n",
    "        performance_window: int,\n",
    "    ) -> None:\n",
    "        # Convert the feature DataFrame to a PyTorch tensor with double precision\n",
    "        self.X: torch.Tensor = torch.tensor(XData.values, dtype=torch.float64)\n",
    "        # Convert the asset return DataFrame to a PyTorch tensor with double precision\n",
    "        self.Y: torch.Tensor = torch.tensor(YData.values, dtype=torch.float64)\n",
    "        # Store the number of observations (scenarios) in the sliding window\n",
    "        self.num_observations: int = num_observations\n",
    "        # Store the number of scenarios in the performance window\n",
    "        self.perf_period: int = performance_window\n",
    "\n",
    "    def __getitem__(\n",
    "        self, index: int\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # Extract the feature window starting at 'index' and spanning 'n_obs + 1' time steps\n",
    "        x: torch.Tensor = self.X[index : index + self.num_observations + 1]\n",
    "        # Extract the realizations window starting at 'index' and spanning 'n_obs' time steps\n",
    "        y: torch.Tensor = self.Y[index : index + self.num_observations]\n",
    "        # Extract the performance window starting after the observations window and spanning 'perf_period + 1' time steps\n",
    "        y_future_performance: torch.Tensor = self.Y[\n",
    "            index\n",
    "            + self.num_observations : index\n",
    "            + self.num_observations\n",
    "            + self.perf_period\n",
    "            + 1\n",
    "        ]\n",
    "        # Return the extracted windows as a tuple\n",
    "        return x, y, y_future_performance\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # Calculate the effective length by subtracting the window sizes from the total data length\n",
    "        total_length: int = len(self.X) - self.num_observations - self.perf_period\n",
    "        # Return the calculated length\n",
    "        return total_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackTest:\n",
    "    \"\"\"Backtest object to store out-of-sample results.\"\"\"\n",
    "\n",
    "    def __init__(self, len_test: int, n_y: int, dates: pd.DatetimeIndex) -> None:\n",
    "        # Initialize the weights array with zeros; dimensions are (len_test, n_y)\n",
    "        self.weights: np.ndarray = np.zeros((len_test, n_y))\n",
    "        # Initialize the returns array with zeros; length is len_test\n",
    "        self.rets: np.ndarray = np.zeros(len_test)\n",
    "        # Store the dates corresponding to the out-of-sample period\n",
    "        self.dates: pd.DatetimeIndex = dates[-len_test:]\n",
    "\n",
    "    def stats(self) -> None:\n",
    "        # Calculate the cumulative product of returns plus one to get the total return index\n",
    "        tri: np.ndarray = np.cumprod(self.rets + 1)\n",
    "        # Calculate the geometric mean return over the out-of-sample period\n",
    "        self.mean: float = (tri[-1]) ** (1 / len(tri)) - 1\n",
    "        # Calculate the volatility (standard deviation) of the returns\n",
    "        self.vol: float = np.std(self.rets)\n",
    "        # Calculate the pseudo-Sharpe ratio, handling division by zero\n",
    "        self.sharpe: float = self.mean / self.vol if self.vol != 0 else np.nan\n",
    "        # Create a DataFrame with dates, realized returns, and total return index\n",
    "        self.returns = pd.DataFrame({\"Date\": self.dates, \"rets\": self.rets, \"tri\": tri})\n",
    "        # Set the 'Date' column as the index of the DataFrame\n",
    "        self.returns = self.returns.set_index(\"Date\")\n",
    "\n",
    "    def plot_cumret(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.returns[\"tri\"], label=\"Cumulative Return\")\n",
    "        plt.title(\"Cumulative Return Over Time\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Cumulative Return\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "\n",
    "\n",
    "# Define the Sharpe loss function\n",
    "def sharpe_loss(z_star: torch.Tensor, y_perf: torch.Tensor) -> torch.Tensor:\n",
    "    loss = -torch.mean(y_perf @ z_star) / torch.std(y_perf @ z_star)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Define the portfolio variance risk function\n",
    "def p_var(z: cp.Variable, c: cp.Variable, x: cp.Expression) -> cp.Expression:\n",
    "    return cp.square(x @ z - c)\n",
    "\n",
    "\n",
    "# Define the Hellinger distance-based DRO optimization layer\n",
    "def hellinger(num_assets: int, num_observations: int, prisk) -> CvxpyLayer:\n",
    "    # Define decision variables\n",
    "    z = cp.Variable((num_assets, 1), nonneg=True)  # Portfolio weights\n",
    "    c_aux = cp.Variable()  # Centering parameter\n",
    "    lambda_aux = cp.Variable(nonneg=True)\n",
    "    xi_aux = cp.Variable()\n",
    "    beta_aux = cp.Variable(num_observations, nonneg=True)\n",
    "    tau_aux = cp.Variable(num_observations, nonneg=True)\n",
    "    mu_aux = cp.Variable()\n",
    "\n",
    "    # Define parameters\n",
    "    ep = cp.Parameter((num_observations, num_assets))  # Residuals matrix\n",
    "    y_hat = cp.Parameter(num_assets)  # Predicted returns\n",
    "    gamma = cp.Parameter(nonneg=True)  # Risk-return trade-off parameter\n",
    "    delta = cp.Parameter(nonneg=True)  # Ambiguity size parameter\n",
    "\n",
    "    # Define constraints\n",
    "    constraints = [\n",
    "        cp.sum(z) == 1,  # Total budget constraint\n",
    "        mu_aux == y_hat @ z,  # Expected return constraint\n",
    "    ]\n",
    "    for i in range(num_observations):\n",
    "        # Constraints based on the risk function\n",
    "        constraints += [xi_aux + lambda_aux >= prisk(z, c_aux, ep[i, :]) + tau_aux[i]]\n",
    "        constraints += [\n",
    "            beta_aux[i] >= cp.quad_over_lin(lambda_aux, tau_aux[i])\n",
    "        ]  # Constraint on the ambiguity set\n",
    "\n",
    "    # Define the objective function\n",
    "    objective = cp.Minimize(\n",
    "        xi_aux\n",
    "        + (delta - 1) * lambda_aux\n",
    "        + (1 / num_observations) * cp.sum(beta_aux)\n",
    "        - gamma * mu_aux\n",
    "    )\n",
    "\n",
    "    # Define the problem\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    # Create a CVXPY layer\n",
    "    cvxpylayer = CvxpyLayer(\n",
    "        problem, parameters=[ep, y_hat, gamma, delta], variables=[z]\n",
    "    )\n",
    "\n",
    "    return cvxpylayer\n",
    "\n",
    "\n",
    "# Define mappings for performance loss functions, risk functions, and optimization layers\n",
    "perf_loss_functions = {\n",
    "    \"sharpe_loss\": sharpe_loss,\n",
    "    # Add other performance loss functions here if needed\n",
    "}\n",
    "\n",
    "risk_functions = {\n",
    "    \"p_var\": p_var,\n",
    "    # Add other risk functions here if needed\n",
    "}\n",
    "\n",
    "opt_layer_functions = {\n",
    "    \"hellinger\": hellinger,\n",
    "    # Add other optimization layer functions here if needed\n",
    "}\n",
    "\n",
    "\n",
    "class E2E_net(nn.Module):\n",
    "    \"\"\"End-to-end Distributionally Robust Optimization (DRO) learning neural net module.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_input_features: int,\n",
    "        num_assets: int,\n",
    "        num_observations: int,\n",
    "        opt_layer: str = \"hellinger\",\n",
    "        prisk: str = \"p_var\",\n",
    "        perf_loss: str = \"sharpe_loss\",\n",
    "        pred_model: str = \"3layer\",\n",
    "        pred_loss_factor: float | None = 0.5,\n",
    "        perf_period: int = 13,\n",
    "        train_pred: bool = True,\n",
    "        train_gamma: bool = True,\n",
    "        train_delta: bool = True,\n",
    "        set_seed: int | None = None,\n",
    "        cache_path: str = \"./cache/\",\n",
    "    ) -> None:\n",
    "        super(E2E_net, self).__init__()\n",
    "\n",
    "        # Set random seed for reproducibility\n",
    "        if set_seed is not None:\n",
    "            torch.manual_seed(set_seed)\n",
    "            self.seed: int = set_seed\n",
    "\n",
    "        self.num_features: int = num_input_features  # Number of input features\n",
    "        self.num_assets: int = num_assets  # Number of assets\n",
    "        self.num_observations: int = (\n",
    "            num_observations  # Number of observations/scenarios\n",
    "        )\n",
    "\n",
    "        # Prediction loss function\n",
    "        if pred_loss_factor is not None:\n",
    "            self.pred_loss_factor: float = pred_loss_factor\n",
    "            self.pred_loss = nn.MSELoss()  # Mean squared error loss\n",
    "        else:\n",
    "            self.pred_loss = None\n",
    "\n",
    "        # Performance loss function\n",
    "        if perf_loss in perf_loss_functions:\n",
    "            self.perf_loss = perf_loss_functions[perf_loss]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown perf_loss function: {perf_loss}\")\n",
    "\n",
    "        self.perf_period: int = perf_period\n",
    "\n",
    "        # Initialize gamma parameter\n",
    "        self.gamma: nn.Parameter = nn.Parameter(\n",
    "            torch.FloatTensor(1).uniform_(0.02, 0.1)\n",
    "        )\n",
    "        self.gamma.requires_grad = train_gamma\n",
    "        self.gamma_init: float = self.gamma.item()\n",
    "\n",
    "        ub: float = (1 - 1 / (num_observations**0.5)) / 2\n",
    "        lb: float = (1 - 1 / (num_observations**0.5)) / 10\n",
    "        self.delta: nn.Parameter = nn.Parameter(torch.FloatTensor(1).uniform_(lb, ub))\n",
    "        self.delta.requires_grad = train_delta\n",
    "        self.delta_init: float = self.delta.item()\n",
    "        self.model_type = \"dro\"\n",
    "\n",
    "        self.pred_model: str = pred_model\n",
    "\n",
    "        if pred_model == \"2layer\":\n",
    "            hidden_size = int(0.5 * (num_input_features + num_assets))\n",
    "            self.pred_layer = nn.Sequential(\n",
    "                nn.Linear(num_input_features, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size, num_assets),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_assets, num_assets),\n",
    "            )\n",
    "        elif pred_model == \"3layer\":\n",
    "            hidden_size1 = int(0.5 * (num_input_features + num_assets))\n",
    "            hidden_size2 = int(0.6 * (num_input_features + num_assets))\n",
    "            self.pred_layer = nn.Sequential(\n",
    "                nn.Linear(num_input_features, hidden_size1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size1, hidden_size2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size2, num_assets),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_assets, num_assets),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pred_model type: {pred_model}\")\n",
    "\n",
    "        # Define the optimization layer\n",
    "        if opt_layer in opt_layer_functions:\n",
    "            if prisk in risk_functions:\n",
    "                self.opt_layer = opt_layer_functions[opt_layer](\n",
    "                    num_assets, num_observations, risk_functions[prisk]\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prisk function: {prisk}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown opt_layer function: {opt_layer}\")\n",
    "\n",
    "        self.cache_path: str = cache_path\n",
    "\n",
    "        # Store initial model state path\n",
    "        self.init_state_path: str = (\n",
    "            cache_path + self.model_type + \"_initial_state_\" + pred_model\n",
    "        )\n",
    "        if not train_gamma:\n",
    "            self.init_state_path += \"_TrainGammaFalse\"\n",
    "        if not train_delta:\n",
    "            self.init_state_path += \"_TrainDeltaFalse\"\n",
    "\n",
    "        # Save initial state of the model\n",
    "        torch.save(self.state_dict(), self.init_state_path)\n",
    "\n",
    "    def forward(\n",
    "        self, X: torch.Tensor, Y: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Generate predictions for each time step in X\n",
    "        Y_hat: torch.Tensor = torch.stack(\n",
    "            [self.pred_layer(x_t) for x_t in X]\n",
    "        )  # Shape: (n_obs+1, n_y)\n",
    "\n",
    "        # Calculate residuals\n",
    "        ep: torch.Tensor = Y - Y_hat[:-1]  # Shape: (n_obs, n_y)\n",
    "\n",
    "        # Extract the last prediction\n",
    "        y_hat: torch.Tensor = Y_hat[-1]  # Shape: (n_y,)\n",
    "\n",
    "        # Solver arguments\n",
    "        solver_args: dict[str, any] = {\n",
    "            \"solve_method\": \"ECOS\",\n",
    "            \"max_iters\": 120,\n",
    "            \"abstol\": 1e-7,\n",
    "        }\n",
    "\n",
    "        # Optimize z_star\n",
    "        z_star: torch.Tensor\n",
    "        (z_star,) = self.opt_layer(\n",
    "            ep, y_hat, self.gamma, self.delta, solver_args=solver_args\n",
    "        )\n",
    "\n",
    "        return z_star, y_hat\n",
    "\n",
    "    def net_train(\n",
    "        self,\n",
    "        train_set: DataLoader,\n",
    "        val_set: DataLoader | None = None,\n",
    "        epochs: int | None = None,\n",
    "        lr: float | None = None,\n",
    "    ) -> float | None:\n",
    "        # Assign number of epochs and learning rate\n",
    "        if epochs is None:\n",
    "            epochs = self.epochs\n",
    "        if lr is None:\n",
    "            lr = self.lr\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        n_train: int = len(train_set)\n",
    "        for epoch in range(epochs):\n",
    "            train_loss: float = 0.0\n",
    "            optimizer.zero_grad()\n",
    "            for _, (x, y, y_perf) in enumerate(train_set):\n",
    "                # Move tensors to the same device as the model\n",
    "                x = x.to(next(self.parameters()).device)\n",
    "                y = y.to(next(self.parameters()).device)\n",
    "                y_perf = y_perf.to(next(self.parameters()).device)\n",
    "\n",
    "                # Forward pass\n",
    "                z_star, y_hat = self(x.squeeze(), y.squeeze())\n",
    "\n",
    "                # Compute loss\n",
    "                if self.pred_loss is None:\n",
    "                    loss = (1 / n_train) * self.perf_loss(z_star, y_perf.squeeze())\n",
    "                else:\n",
    "                    loss = (1 / n_train) * (\n",
    "                        self.perf_loss(z_star, y_perf.squeeze())\n",
    "                        + (self.pred_loss_factor / self.num_assets)\n",
    "                        * self.pred_loss(y_hat, y_perf.squeeze()[0])\n",
    "                    )\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Accumulate loss\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}\")\n",
    "\n",
    "            # Ensure gamma and delta remain positive\n",
    "            for name, param in self.named_parameters():\n",
    "                if name == \"gamma\":\n",
    "                    param.data.clamp_(min=0.0001)\n",
    "\n",
    "        # Validation\n",
    "        if val_set is not None:\n",
    "            n_val: int = len(val_set)\n",
    "            val_loss: float = 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for t, (x, y, y_perf) in enumerate(val_set):\n",
    "                    # Forward pass\n",
    "                    z_val, y_val = self(x.squeeze(), y.squeeze())\n",
    "\n",
    "                    # Compute loss\n",
    "                    if self.pred_loss is None:\n",
    "                        loss = (1 / n_val) * self.perf_loss(z_val, y_perf.squeeze())\n",
    "                    else:\n",
    "                        loss = (1 / n_val) * (\n",
    "                            self.perf_loss(z_val, y_perf.squeeze())\n",
    "                            + (self.pred_loss_factor / self.num_assets)\n",
    "                            * self.pred_loss(y_val, y_perf.squeeze()[0])\n",
    "                        )\n",
    "\n",
    "                    # Accumulate validation loss\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            return val_loss\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    # net_roll_test: Test the e2e neural net\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def net_roll_test(\n",
    "        self,\n",
    "        X: TrainTest,\n",
    "        Y: TrainTest,\n",
    "        n_roll: int,\n",
    "        lr: float,\n",
    "        epochs: int,\n",
    "        load_state: list[bool] = [False, False, False, False],\n",
    "        save_state: list[bool] = [False, False, False, False],\n",
    "    ) -> None:\n",
    "        # Initialize backtest object\n",
    "        portfolio = BackTest(\n",
    "            len(Y.test()) - Y.number_of_observation_per_window,\n",
    "            self.num_assets,\n",
    "            Y.test().index[Y.number_of_observation_per_window :],\n",
    "        )\n",
    "\n",
    "        # Initialize lists to store trained parameters\n",
    "        self.gamma_trained = []\n",
    "        self.delta_trained = []\n",
    "\n",
    "        # Store initial split\n",
    "        init_split = Y.split_ratio\n",
    "\n",
    "        # Calculate window size\n",
    "        win_size = init_split[1] / n_roll\n",
    "\n",
    "        split = [0, 0]\n",
    "        t = 0\n",
    "        for i in range(n_roll):\n",
    "\n",
    "            print(f\"Out-of-sample window: {i+1} / {n_roll}\")\n",
    "\n",
    "            split[0] = init_split[0] + win_size * i\n",
    "            if i < n_roll - 1:\n",
    "                split[1] = win_size\n",
    "            else:\n",
    "                split[1] = 1 - split[0]\n",
    "\n",
    "            X.split_update(split)\n",
    "            Y.split_update(split)\n",
    "            train_set = DataLoader(\n",
    "                SlidingWindow(\n",
    "                    X.train(), Y.train(), self.num_observations, self.perf_period\n",
    "                )\n",
    "            )\n",
    "            test_set = DataLoader(\n",
    "                SlidingWindow(X.test(), Y.test(), self.num_observations, 0)\n",
    "            )\n",
    "            if load_state[i]:\n",
    "                # Reset model parameters to initial state\n",
    "                self.load_state_dict(\n",
    "                    torch.load(\n",
    "                        self.cache_path + \"model_\" + str(i) + \".pt\", weights_only=True\n",
    "                    )\n",
    "                )\n",
    "            # Train the model\n",
    "            self.train()\n",
    "            self.net_train(train_set, lr=lr, epochs=epochs)\n",
    "            # Save the trained model\n",
    "            if save_state[i]:\n",
    "                torch.save(\n",
    "                    self.state_dict(), self.cache_path + \"model_\" + str(i) + \".pt\"\n",
    "                )\n",
    "            self.gamma_trained.append(self.gamma.item())\n",
    "            self.delta_trained.append(self.delta.item())\n",
    "            # Test the model\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                for __, (x, y, y_perf) in enumerate(test_set):\n",
    "                    # Move tensors to the same device as the model\n",
    "                    x = x.to(next(self.parameters()).device)\n",
    "                    y = y.to(next(self.parameters()).device)\n",
    "                    y_perf = y_perf.to(next(self.parameters()).device)  # yt\n",
    "\n",
    "                    z_star, _ = self(x.squeeze(), y.squeeze())  # t,  y-> t-1\n",
    "                    portfolio.weights[t] = z_star.squeeze().cpu().numpy()\n",
    "                    portfolio.rets[t] = (\n",
    "                        y_perf.squeeze().cpu().numpy() @ portfolio.weights[t]\n",
    "                    ).item()\n",
    "                    t += 1\n",
    "\n",
    "        # Reset dataset splits\n",
    "        X.split_update(init_split)\n",
    "        Y.split_update(init_split)\n",
    "\n",
    "        # Calculate portfolio statistics\n",
    "        portfolio.stats()\n",
    "\n",
    "        self.portfolio = portfolio\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    # load_cv_results: Load cross-validation results\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    def load_cv_results(self, cv_results):\n",
    "        self.cv_results = cv_results\n",
    "\n",
    "        # Select and store the optimal hyperparameters\n",
    "        idx = cv_results.val_loss.idxmin()\n",
    "        self.lr = cv_results.lr[idx]\n",
    "        self.epochs = cv_results.epochs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "E2E_net(\n",
       "  (pred_loss): MSELoss()\n",
       "  (pred_layer): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=14, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=14, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=20, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=20, out_features=20, bias=True)\n",
       "  )\n",
       "  (opt_layer): CvxpyLayer()\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Initialize your model\n",
    "dr_net = E2E_net(\n",
    "    num_input_features=n_X,\n",
    "    num_assets=n_Y,\n",
    "    num_observations=number_of_observe_per_window,\n",
    "    prisk=\"p_var\",\n",
    "    train_pred=True,\n",
    "    train_gamma=True,\n",
    "    train_delta=True,\n",
    "    set_seed=19260817,\n",
    "    opt_layer=\"hellinger\",\n",
    "    perf_loss=\"sharpe_loss\",\n",
    "    cache_path=\"./cache/\",\n",
    "    perf_period=13,\n",
    "    pred_loss_factor=0.5,\n",
    ").double()\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dr_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-sample window: 1 / 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m n_roll \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m  \u001b[38;5;66;03m# You can adjust this number based on your data and preference\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mdr_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet_roll_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mY_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_roll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_roll\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.000075\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_roll\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_roll\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Access the portfolio object\u001b[39;00m\n\u001b[0;32m     15\u001b[0m portfolio \u001b[38;5;241m=\u001b[39m dr_net\u001b[38;5;241m.\u001b[39mportfolio\n",
      "Cell \u001b[1;32mIn[55], line 367\u001b[0m, in \u001b[0;36mE2E_net.net_roll_test\u001b[1;34m(self, X, Y, n_roll, lr, epochs, load_state, save_state)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m--> 367\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_state[i]:\n",
      "Cell \u001b[1;32mIn[55], line 267\u001b[0m, in \u001b[0;36mE2E_net.net_train\u001b[1;34m(self, train_set, val_set, epochs, lr)\u001b[0m\n\u001b[0;32m    260\u001b[0m     loss \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m n_train) \u001b[38;5;241m*\u001b[39m (\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperf_loss(z_star, y_perf\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_loss_factor \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_assets)\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_loss(y_hat, y_perf\u001b[38;5;241m.\u001b[39msqueeze()[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    264\u001b[0m     )\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# Accumulate loss\u001b[39;00m\n\u001b[0;32m    270\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\zhang\\anaconda3\\envs\\py12\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zhang\\anaconda3\\envs\\py12\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zhang\\anaconda3\\envs\\py12\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set the number of rolling windows\n",
    "n_roll = 4  # You can adjust this number based on your data and preference\n",
    "\n",
    "# Test the model\n",
    "dr_net.net_roll_test(\n",
    "    X_data,\n",
    "    Y_data,\n",
    "    n_roll=n_roll,\n",
    "    lr=0.000075,\n",
    "    epochs=1,\n",
    "    load_state=[True] * (n_roll),\n",
    "    save_state=[True] * (n_roll),\n",
    ")\n",
    "# Access the portfolio object\n",
    "portfolio = dr_net.portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the cumulative return\n",
    "portfolio.plot_cumret()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe Ratio:  0.1764040025673254\n",
      "Mean Return:  0.00411746782029887\n",
      "Volatility:  0.023341124693173666\n",
      "Annualized Sharpe Ratio:  1.272067352907146\n"
     ]
    }
   ],
   "source": [
    "# print the sharpe ratio\n",
    "print(\"Sharpe Ratio: \", portfolio.sharpe)\n",
    "print(\"Mean Return: \", portfolio.mean)\n",
    "print(\"Volatility: \", portfolio.vol)\n",
    "print(\"Annualized Sharpe Ratio: \", portfolio.sharpe * 52**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2013: 2.566990049343786, 2014: 1.9076831373706442, 2015: 0.12723504122331197, 2016: 1.4555974821027775, 2017: 3.5366021511048613, 2018: 0.12584622128489226, 2019: 2.5444101060103423, 2020: 1.1942350404174613, 2021: 0.8758284373118804}\n",
      "Mean Sharpe Ratio:  1.5927141851299953\n"
     ]
    }
   ],
   "source": [
    "def compute_annualized_sharpe_ratio(\n",
    "    returns: pd.Series | np.ndarray,\n",
    "    risk_free_rate: float = 0.02,\n",
    "    periods_per_year: int = 52,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute the annualized Sharpe Ratio.\n",
    "\n",
    "    Args:\n",
    "        returns (pd.Series | np.ndarray): Periodic returns of the portfolio.\n",
    "        risk_free_rate (float, optional): Annual risk-free rate (e.g., 0.02 for 2%). Defaults to 0.02.\n",
    "        periods_per_year (int, optional): Number of return periods in a year (e.g., 52 for weekly). Defaults to 52.\n",
    "\n",
    "    Returns:\n",
    "        float: Annualized Sharpe Ratio.\n",
    "    \"\"\"\n",
    "    if isinstance(returns, pd.Series):\n",
    "        returns = returns.dropna().values\n",
    "    else:\n",
    "        returns = np.array(returns)\n",
    "        returns = returns[~np.isnan(returns)]\n",
    "\n",
    "    if len(returns) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # Calculate excess returns by subtracting the per-period risk-free rate\n",
    "    excess_returns = returns - (risk_free_rate / periods_per_year)\n",
    "\n",
    "    # Calculate mean and standard deviation of excess returns\n",
    "    mean_excess_return = np.mean(excess_returns) * periods_per_year  # Annualize mean\n",
    "    std_excess_return = np.std(excess_returns, ddof=1) * np.sqrt(\n",
    "        periods_per_year\n",
    "    )  # Annualize std\n",
    "\n",
    "    # Compute Sharpe Ratio with numerical stability\n",
    "    sharpe_ratio = mean_excess_return / (\n",
    "        std_excess_return + 1e-18\n",
    "    )  # Add epsilon to prevent division by zero\n",
    "\n",
    "    return sharpe_ratio\n",
    "\n",
    "\n",
    "def analyze_returns(\n",
    "    returns: pd.DataFrame, risk_free_rate: float = 0.02, frequency: int = 52\n",
    ") -> dict[str, any]:\n",
    "    \"\"\"\n",
    "    Analyze the returns DataFrame to verify date index, count total weeks,\n",
    "    identify years covered, count weeks per year, and compute Sharpe Ratio per year.\n",
    "\n",
    "    Args:\n",
    "        returns (pd.DataFrame): DataFrame containing portfolio returns with a DatetimeIndex.\n",
    "        risk_free_rate (float, optional): Annual risk-free rate (default is 2%). Defaults to 0.02.\n",
    "        frequency (int, optional): Number of periods per year (default is 52 for weekly data). Defaults to 52.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary containing analysis results.\n",
    "    \"\"\"\n",
    "    analysis_results: dict[str, any] = {}\n",
    "\n",
    "    # 1. Verify that the DataFrame is indexed by dates\n",
    "    if not isinstance(returns.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"The DataFrame index must be a pandas DatetimeIndex.\")\n",
    "    analysis_results[\"DateIndexValid\"] = True\n",
    "\n",
    "    # 2. Count the total number of weeks\n",
    "    total_weeks: int = len(returns)\n",
    "    analysis_results[\"TotalWeeks\"] = total_weeks\n",
    "\n",
    "    # 3. Identify all the years covered in the dataset\n",
    "    years: list[int] = returns.index.year.unique().tolist()\n",
    "    years.sort()  # Sort the years in ascending order\n",
    "    analysis_results[\"YearsCovered\"] = years\n",
    "\n",
    "    # 4. Count the number of weeks for each individual year\n",
    "    weeks_per_year: dict[int, int] = {}\n",
    "    grouped = returns.groupby(returns.index.year)\n",
    "\n",
    "    for year, group in grouped:\n",
    "        weeks_count = len(group)\n",
    "        weeks_per_year[year] = weeks_count\n",
    "\n",
    "    analysis_results[\"WeeksPerYear\"] = weeks_per_year\n",
    "\n",
    "    # 5. Compute Sharpe Ratio for each year\n",
    "    sharpe_ratios_per_year: dict[int, float] = {}\n",
    "    for year, group in grouped:\n",
    "        sharpe = compute_annualized_sharpe_ratio(\n",
    "            returns=group[\"rets\"],\n",
    "            risk_free_rate=risk_free_rate,\n",
    "            periods_per_year=analysis_results[\"WeeksPerYear\"][year],\n",
    "        )\n",
    "        sharpe_ratios_per_year[year] = sharpe\n",
    "\n",
    "    analysis_results[\"SharpeRatiosPerYear\"] = sharpe_ratios_per_year\n",
    "\n",
    "    return analysis_results\n",
    "\n",
    "\n",
    "analyze = analyze_returns(portfolio.returns, risk_free_rate=0, frequency=52)\n",
    "print(analyze[\"SharpeRatiosPerYear\"])\n",
    "print(\"Mean Sharpe Ratio: \", np.mean(list(analyze[\"SharpeRatiosPerYear\"].values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_26572\\2896385867.py:90: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  factor_5 = pdr.get_data_famafrench(\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_26572\\2896385867.py:99: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  mom_df = pdr.get_data_famafrench(\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_26572\\2896385867.py:106: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  st_df = pdr.get_data_famafrench(\n",
      "C:\\Users\\zhang\\AppData\\Local\\Temp\\ipykernel_26572\\2896385867.py:113: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  lt_df = pdr.get_data_famafrench(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3680, 8) (3680, 20) (2557, 8) (2557, 20)\n",
      "Ticker          AMZN       JPM       NEM       BAC         T       HAL  \\\n",
      "Date                                                                     \n",
      "1997-05-19 -0.012040 -0.010681  0.016835 -0.016563  0.002179 -0.009966   \n",
      "1997-05-20 -0.042685  0.041835  0.003312  0.012632  0.006521  0.006711   \n",
      "1997-05-21 -0.127392 -0.034973 -0.003301 -0.027027 -0.017278  0.008333   \n",
      "1997-05-22 -0.021891  0.016107  0.000000  0.000000 -0.017582  0.001654   \n",
      "1997-05-23  0.074622  0.005284  0.013245  0.008547  0.017897  0.000000   \n",
      "\n",
      "Ticker           MCD       WMT        ED      COST       PFE         C  \\\n",
      "Date                                                                     \n",
      "1997-05-19  0.002403  0.020920  0.012931  0.038910 -0.003722  0.009050   \n",
      "1997-05-20 -0.016787  0.000000  0.000000  0.029962  0.004981  0.033632   \n",
      "1997-05-21 -0.007317 -0.016394 -0.008511 -0.040000 -0.006195 -0.030368   \n",
      "1997-05-22  0.000000 -0.008333 -0.017167 -0.020833 -0.008729 -0.006711   \n",
      "1997-05-23  0.004914  0.012605  0.013100 -0.013540  0.011321  0.018018   \n",
      "\n",
      "Ticker           LMT        VZ       XOM       CAT       JNJ      MSFT  \\\n",
      "Date                                                                     \n",
      "1997-05-19  0.016643  0.000000  0.017204 -0.005076  0.016597 -0.002708   \n",
      "1997-05-20  0.013643  0.000000  0.000000 -0.010205 -0.006123  0.034746   \n",
      "1997-05-21  0.008075 -0.001815  0.006342 -0.002577 -0.024640  0.010493   \n",
      "1997-05-22  0.005340 -0.014546 -0.016807  0.007752  0.002105  0.002077   \n",
      "1997-05-23 -0.010624  0.016606  0.023504  0.000000  0.006302  0.018653   \n",
      "\n",
      "Ticker           DIS      AAPL  \n",
      "Date                            \n",
      "1997-05-19  0.029366 -0.014492  \n",
      "1997-05-20  0.003003  0.014705  \n",
      "1997-05-21 -0.016467 -0.021738  \n",
      "1997-05-22  0.000000 -0.014814  \n",
      "1997-05-23  0.013699  0.015036   Ticker          AMZN       JPM       NEM       BAC         T       HAL  \\\n",
      "Date                                                                     \n",
      "2011-08-03 -0.008219  0.001507  0.017637  0.005269  0.010966  0.007596   \n",
      "2011-08-04 -0.040389 -0.049624 -0.046918 -0.074423 -0.024407 -0.094798   \n",
      "2011-08-05  0.006055 -0.008439 -0.000552 -0.074745  0.005212 -0.019571   \n",
      "2011-08-08 -0.044401 -0.094149 -0.005146 -0.203182 -0.042517 -0.098959   \n",
      "2011-08-09  0.058802  0.068702  0.023462  0.167435  0.041516  0.054678   \n",
      "\n",
      "Ticker           MCD       WMT        ED      COST       PFE         C  \\\n",
      "Date                                                                     \n",
      "2011-08-03  0.005643 -0.007740  0.006886  0.019712  0.005016  0.005939   \n",
      "2011-08-04 -0.014730 -0.023011 -0.017097 -0.007395 -0.037160 -0.065753   \n",
      "2011-08-05  0.009493  0.014970  0.029571  0.003006  0.007488 -0.039357   \n",
      "2011-08-08 -0.034909 -0.037954 -0.053689 -0.079750 -0.047455 -0.164175   \n",
      "2011-08-09  0.046888  0.038839  0.029558  0.057207  0.056423  0.138462   \n",
      "\n",
      "Ticker           LMT        VZ       XOM       CAT       JNJ      MSFT  \\\n",
      "Date                                                                     \n",
      "2011-08-03 -0.001629  0.013525 -0.001542 -0.009261 -0.000315  0.004478   \n",
      "2011-08-04 -0.030451 -0.029191 -0.049923 -0.069901 -0.024759 -0.036404   \n",
      "2011-08-05  0.021032  0.003723  0.013272  0.016081  0.014068 -0.010023   \n",
      "2011-08-08 -0.053831 -0.055064 -0.061882 -0.092208 -0.025354 -0.046729   \n",
      "2011-08-09  0.028737  0.035327  0.020659  0.059080  0.017670  0.044935   \n",
      "\n",
      "Ticker           DIS      AAPL  \n",
      "Date                            \n",
      "2011-08-03  0.012442  0.009410  \n",
      "2011-08-04 -0.055570 -0.038719  \n",
      "2011-08-05 -0.004809 -0.009937  \n",
      "2011-08-08 -0.061114 -0.054628  \n",
      "2011-08-09  0.050560  0.058889  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from pandas_datareader import data as pdr\n",
    "\n",
    "# Assuming TrainTest is defined elsewhere in your codebase\n",
    "# from your_module import TrainTest\n",
    "\n",
    "\n",
    "def AV_yFinance(\n",
    "    start: str,\n",
    "    end: str,\n",
    "    split: List[float],\n",
    "    freq: str = \"weekly\",\n",
    "    n_obs: int = 104,\n",
    "    n_y: Optional[int] = None,\n",
    "    use_cache: bool = False,\n",
    "    save_results: bool = False,\n",
    ") -> Tuple[TrainTest, TrainTest]:\n",
    "\n",
    "    if use_cache:\n",
    "        X = pd.read_pickle(f\"./cache/factor_{freq}.pkl\")\n",
    "        Y = pd.read_pickle(f\"./cache/asset_{freq}.pkl\")\n",
    "    else:\n",
    "        # Define the list of tickers\n",
    "        tick_list = [\n",
    "            \"AAPL\",\n",
    "            \"MSFT\",\n",
    "            \"AMZN\",\n",
    "            \"C\",\n",
    "            \"JPM\",\n",
    "            \"BAC\",\n",
    "            \"XOM\",\n",
    "            \"HAL\",\n",
    "            \"MCD\",\n",
    "            \"WMT\",\n",
    "            \"COST\",\n",
    "            \"CAT\",\n",
    "            \"LMT\",\n",
    "            \"JNJ\",\n",
    "            \"PFE\",\n",
    "            \"DIS\",\n",
    "            \"VZ\",\n",
    "            \"T\",\n",
    "            \"ED\",\n",
    "            \"NEM\",\n",
    "        ]\n",
    "\n",
    "        if n_y is not None:\n",
    "            tick_list = tick_list[:n_y]\n",
    "\n",
    "        # Download asset data using yfinance\n",
    "        data = yf.download(\n",
    "            tick_list,\n",
    "            start=start,\n",
    "            end=end,\n",
    "            progress=False,\n",
    "            group_by=\"ticker\",\n",
    "            auto_adjust=True,  # Adjusted close prices\n",
    "            threads=True,  # Enable multi-threading for faster downloads\n",
    "        )\n",
    "\n",
    "        if data.empty:\n",
    "            raise ValueError(\n",
    "                \"No data downloaded. Please check the ticker symbols and date range.\"\n",
    "            )\n",
    "\n",
    "        # Extract Adjusted Close prices\n",
    "        if len(tick_list) == 1:\n",
    "            # For single ticker, data['Close'] is a Series, convert to DataFrame\n",
    "            adj_close = data[\"Close\"].to_frame()\n",
    "            adj_close.columns = tick_list\n",
    "        else:\n",
    "            # For multiple tickers, use xs to extract 'Close' for all tickers\n",
    "            try:\n",
    "                adj_close = data.xs(\"Close\", level=1, axis=1)\n",
    "            except KeyError:\n",
    "                raise KeyError(\"Close prices not found in the downloaded data.\")\n",
    "\n",
    "        # Compute daily returns as percentage change\n",
    "        Y = adj_close.pct_change().dropna()\n",
    "\n",
    "        # Download factor data from Kenneth French's data library\n",
    "        dl_freq = \"_daily\"\n",
    "\n",
    "        try:\n",
    "            # 5-Factor Model\n",
    "            factor_5 = pdr.get_data_famafrench(\n",
    "                \"F-F_Research_Data_5_Factors_2x3\" + dl_freq,\n",
    "                start=start,\n",
    "                end=end,\n",
    "            )[0]\n",
    "            rf_df = factor_5[\"RF\"]\n",
    "            factor_5 = factor_5.drop([\"RF\"], axis=1)\n",
    "\n",
    "            # Momentum Factor\n",
    "            mom_df = pdr.get_data_famafrench(\n",
    "                \"F-F_Momentum_Factor\" + dl_freq,\n",
    "                start=start,\n",
    "                end=end,\n",
    "            )[0]\n",
    "\n",
    "            # Short-Term Reversal Factor\n",
    "            st_df = pdr.get_data_famafrench(\n",
    "                \"F-F_ST_Reversal_Factor\" + dl_freq,\n",
    "                start=start,\n",
    "                end=end,\n",
    "            )[0]\n",
    "\n",
    "            # Long-Term Reversal Factor\n",
    "            lt_df = pdr.get_data_famafrench(\n",
    "                \"F-F_LT_Reversal_Factor\" + dl_freq,\n",
    "                start=start,\n",
    "                end=end,\n",
    "            )[0]\n",
    "\n",
    "            # Concatenate all factors and convert to decimal\n",
    "            X = pd.concat([factor_5, mom_df, st_df, lt_df], axis=1) / 100\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to download factor data: {e}\")\n",
    "\n",
    "        # Align factor data (X) with asset returns (Y) based on dates\n",
    "\n",
    "        # Remove timezone from Y.index if present\n",
    "        if Y.index.tz is not None:\n",
    "            Y.index = Y.index.tz_convert(None)\n",
    "\n",
    "        # Ensure X.index is also timezone-naive\n",
    "        if X.index.tz is not None:\n",
    "            X.index = X.index.tz_convert(None)\n",
    "\n",
    "        # Now, perform the alignment\n",
    "        try:\n",
    "            X = X.loc[Y.index]\n",
    "        except KeyError as e:\n",
    "            missing_dates = Y.index.difference(X.index)\n",
    "            if not missing_dates.empty:\n",
    "                print(f\"Missing dates in factor data: {missing_dates}\")\n",
    "                # Optionally, you can drop missing dates or handle them differently\n",
    "                Y = Y.loc[Y.index.intersection(X.index)]\n",
    "                X = X.loc[X.index.intersection(Y.index)]\n",
    "            else:\n",
    "                raise e  # Re-raise if no missing dates found\n",
    "\n",
    "        # Resample data if frequency is not daily\n",
    "        freq_lower = freq.lower()\n",
    "        if freq_lower in [\"weekly\", \"wk\", \"1wk\"]:\n",
    "            Y = Y.resample(\"W-FRI\").apply(lambda x: (x + 1).prod() - 1)\n",
    "            X = X.resample(\"W-FRI\").apply(lambda x: (x + 1).prod() - 1)\n",
    "        elif freq_lower in [\"monthly\", \"1mo\"]:\n",
    "            Y = Y.resample(\"M\").apply(lambda x: (x + 1).prod() - 1)\n",
    "            X = X.resample(\"M\").apply(lambda x: (x + 1).prod() - 1)\n",
    "        # Add more resampling frequencies if needed\n",
    "\n",
    "        # Handle missing values by forward and backward filling using ffill() and bfill()\n",
    "        Y = Y.ffill().bfill()\n",
    "        X = X.ffill().bfill()\n",
    "\n",
    "        # Convert the index to 'YYYY-MM-DD' format\n",
    "        X.index = X.index.strftime(\"%Y-%m-%d\")\n",
    "        Y.index = Y.index.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Optionally save the results to cache\n",
    "        if save_results:\n",
    "            os.makedirs(\"./cache\", exist_ok=True)\n",
    "            X.to_pickle(f\"./cache/factor_{freq}.pkl\")\n",
    "            Y.to_pickle(f\"./cache/asset_{freq}.pkl\")\n",
    "\n",
    "    # Partition dataset into training and testing sets. Lag the data by one observation\n",
    "    # Using the provided TrainTest class\n",
    "    X_train_test = TrainTest(X[:-1], n_obs, split)\n",
    "    Y_train_test = TrainTest(Y[1:], n_obs, split)\n",
    "\n",
    "    return X_train_test, Y_train_test\n",
    "\n",
    "\n",
    "start_paddling = \"1990-01-01\"\n",
    "end_paddling = \"2021-09-30\"  # Data frequency and start/end dates\n",
    "daily_frequency = \"daily\"\n",
    "xf_train_test, yf_train_test = AV_yFinance(\n",
    "    start=start_paddling,\n",
    "    end=end_paddling,\n",
    "    split=[0.6, 0.4],\n",
    "    freq=daily_frequency,\n",
    "    n_obs=104,\n",
    "    n_y=20,\n",
    "    use_cache=False,\n",
    "    save_results=True,\n",
    ")\n",
    "print(\n",
    "    xf_train_test.train().shape,\n",
    "    yf_train_test.train().shape,\n",
    "    xf_train_test.test().shape,\n",
    "    yf_train_test.test().shape,\n",
    ")\n",
    "print(yf_train_test.train().head(), yf_train_test.test().head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
